<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="魏笙葭"><meta name="copyright" content="魏笙葭"><meta name="generator" content="Hexo 7.3.0"><meta name="theme" content="hexo-theme-yun"><title>Transformer学习记录 | WeiSJ&amp;HEXO</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...undefined?.options,
  });
});</script><link class="aplayer-style-marker" rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script class="aplayer-script-marker" src="https://fastly.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" defer></script><script class="meting-script-marker" src="https://fastly.jsdelivr.net/npm/meting@1/dist/Meting.min.js" defer></script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"lengnian.github.io","root":"/","title":"魏笙葭的小窝","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="WeiSJ&HEXO" type="application/atom+xml"><meta name="description" content="本文用于记录自己在学习Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。 Self-attentionPersonal understandself-attention可以和fc交替使用，输入几个vector，就会输出几个vector。  默认计算注意力的方法使用Dot-product 假设在a之前还有其它运算，所以命名为 $ a^">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer学习记录">
<meta property="og:url" content="http://lengnian.github.io/posts/3b51a2ff/index.html">
<meta property="og:site_name" content="WeiSJ&amp;HEXO">
<meta property="og:description" content="本文用于记录自己在学习Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。 Self-attentionPersonal understandself-attention可以和fc交替使用，输入几个vector，就会输出几个vector。  默认计算注意力的方法使用Dot-product 假设在a之前还有其它运算，所以命名为 $ a^">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131028202.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131316164.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206132747390.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133116827.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133314813.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133534659.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133708410.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133823384.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151609909.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151742513.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155029432.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155823308.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155316616.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155536850.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161249953.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161453825.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161732336.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161950727.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206162515398.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206163352894.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206164332001.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165024419.png">
<meta property="og:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165109208.png">
<meta property="article:published_time" content="2024-12-06T04:57:27.000Z">
<meta property="article:modified_time" content="2024-12-16T05:23:01.963Z">
<meta property="article:author" content="魏笙葭">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lengnian.github.io/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131028202.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="魏笙葭"><img width="96" loading="lazy" src="/images/avatar1.png" alt="魏笙葭"><span class="site-author-status" title="不想科研">😭</span></a><div class="site-author-name"><a href="/about/">魏笙葭</a></div><span class="site-name">WeiSJ&HEXO</span><sub class="site-subtitle">享受独处</sub><div class="site-description">慢热且长情</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">30</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">22</span></a></div><a class="site-state-item hty-icon-button" href="/guestbook/" title="留言板"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:message-2-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/LengNian" title="GitHub" target="_blank" style="color:#6E5494"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.mit.edu/" title="School" target="_blank" style="color:#7E2222"><span class="icon iconify" data-icon="ri:school-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:1031477927@qq.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=1544549234" title="网易云音乐" target="_blank" style="color:#C10D0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="知乎" target="_blank" style="color:#0084FF"><span class="icon iconify" data-icon="ri:zhihu-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.link" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><span class="icon iconify" data-icon="ri:train-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="QQ" target="_blank" style="color:#12B7F5"><span class="icon iconify" data-icon="ri:qq-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="微信公众号" target="_blank" style="color:#1AAD19"><span class="icon iconify" data-icon="ri:wechat-2-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="Twitter" target="_blank" style="color:#1da1f2"><span class="icon iconify" data-icon="ri:twitter-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" title="Telegram Channel" target="_blank" style="color:#0088CC"><span class="icon iconify" data-icon="ri:telegram-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a><a class="links-item hty-icon-button" href="/girls/" title="我的小天使们" style="color:#FF0505"><span class="icon iconify" data-icon="ri:hearts-fill"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-attention"><span class="toc-number">1.</span> <span class="toc-text">Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Personal-understand"><span class="toc-number">1.1.</span> <span class="toc-text">Personal understand</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Matrix-Multiply-show"><span class="toc-number">1.2.</span> <span class="toc-text">Matrix Multiply show</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-Self-attention"><span class="toc-number">2.</span> <span class="toc-text">Multi-head Self-attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">3.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compare"><span class="toc-number">4.</span> <span class="toc-text">Compare</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-number">5.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder"><span class="toc-number">5.1.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decoder"><span class="toc-number">5.2.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Input-Output"><span class="toc-number">5.2.1.</span> <span class="toc-text">Input &amp; Output</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Masked-multi-head-attention"><span class="toc-number">5.2.2.</span> <span class="toc-text">Masked multi-head-attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Cross-attention"><span class="toc-number">5.2.3.</span> <span class="toc-text">Cross attention</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training"><span class="toc-number">5.3.</span> <span class="toc-text">Training</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0-%E8%A7%86%E9%A2%91"><span class="toc-number">6.</span> <span class="toc-text">参考文章(视频)</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="http://LengNian.github.io/posts/3b51a2ff/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="魏笙葭"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="WeiSJ&amp;HEXO"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Transformer学习记录</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-12-06 12:57:27" itemprop="dateCreated datePublished" datetime="2024-12-06T12:57:27+08:00">2024-12-06</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2024-12-16 13:23:01" itemprop="dateModified" datetime="2024-12-16T13:23:01+08:00">2024-12-16</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">1k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">4m</span></span></span><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><span class="icon iconify" data-icon="ri:eye-line"></span> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%86%99%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">写笔记</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/Transformer/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">Transformer</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>本文用于记录自己在学习Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。</p>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><h4 id="Personal-understand"><a href="#Personal-understand" class="headerlink" title="Personal understand"></a>Personal understand</h4><p>self-attention可以和fc交替使用，输入几个vector，就会输出几个vector。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131028202.png" alt="image-20241206131028202" loading="lazy"></p>
<p>默认计算注意力的方法使用Dot-product</p>
<p>假设在a之前还有其它运算，所以命名为 $ a^1, a^2,…$。</p>
<ul>
<li><p>如图，query，key，value是如何产生的？</p>
<p>有一个$ W^q $矩阵,通过和输入$a^i$相乘,就能得到$q^i$,也就是query。</p>
<p>同理。也存在一个$W^k, W^v$矩阵，和输入$a^i$相乘，就得到 $k^i, k^v$,也就是key, value。</p>
<p>$query和key进行一个dot product就得到了注意力分数也就是图中所示的\alpha$</p>
<p>以$a^1$为例，$q^1$ dot product  $k^1以后得到了\alpha_{1,1}, q^1和k^2得到了\alpha_{1,2}…$</p>
<p>**我的理解就是$\alpha_{1,j}$就是特征1和特征1、特诊2…特征j之间的注意力分数，为了更好运算，将这些进行一个softmax，统一到(0, 1)之间，得到了 **${\alpha _{1, j}}^{‘}$。</p>
</li>
</ul>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131316164.png" alt="image-20241206131316164" loading="lazy"></p>
<p>之后将$\alpha^{‘}_{1,j}和对应的v^j相乘，就得到了b^1$。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206132747390.png" alt="image-20241206132747390" loading="lazy"></p>
<p> 上面的两张图是对输入$a^1$进行计算的一个过程。其他的输入也一样。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133116827.png" alt="image-20241206133116827" loading="lazy"></p>
<h4 id="Matrix-Multiply-show"><a href="#Matrix-Multiply-show" class="headerlink" title="Matrix Multiply show"></a>Matrix Multiply show</h4><p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133314813.png" alt="image-20241206133314813" loading="lazy"></p>
<p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133534659.png" alt="image-20241206133534659" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133708410.png" alt="image-20241206133708410" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133823384.png" alt="image-20241206133823384" loading="lazy"></p>
<p><strong>在整个self-attention过程中，只有</strong>   $W^k,W^v,W^q$ <strong>是需要通过训练学习的</strong>。</p>
<h3 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h3><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151609909.png" alt="image-20241206151609909" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151742513.png" alt="image-20241206151742513" loading="lazy"></p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>位置编码通过一种特殊的方式产生，不一定要通过sin&#x2F;cos产生，也可以通过网络学习产生。</p>
<h3 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h3><p>self-attention就是复杂的CNN</p>
<p>普通的RNN只考虑了左边最近的一个词，而且无法并行处理，而self-attention可以并行处理，并且考虑了所有词之间的关系。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155029432.png" alt="image-20241206155029432" loading="lazy"><br>Encoder可以简化成这种结构 ，里面包含</p>
<ul>
<li><p>residual connection </p>
</li>
<li><p>layer norm</p>
</li>
</ul>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155823308.png" alt="image-20241206155823308" loading="lazy"></p>
<p>每个block的结构又如下图所示<br><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155316616.png" alt="image-20241206155316616" loading="lazy"></p>
<p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155536850.png" alt="image-20241206155536850" loading="lazy"></p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><h5 id="Input-Output"><a href="#Input-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h5><p> voc_size:就是可能输出的单词总数，比如26个字母，或者2000个汉字</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161249953.png" alt="image-20241206161249953" loading="lazy"></p>
<p>这里的distribution经过一个softmax，加起来的总和是1，选择一个概率最大的，作为decoder的下一个输入，Decoder看到的输入其实就是自己上一次的输出，所以是可能看到错误的。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161453825.png" alt="image-20241206161453825" loading="lazy"></p>
<h5 id="Masked-multi-head-attention"><a href="#Masked-multi-head-attention" class="headerlink" title="Masked multi-head-attention"></a>Masked multi-head-attention</h5><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161732336.png" alt="image-20241206161732336" loading="lazy"></p>
<p>可以注意到Decoder中有一个Masked Multi-Head Attention，这里的masked的意思就是看不到在输入之后的内容</p>
<p>比如处理 $a^1$时，只能和$a^1自己计算得到b^1$,处理$a^2时，只能和a^1,a^2进行计算得到b^2,在处理a^4时，则是和a^1,a^2,a^3,a^4进行计算得到b^4$。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161950727.png" alt="image-20241206161950727" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206162515398.png" alt="image-20241206162515398" loading="lazy"></p>
<p><strong>为什么要用masked？</strong></p>
<p>本文刚开始提到的self-attention的输入是一次性给进去的，所以计算注意力时可以使用后面的，而transformer中的输出其实是一步一步产生，也就是处理$a1$时，并不知道$a^2$的值，处理$a^2时，只能用a^2,a^1,而并不知道a^3,a^4$。所以这里就要用masked。</p>
<p>这里要有特殊符号，BEGIN和END，用于开始开始和结束。 </p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206163352894.png" alt="image-20241206163352894" loading="lazy"></p>
<h5 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross attention"></a>Cross attention</h5><p>Decoder提供一个query，Encoder提供key和value，使用q和k计算注意力分数以后，再和c相乘，就是交叉注意力机制的过程。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206164332001.png" alt="image-20241206164332001" loading="lazy"></p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165024419.png" alt="image-20241206165024419" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165109208.png" alt="image-20241206165109208" loading="lazy"></p>
<p>Decoder每一个输出都有一个cross entropy，希望cross entropy的总和最少，同时模型还要能够输出结束标记END。</p>
<h3 id="参考文章-视频"><a href="#参考文章-视频" class="headerlink" title="参考文章(视频)"></a>参考文章(视频)</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1o2421A7Dr/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">【研1基本功 （真的很简单）注意力机制】手写多头注意力机制_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1cV411X7XN/?spm_id_from=333.337.search-card.all.click&vd_source=dff3ad76ed17ff6ff5fee17de98f73c5">Transformer终于有拿得出手得教程了！ 台大李宏毅自注意力机制和Transformer详解！通俗易懂，草履虫都学的会！_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13z421U7cs/?spm_id_from=333.337.search-card.all.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">【官方双语】GPT是什么？直观解释Transformer | 深度学习第5章_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XH4y1T76e/?spm_id_from=333.337.search-card.all.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)_哔哩哔哩_bilibili</a></p>
<p> <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">I'm so cute. Please give me money.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="/images/Wechat_pay.png"><img loading="lazy" src="/images/Wechat_pay.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><span class="icon iconify" data-icon="ri:wechat-pay-line"></span></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>魏笙葭</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://lengnian.github.io/posts/3b51a2ff/" title="Transformer学习记录">http://lengnian.github.io/posts/3b51a2ff/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/posts/6c13436b/" rel="prev" title="CounTR-论文阅读与源码理解"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">CounTR-论文阅读与源码理解</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/posts/effa8074/" rel="next" title="Github克隆本地仓库再次上传文件"><span class="post-nav-text">Github克隆本地仓库再次上传文件</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>若您想及时得到回复提醒,建议发送邮件(邮箱在About me中可以找到)。</span><br></div><style>.utterances {
  max-width: 100%;
}</style><script src="https://utteranc.es/client.js" repo="LengNian/Blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; CopyRight 2023 – 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 魏笙葭</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div><div class="live-time"><span>感谢陪伴</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2024-07-22T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = ` ${passDay} 天 ${passHour} 小时 ${passMinute} 分 ${passSecond} 秒`;
}
blog_live_time();
</script></div><div id="busuanzi"><span id="busuanzi_container_site_uv" title="总访客量"><span><span class="icon iconify" data-icon="ri:user-line"></span></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><span class="icon iconify" data-icon="ri:eye-line"></span></span><span id="busuanzi_value_site_pv"></span></span><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="footer-custom-text">Edited by 魏笙葭</div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:search-line"></span></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="https://fastly.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js"></script><script src="/js/search/local-search.js" defer type="module"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><span class="icon iconify" data-icon="ri:close-line"></span></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div class="search-result-container"></div></div><script>function initMourn() {
  const date = new Date();
  const today = (date.getMonth() + 1) + "-" + date.getDate()
  const mourn_days = ["4-4","9-18","12-13"]
  if (mourn_days.includes(today)) {
    document.documentElement.style.filter = "grayscale(1)";
  }
}
initMourn();</script><div class="aplayer no-destroy" id="aplayer" data-id="4867800677" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay data-theme="#ad7a86" data-loop="all" data-order="random" data-preload="auto" data-volume="0.7" data-mutex data-lrctype="0" data-listfolded data-listmaxheight="340px" data-storagename="metingjs"></div><script src="https://fastly.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script><script>const images = [...document.querySelectorAll('.markdown-body img')]
mediumZoom(images)</script><style>.medium-zoom-image {
  z-index: 99;
}</style><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":-30,"vOffset":-20},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>