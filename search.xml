<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>About me</title>
    <url>/posts/653763f0/</url>
    <content><![CDATA[<h3 id="在代码的诗行里，我与你相遇"><a href="#在代码的诗行里，我与你相遇" class="headerlink" title="在代码的诗行里，我与你相遇"></a>在代码的诗行里，我与你相遇</h3><ul>
<li><strong>博客作者:</strong> &nbsp;&nbsp;魏笙葭</li>
<li><strong>转载请注明出处!</strong></li>
</ul>
<blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                                                                   宁可枝头抱香死，何曾吹落北风中。</p>
</blockquote>
<p>开始进行一个自我介绍</p>
<h4 id="经历"><a href="#经历" class="headerlink" title="经历"></a>经历</h4><p>大家好!<br>姓名:&nbsp; 魏笙葭(不是真名，看到网上一种起昵称方式，故拿来使用)<br>性别:&nbsp; 男<br>专业:&nbsp; 计算机科学与技术(高中时曾非常渴望学医或者兵器类，但由于身体原因未能遂愿)</p>
<ul>
<li>2002.07 出生在山西省某个小县城；</li>
<li>2008-2020 在家门口读完了我的小学，中学，学习不好，但是也勉强考上了大学；</li>
<li>2020-2024 就读于某不知名二本大学，四年过的自认为相比于同年级的人还不错，认识了很多朋友，一起比赛、学习、约饭、拍照。从2023年寒假开始准备考研，2024年4月考研至某双非(具体考研过程可以在我的回忆录中找到)。有点遗憾,但也是自己努力的结果；</li>
<li>2024-至今 读研,目前还在暑假中,之后再来填坑；</li>
</ul>
<h4 id="个人简介"><a href="#个人简介" class="headerlink" title="个人简介"></a>个人简介</h4><ul>
<li><p>熬夜选手</p>
</li>
<li><p>写代码会写注释，方便别人也方便自己</p>
</li>
<li><p>慢热，长情，特立独行</p>
</li>
<li><p>别人说我是恋爱脑，现在感觉自己可能确实是</p>
</li>
<li><p>对不认识的人没什么话，很外向，但是熟悉以后就会变为话痨</p>
</li>
<li><p>不爱展示自己，尤其是在自己不擅长的领域</p>
</li>
<li><p>宅</p>
</li>
<li><p>脑子不聪明，比较愚笨，一个知识可能要花费比别人很久的时间才能弄明白</p>
</li>
<li><p>对自己想要做的事情可以很好的坚持下来</p>
</li>
<li><p>目前仍在追一部小说《神秘快递家族》系列（小学无意发现的，很好看）</p>
</li>
<li><p>大学时的自己有些仍由自己脾气，有些东西都被自己搞砸，希望自己可以克制一下自己性子</p>
</li>
<li><p>喜欢穿速干半袖，总觉得棉半袖很难受，爱穿冲锋衣，爱穿黑衣服黑裤子</p>
</li>
</ul>
<h4 id="业余爱好"><a href="#业余爱好" class="headerlink" title="业余爱好"></a>业余爱好</h4><ul>
<li>曾经喜欢画一些很可爱的玩意；<img src="/posts/653763f0/1.jpg" class loading="lazy">
<img src="/posts/653763f0/2.jpg" class loading="lazy">
<img src="/posts/653763f0/3.jpg" class loading="lazy">
<img src="/posts/653763f0/4.jpg" class loading="lazy"></li>
<li>喜欢跑步，平常会跑5km&#x2F;3km， 偶尔跑10km。曾经跑步时一味的追求配速，后来大腿外侧受伤，休息了八个月才有所好转，所以现在跑步不看配速，怎么舒服怎么来，大概就是5’30~6’30左右。</li>
<li>喜欢音乐，流行乐尤其是苦情歌，纯音乐（午睡时经常靠此催眠）</li>
<li>喜欢乐器，学过七年萨克斯，现在尤其喜欢钢琴，有点后悔当初没有去学钢琴或者没有坚持学下来电子琴（学萨克斯前一直在学电子琴）</li>
</ul>
<h4 id="交友原则"><a href="#交友原则" class="headerlink" title="交友原则"></a>交友原则</h4><ul>
<li>喜欢和志同道合的人聊天，如果你想和我交流可以给我发邮件<a href="mailto:&#x77;&#x65;&#x69;&#x73;&#106;&#x5f;&#108;&#x6e;&#x40;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#x6d;">&#x77;&#x65;&#x69;&#x73;&#106;&#x5f;&#108;&#x6e;&#x40;&#x31;&#x36;&#x33;&#x2e;&#x63;&#x6f;&#x6d;</a>，也可以把你的QQ在邮件中偷偷告诉我，我去加你。正常情况会每隔两三天检查邮箱，如果我回复不及时，可以多发邮件催促我。</li>
<li>无论是交友或者指正我的错误，希望大家可以开门见山，千万不要发在吗？有时间嘛？等，这些内容看到瘆得慌，我也会直接忽略掉。大家的邮件我看到会及时回复。</li>
</ul>
<h4 id="关于本站"><a href="#关于本站" class="headerlink" title="关于本站"></a>关于本站</h4><p>其实看到别人都在搞一个自己的博客，自己也有点心动，所以也自己搞一个，可以用于自己记录一些内容，喜欢自己可以保持初心，如果在阅读过程发现我有什么写的错误的地方，欢迎指正(可以通过邮件联系我)。</p>
<p>Q：博客的内容一定是正确的嘛？</p>
<p>A：不一定，甚至会有些错别字（别在意）。如果发现博客内容有错误的地方，请一定要指出来，非常欢迎大家指出我的错误。</p>
<p>Q：博客主要发什么内容？</p>
<p>A：主要发一些个人的随笔、回忆录、学习过程中的知识点、一些遇到的问题的解决方法，后续也计划发一些论文的内容。会当作自己的一个小空间，内容肯定会很杂，可以根据标签来寻找自己感兴趣的内容。</p>
<p>Q：怎么称呼？</p>
<p>A：只反感一些不尊敬的称呼，如果你愿意，可以用一些叠词非常肉麻的东西来称呼，也可以直接称呼我魏笙葭。</p>
<p>Q：怎么给我发邮件？</p>
<p>A：在交友原则中已经提到，千万不要发在吗？有时间嘛？等内容，可以进行一个简短的来意说明，然后直接说主要内容。</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>回忆录</tag>
      </tags>
  </entry>
  <entry>
    <title>CSRNet-论文阅读与源码理解</title>
    <url>/posts/56e3b13/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><em>前言</em></h3><p>国庆期间读了CSRNet这篇论文，本文不是对论文进行翻译，只是对文章进行简略的总结，并对部分内容写出我的理解，主要是将自己阅读和复现过程中遇到的些问题在这里记录一下，希望可以帮助到有需要的人，同时，本文的内容均为个人理解，如若有误，欢迎在评论区或者邮件指正。</p>
<p>博客不知道出了什么问题，图像上传一直出问题，所以本文的图麻烦大家自己对照原论文去查看。</p>
<p>论文原文： <a href="https://ieeexplore.ieee.org/document/8578218">CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes | IEEE Conference Publication | IEEE Xplore</a></p>
<p>源代码 ：<a href="https://github.com/leeyeehoo/CSRNet-pytorch">leeyeehoo&#x2F;CSRNet-pytorch: CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes (github.com)</a></p>
<h3 id="环境调试"><a href="#环境调试" class="headerlink" title="环境调试"></a><em>环境调试</em></h3><p>环境调试可以参考文章[<a href="https://blog.csdn.net/wpw5499/article/details/111496715">CSRNet] CSRNet-pytorch 复现过程记录-CSDN博客</a>，这篇文章介绍的很详细，我就不在这里赘述了，可以参考这篇文章的过程完成调试。</p>
<p>不想调试的也可以去看我调试之后的代码，同时代码中还加上了我自己对代码理解的中文注释。<a href="https://github.com/LengNian/PaperRecurrent/tree/main/CSRNet">PaperRecurrent&#x2F;CSRNet at main · LengNian&#x2F;PaperRecurrent (github.com)</a></p>
<h3 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a><em>读论文</em></h3><p>这篇文章提出一个可以应用到高度拥挤场景的卷积神经网络CSRNet。CSRNet是由两部分组成，前置的网络就是有卷积层，池化层等组成，后置网络则是由空洞卷积层组成。值得注意的是，前置的网络，作者直接使用了VGG16的网络结构。</p>
<p>在这篇论文中，作者通过对MCNN进行测试，如图所示，发现MCNN的三列在不同样本上错误率几乎相同，所以不排除是在同一个地方犯错的可能，这可能说明这三列学习到的特征是几乎相同的，这似乎就违背了作者设计MCNN的初衷，也就是利用有不同感受野的三列去学习不同尺度的特征。</p>
<p>Fig 1(对应论文中的Figure 2)</p>


<p>如果一味的追求网络深度去堆积卷积层和池化层会使图像的尺寸变小，同时也会影响生成密度图的质量，哪么怎样可以不压缩图像大小呢？作者提到，如果使用转置卷积会增加模型的复杂度，并且速度慢，在这里并不合适，而使用空洞卷积既可以扩大感受野，也不会增加参数量和计算量。（比如，使用卷积核大小为k×k的空洞卷积，其尺寸会被放大为k+(k-1)(r-1)，r为步长）。</p>
<p>作者通过一系列操作验证了空洞卷积比转置卷积更有效，即进行两组实验，一组是使用空洞卷积，另一组是使用卷积+池化+转置卷积，因为第二组的尺寸会变小，为了保持和第一组的大小保持一致，作者对图像进行上采样，对比结果如图所示。很明显可以看到空洞卷积的效果更好。 </p>
<p>Fig 2(对应论文中的Figure 4)</p>


<p>作者提出四种不同的CSRNet架构，通过实验最后使用了B结构。</p>
<p>Fig 3(对应论文中的Table 3)</p>


<p>此外，作者还在论文中提到了数据增强，会生成9张图像，其中前4张是不重叠的原图像的四分之一的图像，后五张是随机裁剪。</p>
<h3 id="源码理解"><a href="#源码理解" class="headerlink" title="源码理解"></a><em>源码理解</em></h3><p>感觉这篇文章的代码阅读起来相比MCNN容易一些。</p>
<p>在train.py中有这么一段代码</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">save_checkpoint(&#123;
    &#39;epoch&#39;: epoch + 1,
    &#39;arch&#39;: args.pre,
    &#39;state_dict&#39;: model.state_dict(),
    &#39;best_prec1&#39;: best_prec1,
    &#39;optimizer&#39; : optimizer.state_dict(),
&#125;, is_best,args.task)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>这里提到了保存优化器的状态，我以前没有注意过这点，上网查阅以后，这通常是一种预防措施，并不会占用太多的存储空间，而且处理起来也相对比较简单。</strong></p>
<p>在val.py中对图像进行标准化，作者有这么一段代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span>img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">92.8207477031</span>
img<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span>img<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">95.2757037428</span>
img<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">=</span>img<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token number">104.877445883</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>刚开始减这么一串数字我不太理解，后来在github中看到有人说这些数字可能是每个通道的均值。（但是每幅图的均值应该是不一样的吧，所以这里还是不太明白，如果我说的有错误，欢迎大家给我指正。）</p>
<p>论文中提到裁剪四分之一的图像，在dataset.py中实现listDataset中提到</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> train<span class="token punctuation">:</span>
	root <span class="token operator">=</span> root <span class="token operator">*</span><span class="token number">4</span>
random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>root<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>通过对路径*4，并打乱，完成四个不同四分之一图像的裁剪。</p>
<p>在image.py中</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token boolean">False</span><span class="token punctuation">:</span>
    crop_size <span class="token operator">=</span> <span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span>img<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token operator">&lt;=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>我认为这里的False应该改为train，结合其它地方也可以得出，这里是标记是否要开启数据增强。</p>
<p>在Shanghai Tech中，数据集的作者给出了.jpg和.mat文件，然后CSRNet的作者在make_dataset.ipynb文件中利用.mat文件生成了.h5文件，.h5文件在image.py中用于作为标签使用。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><em>参考文献</em></h3><p>[<a href="https://blog.csdn.net/wpw5499/article/details/111496715">CSRNet] CSRNet-pytorch 复现过程记录-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/judgechen1997/article/details/100893647">CSRNet浅析_csrnet代码解读-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/m0_46654197/article/details/108849993#:~:text=%E5%9F%BA%E4%BA%8E%E8%BF%99%E5%87%A0%E4%B8%AA%E7%89%B9%E7%82%B9%EF%BC%8CCS">深度学习：人群密度估计CSRNet(cvpr 2018)论文源代码详解_data.py train.py test.py utils.py-CSDN博客</a></p>
]]></content>
      <categories>
        <category>人群计数</category>
        <category>论文复现</category>
        <category>目标计数</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+GitHub Pages 个人博客搭建</title>
    <url>/posts/866b0119/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>很久之前在网上看到别人自己的博客网站，感觉很好看。等到有时间，便自己也搭建了一个自己的网站，就好像自己的一个小空间，里面可以放些自己的内容。自己在搭建过程中也遇到了很多坑，所以结合自己搭建的过程，在这里记录一下。</p>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="GitHub注册"><a href="#GitHub注册" class="headerlink" title="GitHub注册"></a>GitHub注册</h4><p>要现在github注册一个账号，并且需要记住的用户名以及邮箱（后面会用）</p>
<h4 id="Nodejs环境"><a href="#Nodejs环境" class="headerlink" title="Nodejs环境"></a>Nodejs环境</h4><p>Hexo是一款基于nodejs的博客框架，所以首先需要先安装nodejs。</p>
<p><a href="https://nodejs.org/en/">Node.js — Run JavaScript Everywhere (nodejs.org)</a>在这里下载。</p>
<p>下载之后使用Win+R进入cmd去验证是否安装成功。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">nodejs</span><span class="token expression">安装后，该命令会显示nodejs的版本号</span></span>
node <span class="token operator">-</span>v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<pre class="line-numbers language-c" data-language="c"><code class="language-c"># 还有很重要的一个npm安装，通过该命令可以显示npm的版本号
npm <span class="token operator">-</span>v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="Git环境"><a href="#Git环境" class="headerlink" title="Git环境"></a>Git环境</h4><p>在Git的官网进行下载。<a href="https://git-scm.com/">Git (git-scm.com)</a></p>
<p>傻瓜式安装即可。</p>
<p>通过一下代码检查是否成功（Win+R，输入cmd）</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">git <span class="token operator">--</span>version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>在桌面鼠标右击可以看到这个选项</p>
<img src="/posts/866b0119/1.png" class loading="lazy">

<p>Git是分布式的版本控制系统，需要将用户名和邮箱设置为标识，点击Git Bash Here后输入如下内容：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">git config <span class="token operator">--</span>global user<span class="token punctuation">.</span>name <span class="token string">"user_name"</span> # user_name 填GitHub的用户名<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<pre class="line-numbers language-c" data-language="c"><code class="language-c">git config <span class="token operator">--</span>global user<span class="token punctuation">.</span>email <span class="token string">"user_email"</span> # user_email填GitHub注册的邮箱<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>比如我的用户名为wsj，注册邮箱为<a href="mailto:&#x77;&#x73;&#106;&#49;&#x31;&#x31;&#x32;&#x32;&#50;&#x40;&#x67;&#x6d;&#x61;&#x69;&#108;&#x2e;&#x63;&#x6f;&#x6d;">&#x77;&#x73;&#106;&#49;&#x31;&#x31;&#x32;&#x32;&#50;&#x40;&#x67;&#x6d;&#x61;&#x69;&#108;&#x2e;&#x63;&#x6f;&#x6d;</a>（后文以此为基础演示），那么就需要填入以下内容：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">git config <span class="token operator">--</span>global user<span class="token punctuation">.</span>name <span class="token string">"wsj"</span>
git config <span class="token operator">--</span>global user<span class="token punctuation">.</span>email <span class="token string">"wsj111222@gmail.com"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h4 id="搭建GitHub博客"><a href="#搭建GitHub博客" class="headerlink" title="搭建GitHub博客"></a>搭建GitHub博客</h4><p><strong>注：这里新建仓库时，需要注意仓库的分支，如果默认分支和之后配置站点文件的分支不同会出现预览和实际不同的问题，<a href="#emphasize">见问题2</a>因为master容易带有侮辱性色彩，所以这里建议使用main分支，也建议在建仓库前修改默认分支为main。（后文配置站点文件以main为例）</strong></p>
<ul>
<li>修改全局默认分支右击头像-&gt;Settings-&gt;Repositories，修改default branch为main。</li>
<li>修改单个仓库默认分支，点击仓库，在仓库中点击Settings，在general中修改default branch为main。</li>
</ul>
<p>去GitHub上新建一个仓库，仓库命名一定命名为 username.github.io，username替换为自己实际的用户名，比如wsj.github.io，并且选择为public。</p>
<img src="/posts/866b0119/2.png" class loading="lazy">

<h4 id="配置SSH-key"><a href="#配置SSH-key" class="headerlink" title="配置SSH key"></a>配置SSH key</h4><p>点击Git Bash Here，输入以下命令：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">ssh<span class="token operator">-</span>keygen <span class="token operator">-</span>t rsa <span class="token operator">-</span>C <span class="token string">"user.email"</span> # user<span class="token punctuation">.</span>email为GitHub注册时的邮箱

# 例如
ssh<span class="token operator">-</span>keygen <span class="token operator">-</span>t rsa <span class="token operator">-</span>C <span class="token string">"wsj111222@gmail.com"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>连续按回车，不需要手动输入任何东西。</p>
<p>打开C:\用户\用户名下找到.ssh文件。使用记事本打开id_rsa.pub文件，将其内容全部复制。</p>
<p>紧接着在GitHub中点击头像，在settings中找到SSH and GPG keys，点击New SSH key。</p>
<img src="/posts/866b0119/3.png" class loading="lazy">

<img src="/posts/866b0119/4.png" class loading="lazy">

<p>将刚才复制的内容全部黏贴到Key中，Title可以任意填写。</p>
<p><strong>注意：id_rsa.pub是公钥，可以给别人，id_rsa是私钥，不可泄漏。</strong></p>
<p>此时，在Git中验证公钥配置是否成功，右击打开Git Bash here，输入</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">ssh <span class="token operator">-</span>T git@github<span class="token punctuation">.</span>com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>配置成功显示如下：</p>
<img src="/posts/866b0119/5.png" class loading="lazy">

<h3 id="本地配置Hexo"><a href="#本地配置Hexo" class="headerlink" title="本地配置Hexo"></a>本地配置Hexo</h3><p>在电脑的文件夹中新建一个文件夹，用于存放博客的相关内容，命名、存放目录随意。如我在E:\My Blogs创建。</p>
<h4 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h4><p>进入该目录即进入E:\My Blogs，打开Git（右击Git Bush here），输入以下命令</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">npm install <span class="token operator">-</span>g hexo<span class="token operator">-</span>cli<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h4 id="初始化hexo"><a href="#初始化hexo" class="headerlink" title="初始化hexo"></a>初始化hexo</h4><pre class="line-numbers language-c" data-language="c"><code class="language-c"># 输入命令
hexo init<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>初始化成功后会提示：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">init successful：
INFO  Cloning hexo<span class="token operator">-</span>starter https<span class="token operator">:</span><span class="token comment">//github.com/hexojs/hexo-starter.git</span>
INFO  Install dependencies
INFO  Start blogging with Hexo<span class="token operator">!</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>初始化成功后也会在目录中多出很多文件。</p>
<h4 id="配置站点文件"><a href="#配置站点文件" class="headerlink" title="配置站点文件"></a>配置站点文件</h4><p>在hexo中根据_config.yml文件进行配置。在根目录下的 _config.yml文件称为站点配置文件，即（E:\My Blogs\ _config.yml），使用不同主题时也会有配置文件，会位于E:\My Blogs\themes文件夹下，称为主题配置文件。</p>
<p>现在打开根目录下的站点配置文件，在最后找到deploy：修改其中的内容。</p>
<p>repo可以进入仓库，点击code，选择SSH或者HTTPS进行复制。</p>
<img src="/posts/866b0119/7.png" class loading="lazy">

<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">Deployment</span></span>
## Docs<span class="token operator">:</span> https<span class="token operator">:</span><span class="token comment">//hexo.io/docs/one-command-deployment</span>
deploy<span class="token operator">:</span>
  type<span class="token operator">:</span> git
  # 这里使用了SSH配置<span class="token punctuation">(</span>推荐<span class="token punctuation">)</span>。如果使用https，则是https<span class="token operator">:</span><span class="token comment">//github.com/owner/username.github.io.git</span>
  repo<span class="token operator">:</span> git@github<span class="token punctuation">.</span>com<span class="token operator">:</span>owner<span class="token operator">/</span>username<span class="token punctuation">.</span>github<span class="token punctuation">.</span>io<span class="token punctuation">.</span>git    
  branch<span class="token operator">:</span> main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>注：这里的branch是main还是master，应该根据自己的默认分支走，但是建议选择main，2020年10月1日后master正式由main取代，master容易带有侮辱性色彩。如果这里的branch没有设置正确，容易踩坑<a href="#emphasize">见问题2</a></p>
<h4 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h4><p>在Git中输入以下命令：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">Chexo g  # 生成静态文件<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>执行完该命令，Hexo 就会在 <code>public</code> 文件夹中生成相关的 <code>html</code> 文件，这些文件将来都是要提交到 <code>GitHub</code> 上的 <code>username.github.io</code> 的仓库中去的。</p>
<h4 id="预览主题"><a href="#预览主题" class="headerlink" title="预览主题"></a>预览主题</h4><p>在Git中输入以下命令：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">hexo s<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>打开浏览器访问 <code>http://localhost:4000</code> 即可看到内容C</p>
<img src="/posts/866b0119/6.png" class loading="lazy">

<p>我这里第2行是因为使用了主题，不会影响预览。</p>
<h4 id="安装部署工具"><a href="#安装部署工具" class="headerlink" title="安装部署工具"></a>安装部署工具</h4><p>部署到GitHub使用hexo d命令，但是在此之前要先安装一个部署插件</p>
<pre class="line-numbers language-C" data-language="C"><code class="language-C">npm install hexo-deployer-git --save # 安装部署插件<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h4 id="部署到GitHub"><a href="#部署到GitHub" class="headerlink" title="部署到GitHub"></a>部署到GitHub</h4><p>输入</p>
<pre class="line-numbers language-C" data-language="C"><code class="language-C">hexo d<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>部署成功后，在浏览器中输入username.github.io，例如wsj.github.io，就可以进入自己的博客啦！</p>
<h3 id="我遇到的问题"><a href="#我遇到的问题" class="headerlink" title="我遇到的问题"></a>我遇到的问题</h3><h4 id="问题1："><a href="#问题1：" class="headerlink" title="问题1："></a>问题1：</h4><p>在hexo初始化，即hexo init过程中出现</p>
<pre class="line-numbers language-C" data-language="C"><code class="language-C">INFO  Cloning hexo-starter https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-starter.git
fatal: unable to access &#39;https:&#x2F;&#x2F;github.com&#x2F;hexojs&#x2F;hexo-starter.git&#x2F;&#39;: Failed to connect to github.com port 443 after 21055 ms: Couldn&#39;t connect to server
WARN  git clone failed. Copying data instead
FATAL Something&#39;s wrong. Maybe you can find the solution here: https:&#x2F;&#x2F;hexo.io&#x2F;docs&#x2F;troubleshooting.html<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>  可能是网速问题，解决方法：可以多次尝试hexo init解决。C</p>
<h4 id="问题2："><a href="#问题2：" class="headerlink" title="问题2："></a>问题2：<span id="emphasize"></span></h4><p>我第一次配置时，明明使用hexo s预览没有问题，但是进入username.github.io中却没有变化（可以区分清楚不同分支者可以跳过此问题，此问题对于新手来说建议博客仓库中有一个分支）。</p>
<p>最后的原因就是因为branch没有配置正确，默认分支为master，而实际配置站点分支为main。</p>
<p>解决方法：删除仓库以及博客文件夹，重新配置。</p>
<h4 id="问题3："><a href="#问题3：" class="headerlink" title="问题3："></a>问题3：</h4><p>LF will be replaced by CRLF</p>
<p>Git中提示warning: LF will be replaced by CRLF in</p>
<p>执行：</p>
<pre class="line-numbers language-C" data-language="C"><code class="language-C">git config --global core.safecrlf false<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://blog.csdn.net/qq_62928039/article/details/130248518">Hexo+GitHub搭建个人博客教程（2023最新版）_hexo github个人网站教程-CSDN博客</a></p>
<p><a href="https://fanyfull.github.io/2021/10/16/Github-Hexo-%E7%9C%9F-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA-GitHub-%E9%9D%99%E6%80%81%E5%8D%9A%E5%AE%A2/#%E4%BD%BF%E7%94%A8-hexo-%E5%8D%9A%E5%AE%A2%E6%A1%86%E6%9E%B6">GiHub + Hexo 真·从零开始搭建个人博客 - Fany Full’s Blog</a></p>
<p><a href="https://blog.csdn.net/Lu_xiuyuan/article/details/112056997">关于hexo更新到GitHub后博客内容未变问题_hexo 部署到gitee 已经部署了 但是博客没更新-CSDN博客</a></p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN对抗网络原理与代码实现</title>
    <url>/posts/9069b95a/</url>
    <content><![CDATA[<h4 id="GAN网络"><a href="#GAN网络" class="headerlink" title="GAN网络"></a>GAN网络</h4><p>生成对抗网络（GAN，Generative adversarial network）是一种深度学习网络，他的灵感来自与零和博弈思想。GAN其实是由两个神经网络组成，分别是生成网络Generator（下文简称为G）和判别网络Discriminator（下文简称为D）。</p>
<p>生成网络G用于生成模拟数据，判别网络D用于判别生成的数据是真是假，通过两个网络的博弈，不断使生成网络的生成数据更加逼真，导致判别网络无法判别真假，使判别网络的判断也要更加准确。这二者相互博弈，所以运用的是零和博弈思想，并且叫做生成对抗网络。</p>
<p>在一篇文章中看到作者这样比喻GAN网络。将小偷比作生成网络，警察比作判别网络。小偷只有伪装的不像小偷才能不被警察发现。最初小偷伪装的很差，总被警察发现，所以小偷不断提升自己，被警察抓住的几率变小一些，而警察发现小偷伪装技术变强也不断提高自己的业务能力。就这样，小偷和警察不断对抗提升自己，最后成为了伪装技术高超的小偷和抓捕技术高超的警察。</p>
<h4 id="GAN网络基本结构"><a href="#GAN网络基本结构" class="headerlink" title="GAN网络基本结构"></a>GAN网络基本结构</h4><p>我在网上找到这么一张图片，形象的展示了GAN的架构。</p>
<img src="/posts/9069b95a/1.png" class loading="lazy">

<p>接下来自己介绍一下生成网络和判别网络。</p>
<h5 id="生成网络Generator"><a href="#生成网络Generator" class="headerlink" title="生成网络Generator"></a>生成网络Generator</h5><p>生成网络的输入是随机数据（噪声），输出是一幅图像，往往是假图像。生成网络的模型结构可以随意，可以是简单的多层全连接神经网络，也可以是反卷积网络等。我们可以认为生成网络的输入包含着输出所要携带的信息，以MNIST手写数据集为例，输入可以表明了输出的数字为几，模糊程度等内容，因为这里对其具体信息不做要求，只希望能最大程度地骗过判别器，所以输入最好是可以满足常见分布的数据。</p>
<h5 id="判别网络Discriminator"><a href="#判别网络Discriminator" class="headerlink" title="判别网络Discriminator"></a>判别网络Discriminator</h5><p>判别网络顾名思义其作用就是进行判别。判别网络的输入既有真实图片，也有生成网络生成的假图片。判别网络的输出会是一个概率，来说明输入的图像是真实图片还是假图片，而不是判别输入图像的类别是什么，是一个二分类问题。仍然以MNIST手写数据集为例，判别网络不是判别输入的图像是数字几，而是判定这副图像是不是由生成网络产生。和生成网络一样，判别网络的模型结构可以是多层全连接网络，也可以是卷积神经网络等。</p>
<h4 id="GAN网络优化"><a href="#GAN网络优化" class="headerlink" title="GAN网络优化"></a>GAN网络优化</h4><p>我们根据上文已经知道了GAN中包含两个网络，这两个网络的作用不同，所以这两个网络的优化目标就不同，自然其损失函数也是不相同的。</p>
<p>首先看<strong>生成网络的损失函数：</strong></p>
<div>
$$
L_G=H(1,D(G(z))) \tag{1}
$$
</div>
H是指交叉熵损失函数，D是判别网络，G是生成网络，z是输入到生成网络的随机数据，也可以理解为噪声。G(z)代表生成的假图片，D(G(z))就是对生成的假图片的概率判断。1代表是真，0代表假。这里可能会问，明明生成的是假网络，label应该是0，那为什么要和1进行比较呢？我的理解是**生成网络的目标就是要生成可以迷惑判别网络的假图片，所以就要让假图片与真图片非常接近，所以你就要让网络去不断逼近真实图片，所以这里的label是1。如果label是0的话，就要最大化输出概率与0的距离才能达到生成网络的目的，而我们通常使用最小化，所以这里的label就是1**。所以生成网络的损失函数就是要让由随机噪声经过生成网络得到的假图片被判别网络判别为真图片的概率越大。

<p>接下来看<strong>判别网络的损失函数：</strong></p>
<div>
$$
L_D = H(1,D(x))+H(0,D(G(z))) \tag{2}
$$
</div>
判别网络的损失函数由两部分组成。同上，H表示交叉熵损失函数，D是判别网络，G是生成网络，x代表的是真实数据，z表示输入的随机数据。对于判别网络的损失函数，我的理解是**x是真实的图片，对应的label就是1，而G(z)是生成网络的假图片，对应的实际label为0，判别网络就是要将真实图片识别为真实图片，假图片识别为假图片，所以这里就是要最小化真实图片经过判别网络的概率和1之间的距离，以及假图片经过判别网络的概率和0之间的距离。**

<h4 id="GAN网络训练"><a href="#GAN网络训练" class="headerlink" title="GAN网络训练"></a>GAN网络训练</h4><p>知道了GAN网络的优化目标，接下来看一下GAN网络的训练过程。</p>
<p>GAN网络采用的是交替训练的策略。</p>
<p>也就是首先固定一个网络，训练另一个网络，两者交替的进行。我们默认真实图片的标签为1，假图片的标签为0。</p>
<p>对于判别网络的训练，让真实图片和假图片经过判别网络，然后计算和各自对应标签的损失就可以了。</p>
<p>对于生成网络的训练，令生成网络产生的假图片，并通过判别网络，计算判别网络输出的概率值与<strong>label为1</strong>的距离。</p>
<h4 id="PyTorch代码"><a href="#PyTorch代码" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>autograd
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> random
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>utils <span class="token keyword">import</span> save_image
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 定义一个规范化图像的函数</span>
<span class="token keyword">def</span> <span class="token function">to_img</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    out <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> x<span class="token punctuation">)</span>
    out <span class="token operator">=</span> out<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    out <span class="token operator">=</span> out<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> out

<span class="token comment"># 参数设置</span>
batch_size <span class="token operator">=</span> <span class="token number">128</span>
num_epoch <span class="token operator">=</span> <span class="token number">150</span>
z_dimension <span class="token operator">=</span> <span class="token number">100</span>

img_transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 加载数据</span>
mnist <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./dataset/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>img_transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
dataloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>mnist<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>接下来定义判别网络和生成网络，网络主题结构就采用几层全连接层。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义判别器Discriminator</span>
<span class="token keyword">class</span> <span class="token class-name">Discriminator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dis <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dis<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义生成器Generator</span>
<span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>gen <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>gen<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">D <span class="token operator">=</span> Discriminator<span class="token punctuation">(</span><span class="token punctuation">)</span>
G <span class="token operator">=</span> Generator<span class="token punctuation">(</span><span class="token punctuation">)</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    D <span class="token operator">=</span> D<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    G <span class="token operator">=</span> G<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># 定义损失函数，优化器</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 二分类的交叉熵</span>
d_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>D<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0003</span><span class="token punctuation">)</span>
g_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>G<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0003</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>最后就是对GAN的训练。</p>
<pre class="line-numbers language-none"><code class="language-none">d_loss_list &#x3D; []
g_loss_list &#x3D; []

for epoch in range(num_epoch):
    # 不要图片的标签
    for i, (img, _) in enumerate(dataloader):
        # 获取batchsize
        num_img &#x3D; img.size(0)
        # 转换为(batchsize, 784)
        img &#x3D; img.view(num_img, -1)


        real_img &#x3D; img.to(device)  # shape:  [128, 784]
        real_label &#x3D; torch.ones(num_img).to(device)
        fake_label &#x3D; torch.zeros(num_img).to(device)

        # 判别器Discrimination训练
        # 真的图片判别为真，假图片判别为假
        real_out &#x3D; D(real_img)   # shape: [128, 1]
        real_out &#x3D; real_out.squeeze()  # shape: [128,]

        # 真实图片的Loss
        d_loss_real &#x3D; criterion(real_out, real_label)
        # 真实图片的判别值,越接近1越好
        real_scores &#x3D; real_out

        # 计算假图片的损失
        # 随机生成一些噪声
        z &#x3D; torch.randn(num_img, z_dimension).to(device)  # shape: [128, 100]
        # 将噪声通过生成网络，生成假图片，避免梯度传到G中，因为G不需要更新（此时训练D）
        fake_img &#x3D; G(z).detach()
        # 判别器判别假图片
        fake_out &#x3D; D(fake_img)  # shape: [128, 1]
        fake_out &#x3D; fake_out.squeeze()  # shape: [128,]
        # 计算假图片的loss
        d_loss_fake &#x3D; criterion(fake_out, fake_label)
        # 假图片的判别值,越接近0越好
        fake_scores &#x3D; fake_out
        
        # 损失函数和优化
        # 损失包括真损失和假损失
        d_loss &#x3D; d_loss_fake + d_loss_real
        # 梯度清零
        d_optimizer.zero_grad()
        # 反向传播
        d_loss.backward()
        # 前向传播
        d_optimizer.step()

        # 生成器Generator训练
        # 希望生成的假的图片被判别器识别为真的图片
        # 将判别器固定，将假图片传入判别器的结果与真实label对应
        # 方向传播更新生成网络里的参数
        z &#x3D; torch.randn(num_img, z_dimension).to(device)
        # 得到假图片
        fake_img &#x3D; G(z)
        # 将假图片经过判别器得到的结果
        output &#x3D; D(fake_img) # shape: [128, 1]
        output &#x3D; output.squeeze()
        # 将假图片与真实图片的label计算loss
        g_loss&#x3D; criterion(output, real_label)

        g_optimizer.zero_grad()
        g_loss.backward()
        g_optimizer.step()


    d_loss_list.append(d_loss.data.item())
    g_loss_list.append(g_loss.data.item())
        
    print(&#39;Epoch[&#123;&#125;&#x2F;&#123;&#125;],d_loss:&#123;:.6f&#125;,g_loss:&#123;:.6f&#125; &#39;
            &#39;D_real_output: &#123;:.6f&#125;,D_fake_output: &#123;:.6f&#125;&#39;.format(
        epoch+1, num_epoch, d_loss.data.item(), g_loss.data.item(),
        real_scores.data.mean(), fake_scores.data.mean()  # 打印的是真实图片的损失均值
    ))

    if epoch &#x3D;&#x3D; 0:
        real_images &#x3D; to_img(real_img.cpu().data)
        save_image(real_images, &#39;.&#x2F;img&#x2F;real_images.png&#39;)
    elif epoch % 10 &#x3D;&#x3D; 0:
        fake_images &#x3D; to_img(fake_img.cpu().data)
        save_image(fake_images, &#39;.&#x2F;img&#x2F;fake_images-&#123;&#125;.png&#39;.format(epoch + 1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>由于能力有限，代码参考了参考文献中的第五篇文章。并在其基础中将一些pytorch中已经弃用的函数进行了修改。并在其他地方进行了小部分调整。</p>
<p>看看生成的图像，这是原图。</p>
<img src="/posts/9069b95a/2.png" class loading="lazy">

<p>这是训练10轮后的图像</p>
<img src="/posts/9069b95a/3.png" class loading="lazy">

<p>这是训练50轮后的图像</p>
<img src="/posts/9069b95a/4.png" class loading="lazy">

<p>这是训练140轮后的图像</p>
<img src="/posts/9069b95a/5.png" class loading="lazy">



<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p><a href="https://easyai.tech/ai-definition/gan/">一文看懂「生成对抗网络 - GAN」基本原理+10种典型算法+13种应用 (easyai.tech)</a></p>
<p><a href="https://blog.csdn.net/LEEANG121/article/details/104113406">深度学习：GAN 对抗网络原理详细解析（零基础必看）_gan网络-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/Sakura55/article/details/81512600">深度学习—-GAN（生成对抗神经网络）原理解析_gan神经网络-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/DFCED/article/details/105175097">图解 生成对抗网络GAN 原理 超详解_gan原理图-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/jizhidexiaoming/article/details/96485095">pytorch实现GAN-CSDN博客</a></p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>生成对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+GitHub Pages搭建的个人博客加入到Bing搜索引擎</title>
    <url>/posts/7cddb87d/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>搭建好个人博客以后，需要将自己的博客网站加入到搜索引擎中，自己在查找资料的过程中，找到一篇加入到Bing的文章。按照这个方法最后成功了，但是原文章作者省略了几个点，导致自己在设置的时候有点纠结，所以在本文章中完善一下这几点。看到本文的读者可以根据原作者的步骤以及本文中提到的原作者省略的设置细节完成自己博客的设置。</p>
<h4 id="原文循迹"><a href="#原文循迹" class="headerlink" title="原文循迹"></a>原文循迹</h4><p><a href="https://www.cnblogs.com/RainbowC0/p/18107581">将 Github Pages 个人博客录入搜索引擎（以 Bing 为例） - RainbowC0 - 博客园 (cnblogs.com)</a></p>
<p>虽然原文作者是将Gitee.io，但是和Github.io加入是一致的。</p>
<h4 id="补充细节"><a href="#补充细节" class="headerlink" title="补充细节"></a>补充细节</h4><h5 id="步骤三：验证网站"><a href="#步骤三：验证网站" class="headerlink" title="步骤三：验证网站"></a>步骤三：验证网站</h5><p>原文中选择了XML File进行验证，下载BingSiteAuth.xml文件后，需要放到目录中。</p>
<p>那么这个根目录是哪里呢？</p>
<p>其实就是public文件夹下，我直接将BingSiteAuth.xml文件黏贴到public文件夹下，Git中hexo clean &amp;&amp; hexo g &amp;&amp; hexo d。但是当我去github的仓库中看时，仓库中并没有出现BingSiteAuth.xml文件。这是为什么呢？</p>
<p>因为 Hexo 的生成流程不会保留 <code>public</code> 目录下非由 Hexo 生成的原始文件。<code>hexo g</code> 命令会清理 <code>public</code> 目录并重新生成所有内容，但只包括从 <code>source</code> 目录和主题中通过 Hexo 处理的文件。</p>
<p>所以我的办法是先运行hexo clean &amp;&amp; hexo g之后，再将BingSiteAuth.xml黏贴到public下，之后再hexo d。这是github的仓库中就顺利出现了这个文件。</p>
<p>网上我也搜索到了另一种方法，就是直接将BingSiteAuth.xml文件直接加入到github的仓库中，省去hexo c,g,d的操作。(未试验过)</p>
<h5 id="步骤四：添加网站地图"><a href="#步骤四：添加网站地图" class="headerlink" title="步骤四：添加网站地图"></a>步骤四：添加网站地图</h5><p>原文中提到需要将sitemap.xml文件防到根目录下，但是原文只说了Hugo会自带，那么使用Hexo该怎么办呢？</p>
<p>首先在Git中输入：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">npm install hexo<span class="token operator">-</span>generator<span class="token operator">-</span>sitemap <span class="token operator">--</span>save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>该插件可以帮助Hexo自动生成sitemap.xml文件。</p>
<p>然后在站点配置文件_config.yml文件中输入：</p>
<pre class="line-numbers language-none"><code class="language-none"># Sitemap Generator  
sitemap:  
  path: sitemap.xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>之后在Git中使用</p>
<pre class="line-numbers language-none"><code class="language-none">hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>就可以看到在public中生成了sitemap.xml文件了。</p>
<p>然后再通过部署到仓库，就可以在网站地图中完成网站地图的提交。</p>
<h4 id="验证相关文件"><a href="#验证相关文件" class="headerlink" title="验证相关文件"></a>验证相关文件</h4><p>怎么知道sitemap.xml和BingSiteAuth.xml是否可以被搜索到呢？</p>
<p>可以自己直接搜索username.github.io&#x2F;BingSiteAuth.xml和username.github.io&#x2F;sitemap.xml，username就是你的仓库用户名。如果可以看到xml文件中的内容就是上传到仓库成功。</p>
<p>可能需要等一会才能搜索到。</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo-GitHub Pages解决上传文章图片不显示的问题</title>
    <url>/posts/95fa3d73/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>在自己搭建的网站写好文章以后，上传到GitHub上发现文章中的图片无法显示。这个问题改了很长时间，所以在这里记录最后有效的方法。</p>
<h4 id="问题表现"><a href="#问题表现" class="headerlink" title="问题表现"></a>问题表现</h4><img src="/posts/95fa3d73/1.png" class loading="lazy">

<p>自己在Typora中黏贴进去的图片，无论是预览还是部署以后都无法显示，上图为预览中的样子。</p>
<p>当我部署在GitHub上我在控制台中查看图片的位置，竟然是在wsj.github.io&#x2F;1.png，图片的位置跑到了根目录下，显然路径位置不对。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><h5 id="插件安装与配置"><a href="#插件安装与配置" class="headerlink" title="插件安装与配置"></a>插件安装与配置</h5><p>首先使用一个图片路径转换的插件hexo-asset-image，安装指令如下：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c">npm install https<span class="token operator">:</span><span class="token comment">//github.com/CodeFalling/hexo-asset-image --save</span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">npm</span> <span class="token expression">install hexo<span class="token operator">-</span>asset<span class="token operator">-</span>image <span class="token operator">--</span>save</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>安装好之后，需要打开node_modules&#x2F;hexo-asset-image&#x2F;index.js，将其中的代码更换，如下：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token char">'use strict'</span><span class="token punctuation">;</span>
var cheerio <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token char">'cheerio'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span>
function <span class="token function">getPosition</span><span class="token punctuation">(</span>str<span class="token punctuation">,</span> m<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
  <span class="token keyword">return</span> str<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">.</span>length<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

var version <span class="token operator">=</span> <span class="token function">String</span><span class="token punctuation">(</span>hexo<span class="token punctuation">.</span>version<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token char">'.'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
hexo<span class="token punctuation">.</span>extend<span class="token punctuation">.</span>filter<span class="token punctuation">.</span><span class="token keyword">register</span><span class="token punctuation">(</span><span class="token char">'after_post_render'</span><span class="token punctuation">,</span> <span class="token function">function</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
  var config <span class="token operator">=</span> hexo<span class="token punctuation">.</span>config<span class="token punctuation">;</span>
  <span class="token keyword">if</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>post_asset_folder<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
    	var link <span class="token operator">=</span> data<span class="token punctuation">.</span>permalink<span class="token punctuation">;</span>
	<span class="token keyword">if</span><span class="token punctuation">(</span>version<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> <span class="token function">Number</span><span class="token punctuation">(</span>version<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">)</span>
	   var beginPos <span class="token operator">=</span> <span class="token function">getPosition</span><span class="token punctuation">(</span>link<span class="token punctuation">,</span> <span class="token char">'/'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
	<span class="token keyword">else</span>
	   var beginPos <span class="token operator">=</span> <span class="token function">getPosition</span><span class="token punctuation">(</span>link<span class="token punctuation">,</span> <span class="token char">'/'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
	<span class="token comment">// In hexo 3.1.1, the permalink of "about" page is like ".../about/index.html".</span>
	var endPos <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">lastIndexOf</span><span class="token punctuation">(</span><span class="token char">'/'</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
    link <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span>beginPos<span class="token punctuation">,</span> endPos<span class="token punctuation">)</span><span class="token punctuation">;</span>

    var toprocess <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token char">'excerpt'</span><span class="token punctuation">,</span> <span class="token char">'more'</span><span class="token punctuation">,</span> <span class="token char">'content'</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span>var i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> toprocess<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
      var key <span class="token operator">=</span> toprocess<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 
      var $ <span class="token operator">=</span> cheerio<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>
        ignoreWhitespace<span class="token operator">:</span> false<span class="token punctuation">,</span>
        xmlMode<span class="token operator">:</span> false<span class="token punctuation">,</span>
        lowerCaseTags<span class="token operator">:</span> false<span class="token punctuation">,</span>
        decodeEntities<span class="token operator">:</span> false
      <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

      $<span class="token punctuation">(</span><span class="token char">'img'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">each</span><span class="token punctuation">(</span><span class="token function">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
		<span class="token keyword">if</span> <span class="token punctuation">(</span>$<span class="token punctuation">(</span>this<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token char">'src'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
			<span class="token comment">// For windows style path, we replace '\' to '/'.</span>
			var src <span class="token operator">=</span> $<span class="token punctuation">(</span>this<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token char">'src'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token char">'\\'</span><span class="token punctuation">,</span> <span class="token char">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token operator">/</span>http<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token operator">|</span>\<span class="token operator">/</span>\<span class="token operator">/</span><span class="token punctuation">.</span><span class="token operator">*</span><span class="token operator">/</span><span class="token punctuation">.</span><span class="token function">test</span><span class="token punctuation">(</span>src<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span>
			   <span class="token operator">!</span><span class="token operator">/</span><span class="token operator">^</span>\s<span class="token operator">*</span>\<span class="token comment">//.test(src)) &#123;</span>
			  <span class="token comment">// For "about" page, the first part of "src" can't be removed.</span>
			  <span class="token comment">// In addition, to support multi-level local directory.</span>
			  var linkArray <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token char">'/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span><span class="token function">function</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
				<span class="token keyword">return</span> elem <span class="token operator">!=</span> <span class="token char">''</span><span class="token punctuation">;</span>
			  <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			  var srcArray <span class="token operator">=</span> src<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token char">'/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span><span class="token function">function</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
				<span class="token keyword">return</span> elem <span class="token operator">!=</span> <span class="token char">''</span> <span class="token operator">&amp;&amp;</span> elem <span class="token operator">!=</span> <span class="token char">'.'</span><span class="token punctuation">;</span>
			  <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			  <span class="token keyword">if</span><span class="token punctuation">(</span>srcArray<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">)</span>
				srcArray<span class="token punctuation">.</span><span class="token function">shift</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			  src <span class="token operator">=</span> srcArray<span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span><span class="token char">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			  $<span class="token punctuation">(</span>this<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token char">'src'</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span>root <span class="token operator">+</span> link <span class="token operator">+</span> src<span class="token punctuation">)</span><span class="token punctuation">;</span>
			  console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"update link as:-->"</span><span class="token operator">+</span>config<span class="token punctuation">.</span>root <span class="token operator">+</span> link <span class="token operator">+</span> src<span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token punctuation">&#125;</span>
		<span class="token punctuation">&#125;</span><span class="token keyword">else</span><span class="token punctuation">&#123;</span>
			console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"no src attr, skipped..."</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span>$<span class="token punctuation">(</span>this<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
      <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      data<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> $<span class="token punctuation">.</span><span class="token function">html</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>之后，打开站点配置文件_config.yml，修改：</p>
<pre class="line-numbers language-none"><code class="language-none">post_asset_folder: true<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这个指令的作用就是当你使用hexo new “article name”创建一篇新的文章时，可以自动在_post中生成一个与article name同名的文件夹，用于存放图片。当然，手动创建article name.md和article name文件夹也是可以的。<strong>注：但是对于解决文本的问题，post_asset_folder的设置是必须的，因为上文提到的插件，必须在post_asset_folder为true的情况下使用。</strong></p>
<h5 id="Typora图像设置-可选"><a href="#Typora图像设置-可选" class="headerlink" title="Typora图像设置(可选)"></a>Typora图像设置(可选)</h5><p>设置了post_asset_folder后，就意味着文章内容在article name.md中完成，而文章中涉及的图片就要存储在article name文件夹中，一张张复制甚至重命名显然是麻烦的。所以可以在Typora中点击文件-&gt;偏好设置，点击图像，选择复制到指定路径。</p>
<img src="/posts/95fa3d73/2.png" class loading="lazy">

<p><code>./</code>表示当前文件夹，<code>$&#123;filename&#125;</code>表示当前文件名。通过这样设置说明图像如果黏贴到Typora后，就可以自动复制到article name文件夹中，免去自己复制。</p>
<h5 id="文章使用该插件"><a href="#文章使用该插件" class="headerlink" title="文章使用该插件"></a>文章使用该插件</h5><p>该插件的作者表示如图所示：</p>
<img src="/posts/95fa3d73/3.png" class loading="lazy">

<p>引用图片只是文件名，而不能有其他路径，但是我按照这种方法还是不能显示方法。</p>
<img src="/posts/95fa3d73/4.png" class loading="lazy">按照这种方法，仍然错误<img src="/posts/95fa3d73/5.png" class loading="lazy">

<p>但是，我在网上看到了另一种引用图片方法，即使用标签插件，如：</p>
<pre class="line-numbers language-c#" data-language="c#"><code class="language-c#">&#123;% asset_img img.png %&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>比如你的图片命名为img.png，那么在Typora中。你就应该用表示图片。图片得以正常显示。</p>
<h4 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h4><p><a href="https://blog.csdn.net/m0_43401436/article/details/107191688">hexo博客中插入图片失败——解决思路及个人最终解决办法_hexo 文章插入图片失败-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/xjm850552586/article/details/84101345">hexo引用本地图片无法显示-CSDN博客</a></p>
]]></content>
      <categories>
        <category>解问题</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
  <entry>
    <title>MCNN--论文阅读与源码理解</title>
    <url>/posts/8a129f29/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><em>前言</em></h3><p>最近看了一篇人群计数的论文，并进行了复现，主要是用于自己去了解人群计数的整个流程。<br>本文不是对论文进行翻译，只是对文章进行简略的总结，并对部分内容写出我的理解，主要是将自己阅读和复现过程中遇到的些问题在这里记录一下，希望可以帮助到有需要的人，同时，本文的内容均为个人理解，如若有误，欢迎在评论区或者邮件指正。</p>
<p>论文原文： <a href="https://ieeexplore.ieee.org/document/7780439">Single-Image Crowd Counting via Multi-Column Convolutional Neural Network | IEEE Conference Publication | IEEE Xplore</a></p>
<p>源代码 ：<a href="https://gitcode.com/gh_mirrors/cr/crowdcount-mcnn/overview?utm_source=csdn_github_accelerator&isLogin=1">crowdcount-mcnn:Single Image Crowd Counting via MCNN (Unofficial Implementation) - GitCode</a></p>
<p><a href="https://github.com/svishwa/crowdcount-mcnn">svishwa&#x2F;crowdcount-mcnn: Single Image Crowd Counting via MCNN (Unofficial Implementation) (github.com)</a></p>
<h3 id="环境调试"><a href="#环境调试" class="headerlink" title="环境调试"></a><em>环境调试</em></h3><p>可以下载作者开源的代码，但是作者当时上传的代码有些老，所以会有很多不兼容的问题需要自己一步一步去调试。<br>在文章[<a href="https://blog.csdn.net/wpw5499/article/details/106231707">MCNN] Crowd Counting 人群计数 复现过程记录_mcnn复现-CSDN博客</a>中对调试的内容写的非常详细，所以本文在此不赘述。<br>我将自己调试好的程序上传到了Github，大家也可以自行下载<a href="https://github.com/LengNian/PaperRecurrent/tree/main/crowdcount-mcnn-master">PaperRecurrent&#x2F;crowdcount-mcnn-master at main · LengNian&#x2F;PaperRecurrent (github.com)</a></p>
<h3 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a><em>读论文</em></h3><p>该论文提出一种简单但是有效的多列卷积神经网络(MCNN)，每列的感受野大小不同，用于提取不同的特征。同时，提出了Shanghai Tech数据集。<br>MCNN输入的是图像，会输出对应的人群密度图，通过对该密度图进行积分，就可以得到对应的人群数量。<br>因为透视失真的问题，简单的来说就是图像中近大远小的问题，会导致图像中人群的头部大小各异，所以使用不同尺度的卷积核才能捕捉到不同尺寸的人群密度特征。MCNN结构图如图所示。</p>
<img src="/posts/8a129f29/1.png" class loading="lazy">

<p>作者在训练的时候对数据进行了增强，对于训练集数据，作者使用matlab对每张图片随机裁剪9次，论文中提到对训练样本进行下采样1&#x2F;4，这里要注意的是，是对图像的长宽缩减到原来的1&#x2F;4，而不是面积，面积实际缩减为了原来的1&#x2F;16。如图所示。</p>
<img src="/posts/8a129f29/2.png" class loading="lazy">

<p>全篇文章中，到时候最想不明白的就是3.2 Shanghai Tech dataset中的Comparison of different loss functions。这里讲了两种不同的方方式，一种是将图像直接映射到图像中的总人头数(我的理解就是其输出内容就为总人头数)，另一种是将图像映射为密度图(也就是模型的输出就是与输入对应的密度图)。</p>
<div>
$$
L(\Theta) = \frac{1}{2N} \sum_{i=1}^{N}{ \lVert \iint_S F(X_i;\Theta)dxdy - z_i \lVert}^2  \tag{1}
$$
</div>

<p>对于公式(1)，F(X，Θ)就是模型所输出的密度图，通过一个二重积分得到的就是该密度图上的总人数，$ z_i$表示的就是第i个图像中的总人数，通过将预测出来的总人数减去实际总人数，并平方等步骤计算损失函数。</p>
<h3 id="源码理解"><a href="#源码理解" class="headerlink" title="源码理解"></a><em>源码理解</em></h3><p>这部分，我会记录一些我在读源码时一些难理解的点。</p>
<p>第一处是data_loader.py中对数据预处理的部分，即if self.preload:中的部分</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">if self.gt_downsample:
    wd_1 &#x3D; wd_1&#x2F;&#x2F;4
    ht_1 &#x3D; ht_1&#x2F;&#x2F;4
    den &#x3D; cv2.resize(den,(wd_1,ht_1))                
    den &#x3D; den * ((wd*ht)&#x2F;(wd_1*ht_1))
else:
    den &#x3D; cv2.resize(den,(wd_1,ht_1))
    den &#x3D; den * ((wd*ht)&#x2F;(wd_1*ht_1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>我当时最不理解的就是 den &#x3D; den * ((wd<em>ht)&#x2F;(wd_1</em>ht_1))这句代码，这里是对密度图的值进行了改变。</p>
<p><strong>密度图中的每个像素值通常代表了一定区域内的人群密度，对密度图的尺寸修改后，会影响到整个图像所表示的总密度。为了补偿这种影响，需要调整新图像中每个像素的值，使其所代表的总人群密度与原始图像相同，常用方法就是乘一个比例因子，这个比例因子就等于原始图像面积 &#x2F; 新图像面积。</strong></p>
<p>举个例子，比如有一张图像大小为10 * 10，表示100个人，相当于100个像素表表示100个人，平均下来就是每个像素代表一个人。当对图像缩小2倍，即5 * 5，此时相当于25像素表示100个人，那么平均下来就是一个像素四个人，很明显如果继续按照原来的密度图进行计算，会出错，此时乘一个比例因子，即100&#x2F;25&#x3D;4，25*4&#x3D;100。由此可见通过比例因子的调整，大小修改后的密度图表示的内容与之前是相同的。</p>
<p>第二处是crowd_count.py中的内容</p>
<pre class="line-numbers language-none"><code class="language-none">class CrowdCounter(nn.Module):
    def __init__(self):
        super(CrowdCounter, self).__init__()        
        self.DME &#x3D; MCNN()        
        self.loss_fn &#x3D; nn.MSELoss()

    @property
    def loss(self):
        return self.loss_mse
    
    def forward(self,  im_data, gt_data&#x3D;None):        
        im_data &#x3D; src.network.np_to_variable(im_data, is_cuda&#x3D;True, is_training&#x3D;self.training)
        density_map &#x3D; self.DME(im_data)
        
        if self.training:                        
            gt_data &#x3D; src.network.np_to_variable(gt_data, is_cuda&#x3D;True, is_training&#x3D;self.training)
            self.loss_mse &#x3D; self.build_loss(density_map, gt_data)
            
        return density_map<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里使用了property装饰器, 其装饰的函数的返回值可以直接被调用, 这也是为什么虽然__init__中虽没有定义self.loss_mse, 但是为什么后面可以调用。</p>
<p>第三处是data_preparation&#x2F;create_training_set_shetch.m中的内容。</p>
<p>论文中说了对训练集进行增强的过程就是将每张图片随机截取9张原图四分之一的图像作为训练集。在代码中，这里是对长款取了八分之一，我的理解这个八分之一的长宽其实就是属于从中间往左边截八分之一，从中间往右边截八分之一，这样合起来就相当于是截取了四分之一。下图是我所理解的取样四分之一。</p>
<img src="/posts/8a129f29/3.png" class loading="lazy">

<p>相应的代码如下</p>
<pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">for</span> idx <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>num_images
    <span class="token number">i</span> <span class="token operator">=</span> <span class="token function">indices</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">mod</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token function">fprintf</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'Processing %3d/%d files\n'</span><span class="token punctuation">,</span> idx<span class="token punctuation">,</span> num_images<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">end</span>
    <span class="token function">load</span><span class="token punctuation">(</span><span class="token function">strcat</span><span class="token punctuation">(</span>gt_path<span class="token punctuation">,</span> <span class="token string">'GT_IMG_'</span><span class="token punctuation">,</span><span class="token function">num2str</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'.mat'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">;</span>
    input_img_name <span class="token operator">=</span> <span class="token function">strcat</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span><span class="token string">'IMG_'</span><span class="token punctuation">,</span><span class="token function">num2str</span><span class="token punctuation">(</span><span class="token number">i</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    im <span class="token operator">=</span> <span class="token function">imread</span><span class="token punctuation">(</span>input_img_name<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">[</span>h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> c<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>im<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>c <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">)</span>
        im <span class="token operator">=</span> <span class="token function">rgb2gray</span><span class="token punctuation">(</span>im<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">end</span>


    wn2 <span class="token operator">=</span> w<span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">;</span> hn2 <span class="token operator">=</span> h<span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">;</span>
    wn2 <span class="token operator">=</span><span class="token number">8</span> <span class="token operator">*</span> <span class="token function">floor</span><span class="token punctuation">(</span>wn2<span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    hn2 <span class="token operator">=</span><span class="token number">8</span> <span class="token operator">*</span> <span class="token function">floor</span><span class="token punctuation">(</span>hn2<span class="token operator">/</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span>


    annPoints <span class="token operator">=</span>  image_info<span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">&#125;</span><span class="token punctuation">.</span>location<span class="token punctuation">;</span>


    <span class="token keyword">if</span><span class="token punctuation">(</span> w <span class="token operator">&lt;=</span> <span class="token number">2</span><span class="token operator">*</span>wn2 <span class="token punctuation">)</span>
        im <span class="token operator">=</span> <span class="token function">imresize</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span><span class="token punctuation">[</span> h<span class="token punctuation">,</span><span class="token number">2</span><span class="token operator">*</span>wn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>wn2<span class="token operator">/</span>w<span class="token punctuation">;</span>
    <span class="token keyword">end</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span> h <span class="token operator">&lt;=</span> <span class="token number">2</span><span class="token operator">*</span>hn2<span class="token punctuation">)</span>
        im <span class="token operator">=</span> <span class="token function">imresize</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">*</span>hn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>hn2<span class="token operator">/</span>h<span class="token punctuation">;</span>
    <span class="token keyword">end</span>

    <span class="token punctuation">[</span>h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> c<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">size</span><span class="token punctuation">(</span>im<span class="token punctuation">)</span><span class="token punctuation">;</span>
    a_w <span class="token operator">=</span> wn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span> b_w <span class="token operator">=</span> w <span class="token operator">-</span> wn2<span class="token punctuation">;</span>
    a_h <span class="token operator">=</span> hn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">;</span> b_h <span class="token operator">=</span> h <span class="token operator">-</span> hn2<span class="token punctuation">;</span>
    
    im_density <span class="token operator">=</span> <span class="token function">get_density_map_gaussian</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span>annPoints<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token number">j</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">:</span>N
 
        x <span class="token operator">=</span> <span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>b_w <span class="token operator">-</span> a_w<span class="token punctuation">)</span> <span class="token operator">*</span> rand <span class="token operator">+</span> a_w<span class="token punctuation">)</span><span class="token punctuation">;</span>
        y <span class="token operator">=</span> <span class="token function">floor</span><span class="token punctuation">(</span><span class="token punctuation">(</span>b_h <span class="token operator">-</span> a_h<span class="token punctuation">)</span> <span class="token operator">*</span> rand <span class="token operator">+</span> a_h<span class="token punctuation">)</span><span class="token punctuation">;</span>

        x1 <span class="token operator">=</span> x <span class="token operator">-</span> wn2<span class="token punctuation">;</span> y1 <span class="token operator">=</span> y <span class="token operator">-</span> hn2<span class="token punctuation">;</span>
        x2 <span class="token operator">=</span> x <span class="token operator">+</span> wn2<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span> y2 <span class="token operator">=</span> y <span class="token operator">+</span> hn2<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>
        
        
        im_sampled <span class="token operator">=</span> <span class="token function">im</span><span class="token punctuation">(</span>y1<span class="token operator">:</span>y2<span class="token punctuation">,</span> x1<span class="token operator">:</span>x2<span class="token punctuation">,</span><span class="token operator">:</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        im_density_sampled <span class="token operator">=</span> <span class="token function">im_density</span><span class="token punctuation">(</span>y1<span class="token operator">:</span>y2<span class="token punctuation">,</span>x1<span class="token operator">:</span>x2<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>但是这段代码中我还有一处不理解的就是</p>
<pre class="line-numbers language-matlab" data-language="matlab"><code class="language-matlab"><span class="token keyword">if</span><span class="token punctuation">(</span> w <span class="token operator">&lt;=</span> <span class="token number">2</span><span class="token operator">*</span>wn2 <span class="token punctuation">)</span>
     im <span class="token operator">=</span> <span class="token function">imresize</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span><span class="token punctuation">[</span> h<span class="token punctuation">,</span><span class="token number">2</span><span class="token operator">*</span>wn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>wn2<span class="token operator">/</span>w<span class="token punctuation">;</span>
 <span class="token keyword">end</span>
 <span class="token keyword">if</span><span class="token punctuation">(</span> h <span class="token operator">&lt;=</span> <span class="token number">2</span><span class="token operator">*</span>hn2<span class="token punctuation">)</span>
     im <span class="token operator">=</span> <span class="token function">imresize</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">*</span>hn2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>w<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">=</span> <span class="token function">annPoints</span><span class="token punctuation">(</span><span class="token operator">:</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>hn2<span class="token operator">/</span>h<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>w不应该是恒大于2*wn2的嘛，因为wn2是w&#x2F;8，即使有w特别小的情况，wn2被下取整变为了0，所以我对这里的判断不是很理解。</p>
<p>以上就是阅读MCNN论文中的一些记录，如果大家发现什么问题欢迎在评论区指正或者发邮件交流。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><em>参考文献</em></h3><p>[<a href="https://blog.csdn.net/wpw5499/article/details/106231707">MCNN] Crowd Counting 人群计数 复现过程记录_mcnn复现-CSDN博客</a></p>
<p><a href="https://www.jianshu.com/p/a1006c4b6fdc">https://www.jianshu.com/p/a1006c4b6fdc</a></p>
]]></content>
      <categories>
        <category>人群计数</category>
        <category>论文复现</category>
        <category>目标计数</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗攻击:CW算法</title>
    <url>/posts/ffb6a1cc/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><h4 id="CW原理"><a href="#CW原理" class="headerlink" title="CW原理"></a>CW原理</h4><p>CW是一种基于优化的攻击，它同时兼顾高攻击准确率和低对抗扰动两个方面。在论文<a href="https://arxiv.org/pdf/1608.04644">1608.04644 (arxiv.org)</a>中被提出。</p>
<p>在优化过程中需要完成两个目标，一是对抗样本和对应的干净样本的差距应该越小越好；二是对抗样本应该使得模型分类错，且错的那一类的概率越高越好。</p>
<p>最初，CW攻击依赖于对抗性实例的初始表述：</p>
<div>
$$
minimize \quad D(x,x+\delta) \\
such\;that \quad C(x+\delta)=t \\
\qquad x+\delta \in [0,1]^n
\tag{1}
$$
</div>
D是度量对抗样本和干净样本的度量函数，可以选择不同的范数距离(本文主要讲解L2范数)。C表示分类器，t为期望对抗样本被分类成的标签。

<p>上述公式中的C(x+δ)&#x3D;t是非线性的，现有的算法难以解决，想要求解需要选择更适合优化的表达式。所以论文作者定义了一系列的目标函数f当且仅当f(x+δ)≤0时，C(x+δ)&#x3D;t。</p>
<p>所以，优化目标可以修改为如下：</p>
<div>
$$
minimize \quad D(x,x+\delta) \\
such\;that \quad f(x+\delta) \leq 0 \\
\qquad x+\delta \in [0,1]^n
\tag{2}
$$
</div>
目标函数论文中列举了七种，这里只列举部分：
<div>
$$
f_1(x')=-loss_{F,t}(x')+1 \\
f_2(x')=(\underset{i \neq t}{max}(F(x')_i)-F(x')_t)^+ \\
...... \\
f_6(x')=(\underset{i \neq t}{max}(Z(x')_i)-Z(x')_t)^+ \\
......
\tag{3}
$$
</div>
论文作者通过添加常量来调整上面一些公式，目的是为了使函数符合论文中的定义，所以公式(2)就可以转换成以下可求解的新的优化形式。
<div>
$$
minimize \quad D(x,x+\delta)+c \; \cdot \; f(x+\delta)  \\
such\;that \quad x+\delta \in [0,1]^n \\
\tag{4}
$$
</div>
使用范数去实例化距离度量函数，问题就变为了：
<div>
$$
minimize \parallel \delta \parallel_p + c \; \cdot \; f(x+\delta) \\
such \; that \quad x+\delta \in [0,1]^n 
\tag{5}
$$
</div>
为了保证对抗样本是一个有效的图像，必须对扰动δ进行一个约束。如果不进行约束，很容易超出范围，论文中提到了三种方法：

<ul>
<li><p>梯度投影下降：</p>
</li>
<li><p>梯度截断下降：这种方法没有真正意义上的截断样本，而是直接把约束加入到了目标函数f中，将f(x+δ)使用f(min(max(x+δ,0),1))来代替，但是这样也存在梯度消失的问题。</p>
</li>
<li><p>变量引入：通过引入变量w使用x+δ满足约束，此时对抗样本就表示为：</p>
<div>
$$
x_i + \delta_i = \frac{1}{2}(tan(w_i)+1) \tag{6}
$$
</div>
由于tanh的取值范围为[-1,1]，经过这种约束方法，x+δ的取值范围变成了[0,1]，很好的满足了上述所提到的要求。
下面就可以开始进行攻击了，在这里主要使用L2范数。此时公式就变为了：
<div>
$$
minimize \parallel \frac{1}{2}(tanh(w)+1) - x \parallel_2^2 + \; c \; \cdot \; f(\frac{1}{2}(tanh(w)+1)) \\
f(x')=max(max \{ Z(x')_i:i \neq t \}-Z(x')_t, -k)
\tag{7}
$$
</div>
此时，令
<div>
$$
R_n=\frac{1}{2}(tanh(w_n)+1) - X_n  \tag{8}
$$
</div>
这里的Rn表示干净样本和对抗样本的差值，这里如果使用tanh，x的范围就是[0,1]。而加入tanh映射空间后，就扩大到了-inf和inf之间。</li>
</ul>
<p>然后主要说一下目标函数，这里选的是上述论文列出来的第六个。Z(x)代表的就是输入图像还未经过softmax激活函数的向量，这个向量的最大值所对应的标号就是即将被预测的类别。t是希望生成的对抗样本所属于的类别。</p>
<p>max{Z(x’)i，i≠t}表示的就是Z(x)中除去t这个类别以外的最大值，也就是除去t类别以外，最可能将x分类的那个类别的值，将max{Z(x)i,i≠t}-Z(x’)t就是指除去t类别以外最有可能被分类的类别所对应的在未经过softmax激活函数前的向量的最大值和对应t类的的最大值的差。将这个差最小化也就使对抗样本离变成t类越接近。</p>
<p>从(6)可以看到还有一个参数k，这个参数就是置信度。这里可以理解为k越大，置信度越大。k是一个概率，在0~1之间。</p>
<p>为了方便起，对(6)中第一行公式编号为(9)，第二行编号为(10)，此时：</p>
<div>
$$
minimize \parallel R_n \parallel_2^2 + \; c \; \cdot \; f(\frac{1}{2}(tanh(w)+1)) \tag{9}
$$
</div>
<div>
$$
f(x')=max(max \{ Z(x')_i:i \neq t \}-Z(x')_t, -k)  \tag{10}
$$
</div>
如果f(x')的值为max{Z(x')i,i≠t}-Z(x')t,那么(8)中的第二项就是c*max{Z(x')i,i≠t}-Z(x')t，只需要不断减小就可以。

<p>但是如果f(x’)的值为-k，那么(8)中第二项就变为-ck。k越大，-ck就越小，优化过程就会更倾向于(8)中的第一项，即平方误差那项，就可能会导致生成的对抗样本更接近我们希望它变成的那个类别。</p>
<h4 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h4><p>这里包含了目标函数f和计算优化目标两部分的内容。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">def cw_l2_attack(model, images, labels, targeted&#x3D;False, c&#x3D;8, kappa&#x3D;0, max_iter&#x3D;1000, learning_rate&#x3D;0.01):

    # 定义目标函数f
    def f(x):
        # 论文中的 Z(X) 输出 batchsize, num_classes
        outputs &#x3D; model(x)
        # 就是一个one-hot编码,然后预测类别为1,其余为0，共有batch_size行，然后每行根据label置为0或1
        one_hot_labels &#x3D; torch.eye(len(outputs[0])).to(device)[labels]
        # 1-one_hot_labels就是字面意思，原来是0的，就是1-0&#x3D;1，原来是1的就是1-1&#x3D;0，经过处理后除了真实类别以外的都是1
        # 水平方向最大的取值，忽略索引。意思是，除去真实标签，看看每个 batchsize 中哪个标签的概率最大，取出概率
        # 概率值, 标号
        i, _ &#x3D; torch.max((1 - one_hot_labels) * outputs, dim&#x3D;1)
        # 选择真实标签的概率
        j &#x3D; torch.masked_select(outputs, one_hot_labels.bool())

        # 如果有攻击目标，虚假概率减去真实概率，
        if targeted:
            # 如果 i - j 的计算结果小于 -kappa，那么使用 -kappa 作为结果，而不是原始值。
            return torch.clamp(i - j, min&#x3D;-kappa)
        # 没有攻击目标，就让真实的概率小于虚假的概率，逐步降低，也就是最小化这个损失
        else:
            return torch.clamp(j - i, min&#x3D;-kappa)
        

    w &#x3D; torch.zeros_like(images, requires_grad&#x3D;True).to(device)
    optimizer &#x3D; torch.optim.Adam([w], lr&#x3D;learning_rate)
    
    prev &#x3D; 1e10

    for step in range(max_iter):
        a &#x3D; 1 &#x2F; 2 * (nn.Tanh()(w) + 1)
        # 最小化目标的两个部分
        # 第一个目标，对抗样本与原始样本足够接近
        loss1 &#x3D; nn.MSELoss(reduction&#x3D;&#39;sum&#39;)(a, images)
        # 第二个目标，误导模型输出
        loss2 &#x3D; torch.sum(c * f(a))

        cost &#x3D; loss1 + loss2
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        if step % (max_iter &#x2F;&#x2F; 10) &#x3D;&#x3D; 0:
            if cost &gt; prev:
                print(&#39;Attack Stopped due to CONVERGENCE....&#39;)
                return a
            prev &#x3D; cost

    attack_images &#x3D; 1 &#x2F; 2 * (nn.Tanh()(w) + 1)

    return attack_images<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="训练-攻击完整代码"><a href="#训练-攻击完整代码" class="headerlink" title="训练+攻击完整代码"></a>训练+攻击完整代码</h4><p>还是和之前一样，使用自己在MNIST手写数据集上训练的LeNet模型来测试CW算法产生的对抗样本。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">import torch
import numpy as np
import torchvision
import random
import copy
from tqdm import tqdm
import torch.nn as nn
from torchvision import datasets, transforms
from torchsummary import summary
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.utils.data import DataLoader


# 搭建LeNet模型
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()

        # 卷积层
        self.conv &#x3D; nn.Sequential(
            nn.Conv2d(in_channels&#x3D;1, out_channels&#x3D;6, kernel_size&#x3D;5, padding&#x3D;2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2),
            nn.Conv2d(in_channels&#x3D;6, out_channels&#x3D;16, kernel_size&#x3D;5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2)
        )

        # 全连接层
        self.fc &#x3D; nn.Sequential(
            nn.Linear(in_features&#x3D;16 * 5 * 5, out_features&#x3D;120),
            nn.ReLU(),
            nn.Linear(in_features&#x3D;120, out_features&#x3D;84),
            nn.ReLU(),
            nn.Linear(in_features&#x3D;84, out_features&#x3D;10)
        )

    def forward(self, img):
        img &#x3D; self.conv(img)
        img &#x3D; img.view(img.size(0), -1)
        out &#x3D; self.fc(img)
        # out &#x3D; self.softmax(out)
        
        return out

    
net &#x3D; LeNet()
net &#x3D; net.to(device)

device &#x3D; torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

mean &#x3D; 0.1307
std &#x3D; 0.3801

# 对图像变换
transform &#x3D; transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean,), (std,))
]
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python'" data-language="Python'"><code class="language-Python'"># 训练数据集, 测试数据集
train_dataset &#x3D; datasets.MNIST(&#39;..&#x2F;datasets&#x2F;MNIST&#39;, train&#x3D;True, transform&#x3D;transform, download&#x3D;True) # len 60000
test_dataset &#x3D; datasets.MNIST(&#39;..&#x2F;datasets&#x2F;MNIST&#39;, train&#x3D;False, transform&#x3D;transform, download&#x3D;True) # len 10000

# 数据迭代器
train_dataloader &#x3D; DataLoader(train_dataset, batch_size&#x3D;64, shuffle&#x3D;True)  # len 938
test_dataloader &#x3D; DataLoader(test_dataset, batch_size&#x3D;64, shuffle&#x3D;True) # len 157

lr &#x3D; 1e-3
epochs &#x3D; 20
optimizer &#x3D; torch.optim.Adam(net.parameters(), lr&#x3D;lr)
criterion &#x3D; nn.CrossEntropyLoss()
scheduler &#x3D; torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, &#39;min&#39;, factor&#x3D;0.5, verbose&#x3D;True, patience&#x3D;5, min_lr&#x3D;0.0000001)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">train_loss &#x3D; []
train_acc &#x3D; []
val_loss &#x3D; []
val_acc &#x3D; []

for epoch in tqdm(range(epochs)):
    train_losses &#x3D; 0
    train_acces &#x3D; 0
    val_losses &#x3D; 0
    val_acces &#x3D; 0
    
    for x, y in train_dataloader:
        x, y &#x3D; x.to(device), y.to(device)
        output &#x3D; net(x)
        # 计算loss
        loss &#x3D; criterion(output, y)
        # 计算预测值
        _, pred &#x3D; torch.max(output, axis&#x3D;1)
        # 计算acc
        acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

        # 反向传播
        # 梯度清零
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses +&#x3D; loss.item()
        train_acces +&#x3D; acc.item()

    train_loss.append(train_losses &#x2F; len(train_dataloader))
    train_acc.append(train_acces &#x2F; len(train_dataloader))

    # 模型评估
    net.eval()
    with torch.no_grad():
        for x, y in test_dataloader:
            x, y &#x3D; x.to(device), y.to(device)
            output &#x3D; net(x)
            loss &#x3D; criterion(output, y)
            scheduler.step(loss)
            _, pred &#x3D; torch.max(output, axis&#x3D;1)
            acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

            val_losses +&#x3D; loss.item()
            val_acces +&#x3D; acc.item()
        
        val_loss.append(val_losses &#x2F; len(test_dataloader))
        val_acc.append(val_acces &#x2F; len(test_dataloader))

    print(f&quot;epoch:&#123;epoch+1&#125;  train_loss:&#123;train_losses &#x2F; len(train_dataloader):.4f&#125;, train_acc:&#123;train_acces &#x2F; len(train_dataloader):.4f&#125;, val_loss:&#123;val_losses &#x2F; len(test_dataloader):.4f&#125;, val_acc:&#123;val_acces &#x2F; len(test_dataloader)&#125;&quot;)

plt.plot(train_loss, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train loss&#39;)
plt.plot(val_loss, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val loss&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;loss&quot;)
plt.show()


plt.plot(train_acc, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train acc&#39;)
plt.plot(val_acc, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val acc&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;acc&quot;)
plt.show()

PATH &#x3D; &#39;.&#x2F;cw_mnist_lenet1.pth&#39;
torch.save(net, PATH)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">5%|▌         | 1&#x2F;20 [00:08&lt;02:46,  8.76s&#x2F;it]
epoch:1  train_loss:0.0304, train_acc:0.9907, val_loss:0.0406, val_acc:0.9872
 10%|█         | 2&#x2F;20 [00:18&lt;02:50,  9.49s&#x2F;it]
epoch:2  train_loss:0.0301, train_acc:0.9908, val_loss:0.0404, val_acc:0.9872
 15%|█▌        | 3&#x2F;20 [00:28&lt;02:44,  9.70s&#x2F;it]
epoch:3  train_loss:0.0300, train_acc:0.9908, val_loss:0.0403, val_acc:0.9872
 20%|██        | 4&#x2F;20 [00:37&lt;02:29,  9.37s&#x2F;it]
epoch:4  train_loss:0.0298, train_acc:0.9908, val_loss:0.0401, val_acc:0.9871
 25%|██▌       | 5&#x2F;20 [00:48&lt;02:26,  9.79s&#x2F;it]
epoch:5  train_loss:0.0296, train_acc:0.9908, val_loss:0.0399, val_acc:0.9871
 30%|███       | 6&#x2F;20 [00:58&lt;02:19,  9.97s&#x2F;it]
epoch:6  train_loss:0.0294, train_acc:0.9909, val_loss:0.0398, val_acc:0.9872
 35%|███▌      | 7&#x2F;20 [01:07&lt;02:04,  9.57s&#x2F;it]
epoch:7  train_loss:0.0293, train_acc:0.9910, val_loss:0.0396, val_acc:0.9872
 40%|████      | 8&#x2F;20 [01:17&lt;01:57,  9.78s&#x2F;it]
epoch:8  train_loss:0.0291, train_acc:0.9910, val_loss:0.0395, val_acc:0.9872
 45%|████▌     | 9&#x2F;20 [01:28&lt;01:50, 10.06s&#x2F;it]
epoch:9  train_loss:0.0290, train_acc:0.9911, val_loss:0.0393, val_acc:0.9872
 50%|█████     | 10&#x2F;20 [01:36&lt;01:36,  9.68s&#x2F;it]
epoch:10  train_loss:0.0288, train_acc:0.9912, val_loss:0.0392, val_acc:0.9874
 55%|█████▌    | 11&#x2F;20 [01:46&lt;01:27,  9.78s&#x2F;it]
epoch:11  train_loss:0.0287, train_acc:0.9912, val_loss:0.0390, val_acc:0.9874
 60%|██████    | 12&#x2F;20 [01:57&lt;01:19,  9.94s&#x2F;it]
epoch:12  train_loss:0.0286, train_acc:0.9913, val_loss:0.0389, val_acc:0.9875
 65%|██████▌   | 13&#x2F;20 [02:05&lt;01:07,  9.58s&#x2F;it]
epoch:13  train_loss:0.0285, train_acc:0.9914, val_loss:0.0388, val_acc:0.9876
 70%|███████   | 14&#x2F;20 [02:16&lt;00:58,  9.80s&#x2F;it]
epoch:14  train_loss:0.0283, train_acc:0.9914, val_loss:0.0387, val_acc:0.9876
 75%|███████▌  | 15&#x2F;20 [02:26&lt;00:49, 10.00s&#x2F;it]
epoch:15  train_loss:0.0282, train_acc:0.9915, val_loss:0.0386, val_acc:0.9878
 80%|████████  | 16&#x2F;20 [02:37&lt;00:40, 10.09s&#x2F;it]
epoch:16  train_loss:0.0281, train_acc:0.9915, val_loss:0.0384, val_acc:0.9878
 85%|████████▌ | 17&#x2F;20 [02:47&lt;00:30, 10.21s&#x2F;it]
epoch:17  train_loss:0.0280, train_acc:0.9915, val_loss:0.0383, val_acc:0.9878
 90%|█████████ | 18&#x2F;20 [02:56&lt;00:19,  9.75s&#x2F;it]
epoch:18  train_loss:0.0279, train_acc:0.9916, val_loss:0.0382, val_acc:0.988
 95%|█████████▌| 19&#x2F;20 [03:06&lt;00:09,  9.93s&#x2F;it]
epoch:19  train_loss:0.0278, train_acc:0.9916, val_loss:0.0381, val_acc:0.988
100%|██████████| 20&#x2F;20 [03:16&lt;00:00,  9.84s&#x2F;it]
epoch:20  train_loss:0.0277, train_acc:0.9917, val_loss:0.0380, val_acc:0.988<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/ffb6a1cc/1.png" class loading="lazy">

<img src="/posts/ffb6a1cc/2.png" class loading="lazy">

<p>这里没有改变参数，只是对一个参数进行了测试</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 用于可视化
adv_examples &#x3D; []
correct &#x3D; 0
vis_max &#x3D; 10
vis_num &#x3D; 0
rand_num &#x3D; -1

for data, label in  test_dataloader:
    data, label &#x3D; data.to(device), label.to(device)
    attack_data &#x3D; cw_l2_attack(model&#x3D;net, images&#x3D;data, labels&#x3D;label)
    output &#x3D; net(attack_data)
    _, pred &#x3D; torch.max(output, axis&#x3D;1)
    # print(label)
    # print(pred)
    # print(sum(pred &#x3D;&#x3D; label))
    # 统计一下攻击以后正确的数量
    correct +&#x3D; torch.sum(pred &#x3D;&#x3D; label)
    rand_num &#x3D; torch.randint(0, 11, (1,))
    if rand_num % 2 &#x3D;&#x3D; 0 and vis_num &lt; vis_max:
        adv_examples.append((data, label, attack_data, pred))
        vis_num +&#x3D; 1

print(&quot;Attack accuracy:&quot;, float(correct) &#x2F; len(test_dataset))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可视化生成的对抗样本</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
index <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># 当前行画的图片</span>
current <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># 设置当前行画原始还是攻击</span>
odd <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    data<span class="token punctuation">,</span> label<span class="token punctuation">,</span> attack_data<span class="token punctuation">,</span> pred <span class="token operator">=</span> adv_examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    current <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 一个批次的数据只画5张</span>
        <span class="token keyword">if</span> label<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">!=</span> pred<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token keyword">and</span> current <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">:</span>
            index <span class="token operator">+=</span> <span class="token number">1</span>
            current <span class="token operator">+=</span> <span class="token number">1</span>
            plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>
            <span class="token keyword">if</span> current <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> odd <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"original image"</span><span class="token punctuation">)</span>            
            <span class="token keyword">elif</span> current <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> odd <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"attack image"</span><span class="token punctuation">)</span>
                
            <span class="token keyword">if</span> odd <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"&#123;&#125; --> &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> pred<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>attack_data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"&#123;&#125; --> &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>data<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    odd <span class="token operator">+=</span> <span class="token number">1</span>

    plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>需要注意的是，这篇文章中的可视化，我尝试了batch_size不为1，而是64，和前几篇文章截然不同。</p>
<img src="/posts/ffb6a1cc/3.png" class loading="lazy">

<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p><a href="https://cloud.tencent.com/developer/article/1684098">独家解读 | 基于优化的对抗攻击：CW攻击的原理详解与代码解读-腾讯云开发者社区-腾讯云 (tencent.com)</a></p>
<p><a href="https://muyuuuu.github.io/2021/05/04/CW-attack/">对抗攻击篇：CW 攻击算法 | Just for Life. (muyuuuu.github.io)</a></p>
<p><a href="https://blog.csdn.net/weixin_41466575/article/details/117738731">对抗样本之CW原理&amp;coding_cw对抗样本-CSDN博客</a>（对CW算法的手稿理解，很清晰！！！）</p>
<p><a href="https://www.cnblogs.com/tangweijqxx/p/10627360.html">5.基于优化的攻击——CW - 机器学习安全小白 - 博客园 (cnblogs.com)</a></p>
<p><a href="https://blog.csdn.net/m0_55368674/article/details/130242588">CW对抗样本生成算法 torch实现_cw对抗样本生成方法是ifgsm吗-CSDN博客</a>（对置信度k的举例解释）</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>对抗样本生成与防御</tag>
        <tag>对抗攻击</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown数学公式手册</title>
    <url>/posts/fe6b57b2/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>自己在用markdown写数学公式很多符号都记不住，每次都得去查很麻烦，所以自己在这里总结一下常用的，以及自己用过的但比较冷门的，方便自己使用。</p>
<h3 id="数学公式手册"><a href="#数学公式手册" class="headerlink" title="数学公式手册"></a>数学公式手册</h3><h4 id="常用符号"><a href="#常用符号" class="headerlink" title="常用符号"></a>常用符号</h4><ul>
<li><p>行间公式： $     $  （typora如果失效，需要在偏好设置–Markdown–Markdown扩展语法–内联公式）</p>
</li>
<li><p>行间公式：$$  $$</p>
</li>
<li><p>空格：,   ;   \quad  \qquad    四种符号宽度逐渐增加</p>
</li>
<li><p>上标：^（多个字符使用{}）</p>
</li>
<li><p>下标：_（多个字符使用{}）</p>
</li>
<li><p>分数：\frac{分子}{分母}</p>
</li>
<li><p>开方数：\sqrt [根指数，省略时为2] {被开方数}</p>
</li>
<li><p>省略号：\ldots 表示与文本底线对齐的省略号，\cdots 表示与文本中线对齐的省略号</p>
</li>
<li><p>矢量：\vec{矢量}，\overrightarrow{}、\overleftarrow{}、\overleftrightarrow{}表示带箭头的矢量</p>
</li>
<li><p>积分：\int_积分下限^积分上限 {被积表达式}</p>
</li>
<li><p>极限：\lim_{变量 \to 表达式}，\to可以更改为任意符号</p>
</li>
<li><p>累加：\sum_{下标表达式}^{上标表达式} {累加表达式}</p>
</li>
<li><p>累乘：\prod_{下标表达式}^{上标表达式} {累加表达式}</p>
</li>
<li><p>并集：\bigcup_{下标表达式}^{上标表达式} {累加表达式}</p>
</li>
<li><p>交集：\bigcap_{下标表达式}^{上标表达式} {累加表达式}    &#x2F;&#x2F;以上四个符号在行内显示时上下标表达式将会移至右上角和右下角</p>
</li>
<li><p>行标：\tag{行标}</p>
</li>
<li><p>大括号：使用 \left和 \right 来创建自动匹配高度的 (圆括号)，[方括号] 和 {花括号} </p>
</li>
<li><p>添加注释文字：\text{文字}</p>
</li>
<li><p>删除线：\cancel{字符}    \bcancel{字符}   \xcancel{字符}     \cancelto{字符}  最后一种katex不支持，使用删除线必须在公式内使用 \require{cancel}来允许 片段删除线 的显示（katex不需要）</p>
</li>
</ul>
<p>$   \cancel{xy} \qquad  \bcancel{xy} \qquad \xcancel{xy}  \qquad  $</p>
<h4 id="括号和分隔符"><a href="#括号和分隔符" class="headerlink" title="括号和分隔符"></a>括号和分隔符</h4><img src="/posts/fe6b57b2/1.png" class loading="lazy">

<h4 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h4><img src="/posts/fe6b57b2/2.png" class loading="lazy">

<p>部分字母有变量专用形式，以\var开头</p>
<img src="/posts/fe6b57b2/3.png" class loading="lazy">

<h4 id="特殊符号"><a href="#特殊符号" class="headerlink" title="特殊符号"></a>特殊符号</h4><h5 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h5><img src="/posts/fe6b57b2/4.png" class loading="lazy">

<h5 id="集合运算符"><a href="#集合运算符" class="headerlink" title="集合运算符"></a>集合运算符</h5><img src="/posts/fe6b57b2/5.png" class loading="lazy">

<h5 id="对数运算符"><a href="#对数运算符" class="headerlink" title="对数运算符"></a>对数运算符</h5><img src="/posts/fe6b57b2/6.png" class loading="lazy">

<h5 id="三角运算符"><a href="#三角运算符" class="headerlink" title="三角运算符"></a>三角运算符</h5><img src="/posts/fe6b57b2/7.png" class loading="lazy">

<h5 id="微积分运算符"><a href="#微积分运算符" class="headerlink" title="微积分运算符"></a>微积分运算符</h5><img src="/posts/fe6b57b2/8.png" class loading="lazy">

<h5 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h5><img src="/posts/fe6b57b2/9.png" class loading="lazy">

<h5 id="戴帽符号"><a href="#戴帽符号" class="headerlink" title="戴帽符号"></a>戴帽符号</h5><img src="/posts/fe6b57b2/10.png" class loading="lazy">

<h5 id="连线符号"><a href="#连线符号" class="headerlink" title="连线符号"></a>连线符号</h5><img src="/posts/fe6b57b2/11.png" class loading="lazy">

<h5 id="箭头符号"><a href="#箭头符号" class="headerlink" title="箭头符号"></a>箭头符号</h5><img src="/posts/fe6b57b2/12.png" class loading="lazy">

<h4 id="字体转换"><a href="#字体转换" class="headerlink" title="字体转换"></a>字体转换</h4><img src="/posts/fe6b57b2/13.png" class loading="lazy">

<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="https://freeopen.github.io/mathjax/">Markdown 数学公式指导手册 | Freeopen</a></p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗攻击:DeepFool算法</title>
    <url>/posts/134187cf/</url>
    <content><![CDATA[<h4 id="DeepFool原理"><a href="#DeepFool原理" class="headerlink" title="DeepFool原理"></a>DeepFool原理</h4><p>DeepFool算法是一种基于梯度的攻击方法，这种方法看到以后感觉有点像支持向量机。这种方法在论文[<a href="https://arxiv.org/abs/1511.04599">1511.04599] DeepFool: a simple and accurate method to fool deep neural networks (arxiv.org)</a>中提出。</p>
<p>对于线性二分类问题，有</p>
<div>
$$
f(x) = w^Tx+b  \tag{1}
$$
</div>
分类边界定义为
<div>
$$
\mathscr{F} = x : f(x) = 0 \tag{2}
$$
</div>
假设此时有
<div>
$$
f(x_0) > 0 \tag{3}
$$
</div>
这里的x位于直线的下方。我们希望可以添加扰动r，使分类器出错，即
<div>
$$
f(x_0 + r) < 0 \tag{4}
$$
<>/div
我们想要扰动成功，就是要跨过这个分类平面， 

<p>如何添加扰动可以实现这样的目的呢？如下图所示</p>
<img src="/posts/134187cf/1.png" class loading="lazy">

<p>过x0做f(x)的垂线，假设交于点O，点x0与O之间的距离就是点x0到分类边界之间的最短距离，其方向也就是扰动的方向，沿该方向进行扰动，可以保证扰动最小。</p>
<p>我们把这个距离记为r*(x0)，即：</p>
<div>
$$
r_*(x_0): = argmin \Vert r \Vert_2 \tag{5} \\
\qquad\qquad\qquad\qquad\qquad\qquad\qquad subject \; to\; sign(f(x_0+r))\neq sign(f(x_0)) \\
\qquad\quad =-\frac{f(x_0)}{\Vert \omega \Vert_2^{2}}w.
$$
</div>
在论文中也给出了二分类方法的伪代码

<img src="/posts/134187cf/2.png" class loading="lazy">

<p>对于多分类问题，定义fk(x)是分类器在第k类上的概率，那么</p>
<div>
$$
\widehat{k}(\bold{x}) = arg \; \underset{k}{max} \; f_k(\bold{x}) \tag{6}
$$
</div>
就表示分类的最终标签。同二分类的思想一致，**我们要求出x0到每个分类边界的距离（K个类），将距离最短的作为扰动，其方向也就是最终的扰动方向**
<div>
$$
\widehat{l}(x_0) = \underset{k \neq \widehat{k}(\bold{x_0})}{arg \; min} \frac{\vert f_k(\bold{x_0}) - f_{\widehat{k}}(\bold{x_0}) \vert}{\Vert \bold{\omega}_k - \bold{\omega}_{\widehat{k}(\bold{x_0})} \Vert_2}.  \tag{7}
$$
</div>
此外，还需要注意的是，在多分类中，将分类边界定义为
<div>
$$
\mathscr{F}_k = x : f_k(x) - f_{\widehat{k}(x_0)}(x) = 0 \tag{8}
$$
</div>
论文中也给出了伪代码：

<img src="/posts/134187cf/3.png" class loading="lazy">

<h4 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h4><p>DeepFool核心代码参考自Github(<a href="https://github.com/LTS4/DeepFool/blob/575f3d847ee65d78c0e3b0306657e3e6d575ba7b/Python/deepfool.py#L8">github.com</a>)，其中，我对代码进行了部分修改，比如将部分已弃用函数修改，输入图像维度修改等。并且逐句代码增加了注释。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">DeepFool</span><span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> net<span class="token punctuation">,</span> device<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> overshoot<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 源代码中这里输入的image为H*W*3,而我这里为B*H*W*C</span>

    image <span class="token operator">=</span> image<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    
    <span class="token comment"># 源代码这里增维了一次, 我这里有batchsize所以不需要进行增维</span>
    <span class="token comment"># f_image为经过模型的对各个类别的预测</span>
    <span class="token comment"># f_image = net.forward(Variable(image[None, :, :, :], requires_grad=True)).data.cpu().numpy().flatten()</span>
    f_image <span class="token operator">=</span> net<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># shape:[1, 10] -- [10]</span>


    <span class="token comment"># argsort(排序,返回在原数据中的索引，从小到大排,使用[::-1]变为了从大到小)</span>
    I <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>f_image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    I <span class="token operator">=</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>num_classes<span class="token punctuation">]</span>
    <span class="token comment"># 模型实际预测的标签,我这里改为该数据的真实标签</span>
    <span class="token comment"># label = I[0]</span>

    <span class="token comment"># 获取输入图像的形状</span>
    <span class="token comment"># 源代码输入image是H*W*C,我这里需要降维</span>
    <span class="token comment"># input_shape = image.cpu().numpy().shape</span>
    input_shape <span class="token operator">=</span> image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape
    pert_image <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

    w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span>  <span class="token comment"># shape[1, 28, 28]</span>
    r_tot <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span> <span class="token comment"># shape[1, 28, 28]</span>
    
    loop_i <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># 源代码这里又增加一个维度，我只需要直接深拷贝原始图像即可</span>
    <span class="token comment"># x = Variable(pert_image[None, :], requires_grad=True)</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>pert_image<span class="token punctuation">)</span>
    x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

    fs <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># fs shape: [1, 10]</span>
    <span class="token comment"># fs这里的shape是[1,10],所以这里使用fs[0, I[k]],也可以把fs降维成[10]</span>
    <span class="token comment"># 这是把预测的概率值从大到小放到fs_list中了</span>
    fs_list <span class="token operator">=</span> <span class="token punctuation">[</span>fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment"># k_i表示添加扰动以后预测的类别,这里对k_i进行初始化</span>
    k_i <span class="token operator">=</span> label

    <span class="token keyword">while</span> k_i <span class="token operator">==</span> label <span class="token keyword">and</span> loop_i <span class="token operator">&lt;</span> max_iter<span class="token punctuation">:</span>
        <span class="token comment"># pert用于存储最小的扰动</span>
        pert <span class="token operator">=</span> np<span class="token punctuation">.</span>inf
        <span class="token comment"># 计算该预测标签在输入数据x处的梯度</span>
        fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        grad_orig <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 将模型中所有参数的梯度清零</span>
            zero_gradients<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment"># x.grad.zero_()</span>

            <span class="token comment"># 计算第k个标签在输入数据x处的梯度</span>
            fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            cur_grad <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 计算第k个类别和应预测类别在x梯度的差异,也是扰动应该添加的方向</span>
            w_k <span class="token operator">=</span> cur_grad <span class="token operator">-</span> grad_orig
            f_k <span class="token operator">=</span> <span class="token punctuation">(</span>fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">-</span> fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 计算了梯度向量w_k的L2范数,pert_k越小，扰动越小，攻击越有效</span>
            pert_k <span class="token operator">=</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>f_k<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w_k<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token comment"># 选择最小扰动进行保存</span>
            <span class="token keyword">if</span> pert_k <span class="token operator">&lt;</span> pert<span class="token punctuation">:</span>
                <span class="token comment"># 需要移动的距离</span>
                pert <span class="token operator">=</span> pert_k
                <span class="token comment"># 扰动方向</span>
                w <span class="token operator">=</span> w_k
        
        r_i <span class="token operator">=</span> <span class="token punctuation">(</span>pert <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span> <span class="token operator">*</span> w <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
        <span class="token comment"># 累加到总扰动上</span>
        r_tot <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span>r_tot <span class="token operator">+</span> r_i<span class="token punctuation">)</span>

        pert_image <span class="token operator">=</span> image <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> overshoot<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>r_tot<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># x = Variable(pert_image, requires_grad=True)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>pert_image<span class="token punctuation">)</span>
        x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

        fs <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># k_i表示预测标签</span>
        k_i <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>fs<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        loop_i <span class="token operator">+=</span> <span class="token number">1</span>
        
    <span class="token comment"># 放大扰动</span>
    r_tot <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>overshoot<span class="token punctuation">)</span> <span class="token operator">*</span> r_tot

    <span class="token keyword">return</span> r_tot<span class="token punctuation">,</span> k_i<span class="token punctuation">,</span> pert_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="训练-攻击完整代码"><a href="#训练-攻击完整代码" class="headerlink" title="训练+攻击完整代码"></a>训练+攻击完整代码</h4><p>这里的代码是从训练模型到进行对抗样本生成到可视化的完整过程。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> random
<span class="token keyword">import</span> copy
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">from</span> torchsummary <span class="token keyword">import</span> summary
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>gradcheck <span class="token keyword">import</span> zero_gradients

<span class="token comment"># 设置随机种子</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>
<span class="token comment"># 设置GPU随机种子</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>

np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>
random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>

<span class="token comment"># 搭建LeNet模型</span>
<span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 卷积层</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># 全连接层</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>
        img <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        img <span class="token operator">=</span> img<span class="token punctuation">.</span>view<span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        <span class="token comment"># out = self.softmax(out)</span>
        
        <span class="token keyword">return</span> out


device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> LeNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

mean <span class="token operator">=</span> <span class="token number">0.1307</span>
std <span class="token operator">=</span> <span class="token number">0.3801</span>

<span class="token comment"># 对图像变换</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span>mean<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>std<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 训练数据集, 测试数据集</span>
train_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'../datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 60000</span>
test_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'../datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 10000</span>

<span class="token comment"># 数据迭代器</span>
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># len 938</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 157</span>

lr <span class="token operator">=</span> <span class="token number">1e-3</span>
epochs <span class="token operator">=</span> <span class="token number">20</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> <span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.0000001</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 训练模型</span>
train_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
train_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
val_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
val_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_losses <span class="token operator">=</span> <span class="token number">0</span>
    train_acces <span class="token operator">=</span> <span class="token number">0</span>
    val_losses <span class="token operator">=</span> <span class="token number">0</span>
    val_acces <span class="token operator">=</span> <span class="token number">0</span>
    
    <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_dataloader<span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        output <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 计算loss</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        <span class="token comment"># 计算预测值</span>
        _<span class="token punctuation">,</span> pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 计算acc</span>
        acc <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y <span class="token operator">==</span> pred<span class="token punctuation">)</span> <span class="token operator">/</span> output<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        <span class="token comment"># 反向传播</span>
        <span class="token comment"># 梯度清零</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        train_losses <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        train_acces <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_losses <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span>
    train_acc<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_acces <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 模型评估</span>
    net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> test_dataloader<span class="token punctuation">:</span>
            x<span class="token punctuation">,</span> y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            output <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
            _<span class="token punctuation">,</span> pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            acc <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y <span class="token operator">==</span> pred<span class="token punctuation">)</span> <span class="token operator">/</span> output<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

            val_losses <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            val_acces <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        val_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_losses <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span>
        val_acc<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_acces <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"epoch:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string">  train_loss:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>train_losses <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">, train_acc:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>train_acces <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">, val_loss:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>val_losses <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">, val_acc:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>val_acces <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'green'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'train loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'blue'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'val loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_acc<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'green'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'train acc'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>val_acc<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'blue'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'val acc'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"acc"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

PATH <span class="token operator">=</span> <span class="token string">'./deepfool_mnist_lenet.pth'</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-none"><code class="language-none"> 10%|█         | 2&#x2F;20 [00:25&lt;03:37, 12.08s&#x2F;it]epoch:2  train_loss:0.0650, train_acc:0.9804, val_loss:0.0563, val_acc:0.9819864649681529
 15%|█▌        | 3&#x2F;20 [00:37&lt;03:23, 12.00s&#x2F;it]epoch:3  train_loss:0.0642, train_acc:0.9808, val_loss:0.0553, val_acc:0.9821855095541401
 20%|██        | 4&#x2F;20 [00:45&lt;02:47, 10.47s&#x2F;it]epoch:4  train_loss:0.0634, train_acc:0.9810, val_loss:0.0546, val_acc:0.9823845541401274
 25%|██▌       | 5&#x2F;20 [00:53&lt;02:24,  9.63s&#x2F;it]epoch:5  train_loss:0.0627, train_acc:0.9812, val_loss:0.0543, val_acc:0.9823845541401274
 30%|███       | 6&#x2F;20 [01:01&lt;02:06,  9.06s&#x2F;it]epoch:6  train_loss:0.0621, train_acc:0.9814, val_loss:0.0535, val_acc:0.9827826433121019
 35%|███▌      | 7&#x2F;20 [01:09&lt;01:53,  8.72s&#x2F;it]epoch:7  train_loss:0.0616, train_acc:0.9815, val_loss:0.0547, val_acc:0.9822850318471338
 40%|████      | 8&#x2F;20 [01:17&lt;01:41,  8.48s&#x2F;it]epoch:8  train_loss:0.0611, train_acc:0.9817, val_loss:0.0530, val_acc:0.9831807324840764
 45%|████▌     | 9&#x2F;20 [01:25&lt;01:32,  8.42s&#x2F;it]epoch:9  train_loss:0.0606, train_acc:0.9819, val_loss:0.0525, val_acc:0.9833797770700637
 50%|█████     | 10&#x2F;20 [01:34&lt;01:23,  8.31s&#x2F;it]epoch:10  train_loss:0.0603, train_acc:0.9819, val_loss:0.0522, val_acc:0.9834792993630573
 55%|█████▌    | 11&#x2F;20 [01:42&lt;01:15,  8.40s&#x2F;it]epoch:11  train_loss:0.0600, train_acc:0.9821, val_loss:0.0525, val_acc:0.98328025477707
 60%|██████    | 12&#x2F;20 [01:52&lt;01:09,  8.69s&#x2F;it]epoch:12  train_loss:0.0597, train_acc:0.9822, val_loss:0.0517, val_acc:0.9835788216560509
 65%|██████▌   | 13&#x2F;20 [02:01&lt;01:03,  9.05s&#x2F;it]epoch:13  train_loss:0.0595, train_acc:0.9823, val_loss:0.0515, val_acc:0.9836783439490446
 70%|███████   | 14&#x2F;20 [02:11&lt;00:55,  9.21s&#x2F;it]epoch:14  train_loss:0.0592, train_acc:0.9824, val_loss:0.0514, val_acc:0.9835788216560509
 75%|███████▌  | 15&#x2F;20 [02:20&lt;00:46,  9.23s&#x2F;it]epoch:15  train_loss:0.0590, train_acc:0.9824, val_loss:0.0512, val_acc:0.98328025477707
 80%|████████  | 16&#x2F;20 [02:30&lt;00:37,  9.32s&#x2F;it]epoch:16  train_loss:0.0589, train_acc:0.9824, val_loss:0.0517, val_acc:0.9828821656050956
 85%|████████▌ | 17&#x2F;20 [02:39&lt;00:27,  9.15s&#x2F;it]epoch:17  train_loss:0.0586, train_acc:0.9825, val_loss:0.0515, val_acc:0.9828821656050956
 90%|█████████ | 18&#x2F;20 [02:47&lt;00:18,  9.02s&#x2F;it]epoch:18  train_loss:0.0585, train_acc:0.9825, val_loss:0.0509, val_acc:0.9831807324840764
 95%|█████████▌| 19&#x2F;20 [02:56&lt;00:08,  8.91s&#x2F;it]epoch:19  train_loss:0.0583, train_acc:0.9826, val_loss:0.0509, val_acc:0.9831807324840764
100%|██████████| 20&#x2F;20 [03:06&lt;00:00,  9.33s&#x2F;it]epoch:20  train_loss:0.0582, train_acc:0.9826, val_loss:0.0521, val_acc:0.9826831210191083<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>设置deepfool攻击的相关参数</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> 
max_iter <span class="token operator">=</span> <span class="token number">10</span>
overshoot <span class="token operator">=</span> <span class="token number">0.2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">DeepFool</span><span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> net<span class="token punctuation">,</span> device<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> overshoot<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 源代码中这里输入的image为H*W*3,而我这里为B*H*W*C</span>

    image <span class="token operator">=</span> image<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    
    <span class="token comment"># 源代码这里增维了一次, 我这里有batchsize所以不需要进行增维</span>
    <span class="token comment"># f_image为经过模型的对各个类别的预测</span>
    <span class="token comment"># f_image = net.forward(Variable(image[None, :, :, :], requires_grad=True)).data.cpu().numpy().flatten()</span>
    f_image <span class="token operator">=</span> net<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># shape:[1, 10] -- [10]</span>


    <span class="token comment"># argsort(排序,返回在原数据中的索引，从小到大排,使用[::-1]变为了从大到小)</span>
    I <span class="token operator">=</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>f_image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    I <span class="token operator">=</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>num_classes<span class="token punctuation">]</span>
    <span class="token comment"># 模型实际预测的标签,我这里改为该数据的真实标签</span>
    <span class="token comment"># label = I[0]</span>

    <span class="token comment"># 获取输入图像的形状</span>
    <span class="token comment"># 源代码输入image是H*W*C,我这里需要降维</span>
    <span class="token comment"># input_shape = image.cpu().numpy().shape</span>
    input_shape <span class="token operator">=</span> image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape
    pert_image <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

    w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span>  <span class="token comment"># shape[1, 28, 28]</span>
    r_tot <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>input_shape<span class="token punctuation">)</span> <span class="token comment"># shape[1, 28, 28]</span>
    
    loop_i <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># 源代码这里又增加一个维度，我只需要直接深拷贝原始图像即可</span>
    <span class="token comment"># x = Variable(pert_image[None, :], requires_grad=True)</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>pert_image<span class="token punctuation">)</span>
    x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

    fs <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># fs shape: [1, 10]</span>
    <span class="token comment"># fs这里的shape是[1,10],所以这里使用fs[0, I[k]],也可以把fs降维成[10]</span>
    <span class="token comment"># 这是把预测的概率值从大到小放到fs_list中了</span>
    fs_list <span class="token operator">=</span> <span class="token punctuation">[</span>fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment"># k_i表示添加扰动以后预测的类别,这里对k_i进行初始化</span>
    k_i <span class="token operator">=</span> label

    <span class="token keyword">while</span> k_i <span class="token operator">==</span> label <span class="token keyword">and</span> loop_i <span class="token operator">&lt;</span> max_iter<span class="token punctuation">:</span>
        <span class="token comment"># pert用于存储最小的扰动</span>
        pert <span class="token operator">=</span> np<span class="token punctuation">.</span>inf
        <span class="token comment"># 计算该预测标签在输入数据x处的梯度</span>
        fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        grad_orig <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 将模型中所有参数的梯度清零</span>
            zero_gradients<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment"># x.grad.zero_()</span>

            <span class="token comment"># 计算第k个标签在输入数据x处的梯度</span>
            fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            cur_grad <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 计算第k个类别和应预测类别在x梯度的差异,也是扰动应该添加的方向</span>
            w_k <span class="token operator">=</span> cur_grad <span class="token operator">-</span> grad_orig
            f_k <span class="token operator">=</span> <span class="token punctuation">(</span>fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">-</span> fs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 计算了梯度向量w_k的L2范数,pert_k越小，扰动越小，攻击越有效</span>
            pert_k <span class="token operator">=</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>f_k<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w_k<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token comment"># 选择最小扰动进行保存</span>
            <span class="token keyword">if</span> pert_k <span class="token operator">&lt;</span> pert<span class="token punctuation">:</span>
                <span class="token comment"># 需要移动的距离</span>
                pert <span class="token operator">=</span> pert_k
                <span class="token comment"># 扰动方向</span>
                w <span class="token operator">=</span> w_k
        
        r_i <span class="token operator">=</span> <span class="token punctuation">(</span>pert <span class="token operator">+</span> <span class="token number">1e-4</span><span class="token punctuation">)</span> <span class="token operator">*</span> w <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>w<span class="token punctuation">)</span>
        <span class="token comment"># 累加到总扰动上</span>
        r_tot <span class="token operator">=</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span>r_tot <span class="token operator">+</span> r_i<span class="token punctuation">)</span>

        pert_image <span class="token operator">=</span> image <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> overshoot<span class="token punctuation">)</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>r_tot<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># x = Variable(pert_image, requires_grad=True)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>pert_image<span class="token punctuation">)</span>
        x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

        fs <span class="token operator">=</span> net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># k_i表示预测标签</span>
        k_i <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>fs<span class="token punctuation">.</span>data<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        loop_i <span class="token operator">+=</span> <span class="token number">1</span>
        
    <span class="token comment"># 放大扰动</span>
    r_tot <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">+</span>overshoot<span class="token punctuation">)</span> <span class="token operator">*</span> r_tot

    <span class="token keyword">return</span> r_tot<span class="token punctuation">,</span> k_i<span class="token punctuation">,</span> pert_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>生成对抗样本，并进行预测</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
correct <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment"># 产生对抗样本并进行预测类别</span>
<span class="token keyword">for</span> image<span class="token punctuation">,</span> label <span class="token keyword">in</span> test_dataloader<span class="token punctuation">:</span>
    
    r_t<span class="token punctuation">,</span> label_pret<span class="token punctuation">,</span> pert_image <span class="token operator">=</span> DeepFool<span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> net<span class="token punctuation">,</span> device<span class="token punctuation">)</span>

    <span class="token comment"># output = net(pert_image)</span>
    <span class="token comment"># label_false = torch.argmax(output, dim=1)</span>
    <span class="token keyword">if</span> label <span class="token operator">==</span> label_pret<span class="token punctuation">:</span>
        correct <span class="token operator">+=</span> <span class="token number">1</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        num <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> num <span class="token operator">%</span> <span class="token number">5</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">10</span><span class="token punctuation">:</span>
            examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>pert_image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label_pret<span class="token punctuation">,</span> image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">pass</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Test Accuracy: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可视化</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">index <span class="token operator">=</span> <span class="token number">0</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    index <span class="token operator">+=</span> <span class="token number">1</span>
    pert_image<span class="token punctuation">,</span> pert_label<span class="token punctuation">,</span> true_image<span class="token punctuation">,</span> true_label <span class="token operator">=</span> examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span>

    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"pert_image"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"&#123;&#125; --> &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>true_label<span class="token punctuation">,</span> pert_label<span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>pert_image<span class="token punctuation">)</span>

    index <span class="token operator">+=</span> <span class="token number">1</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"ori_image"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"&#123;&#125; --> &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>true_label<span class="token punctuation">,</span> true_label<span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>true_image<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplots_adjust<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/134187cf/4.png" class loading="lazy">

<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p><a href="https://github.com/LTS4/DeepFool/blob/575f3d847ee65d78c0e3b0306657e3e6d575ba7b/Python/deepfool.py#L8">DeepFool&#x2F;Python&#x2F;deepfool.py at 575f3d847ee65d78c0e3b0306657e3e6d575ba7b · LTS4&#x2F;DeepFool (github.com)</a></p>
<p><a href="https://chaoge123456.github.io/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%94%9F%E6%88%90%E7%B3%BB%E5%88%97%EF%BC%9AFGSM%E5%92%8CDeepfool.html/">对抗样本生成系列：FGSM和DeepFool | 小生很忙 (chaoge123456.github.io)</a></p>
<p><a href="https://www.ai2news.com/blog/2277447/">AI安全之对抗样本入门: 5.5 DeepFool算法(deep neural networks,deep fool) - AI牛丝 (ai2news.com)</a></p>
<p><a href="https://as837430732.github.io/2020/06/15/pytorch%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89%E5%9C%A8MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%8D%E7%8E%B0FGSM%E3%80%81DeepFool%E6%94%BB%E5%87%BB/">pytorch实战（二） 在MNIST数据集复现FGSM、DeepFool攻击 | Haoran’s blog (as837430732.github.io)</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1168037">干货 | 攻击AI模型之DeepFool算法-腾讯云开发者社区-腾讯云 (tencent.com)</a></p>
</div>]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>对抗样本生成与防御</tag>
        <tag>对抗攻击</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗攻击:FGSM和BIM算法</title>
    <url>/posts/f891610e/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>初次接触对抗样本，在这里记录一下自己的学习过程，希望可以帮助自己更好理解，也希望可以帮助到有需要的人。</p>
<h4 id="FGSM"><a href="#FGSM" class="headerlink" title="FGSM"></a><strong>FGSM</strong></h4><h5 id="FGSM原理"><a href="#FGSM原理" class="headerlink" title="FGSM原理"></a>FGSM原理</h5><p>FGSM，即Fast Gradient Sign Method，这是一种用于生成对抗性样本的算法。在论文<a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a>中被提到。它是一种基于梯度生成的算法，属于无目标攻击（不要求指定生成的对抗样本的类别，只要求生成的对抗样本不是正确的那个类别即可）</p>
<img src="/posts/f891610e/1.png" class loading="lazy">

<p>在这幅图最左侧是输入图像x，他的正确分类标签y为”panda”，我们将中间这幅图所表达的意思用μ来代替，即μ&#x3D;0.07*(sign(J(θ,x,y)‘))，其中sign是符号函数，J(θ,x,y)是训练网络的损失函数，J(θ,x,y)’表示损失函数J对x的导数。用x’代表生成的对抗样本，即x‘&#x3D;x+μ。从图中可以看到x’的分类标签y’成为了”gibbon“。说明对抗样本成功骗过了模型，使模型的分类错误。</p>
<p>在神经网络中使用梯度下降算法，使梯度降低，从而不断最小化损失值以达到提高准确率的目的。其公式如下：</p>
<div>
$$
\alpha = \alpha - \delta\times\frac{\partial L}{\partial \alpha} \tag{1}
$$
</div>

<p>如果把梯度下降中的后一项看成μ，即</p>
<div>
$$
\eta = \delta\times\frac{\partial L}{\partial \alpha} \tag{2}
$$
</div>
那么，梯度下降的公式就可以写为:
<div>
$$
\alpha = \alpha - \eta \tag{3}
$$
</div>
这与FGSM中x'=x+μ的公式非常像，为什么这么像呢？

<p>梯度代表函数值增加最快的方向，在神经网络中我们要通过反向传播不断降低损失值来达到提高准确率的目的，所i有要减去梯度，而在fgsm中，为了让分类错误，要让损失增大也就是梯度上升，所以这里只需要改为加号就可以了。所以这两个公式是非常像的。</p>
<p><strong>我再写一下我对损失增大的理解，我们直到损失函数是度量真实值与预测值之间的差距的，当损失值越小，说明预测值和真实值越接近，而我们这里是要让分类错误，就是预测值和真实值不同，所以要让这个度量指标变大，其越大说明预测值和真实值之间的差距越大，也就说明模型的分类是错误的。</strong></p>
<p>接下来我们介绍一下FGSM算法中的μ，公式如下：</p>
<div>
$$
\mu = \epsilon sign(\nabla_xJ(\boldsymbol{\theta},\boldsymbol{x},y)) \tag{4}
$$
</div>
在这个公式中，x是输入的原始图像，θ是模型的参数，y是原始图像x的真实类别，J是损失函数，∇x 表示对 x 求偏导，sign是符号函数。我们在这里看一下符号函数的公式。
<div>
$$
\text{sign}(x) = 
  \begin{cases} 
   -1 & \text{if } x < 0, \\
   0 & \text{if } x = 0, \\
   1 & \text{if } x > 0.
  \end{cases} 
  \tag{5}
$$
</div>
在FGSM中引入符号函数可以确定对抗扰动的方向。在FGSM中不需要关心具体的梯度大小，只需要知道方向即可通过确定对抗扰动的方向。（**这里我的理解是，FGSM要寻找一个可以使模型分类错误的有效扰动，而不是最优的扰动的大小，所以只关心梯度方向，具体扰动大小则由epsilon确定，初次之外，有资料中说，对抗性扰动的泛化性因不同的模型和数据集，具体的梯度大小会有所不同，而梯度方向包含了导致模型判断错误的关键信息，所以更注重梯度的方向。对于使用符号函数的相关原因，欢迎大家一起讨论。**）

<p>这里的ε控制着扰动的大小。ε较小，对抗扰动不易被察觉，如果过大，扰动会很明显，容易被识别出来。</p>
<h5 id="Pytorch代码实现"><a href="#Pytorch代码实现" class="headerlink" title="Pytorch代码实现"></a>Pytorch代码实现</h5><p>FGSM的代码在Pytorch已经实现(<a href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html?highlight=fgsm">Adversarial Example Generation — PyTorch Tutorials 2.4.0+cu124 documentation</a>)</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">FGSM_attack</span><span class="token punctuation">(</span>image<span class="token punctuation">,</span> epsilons<span class="token punctuation">,</span> data_grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 符号函数</span>
    sign_data_grad <span class="token operator">=</span> data_grad<span class="token punctuation">.</span>sign<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 实现上述(5)公式</span>
    perturbed_image <span class="token operator">=</span> image <span class="token operator">+</span> epsilons <span class="token operator">*</span> sign_data_grad
    <span class="token comment"># 限制元素值在指定的范围内</span>
    perturbed_image <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>perturbed_image<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> perturbed_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="训练-攻击完整代码"><a href="#训练-攻击完整代码" class="headerlink" title="训练+攻击完整代码"></a>训练+攻击完整代码</h5><p>这部分自己搭建一个LeNet神经网络，并在MNIST手写数据集上进行训练，之后使用FGSM方法去生成对抗样本，测试训练的网络的分类准确率。</p>
<p>搭建LeNet模型</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 搭建LeNet模型</span>
<span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 卷积层</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        <span class="token comment"># 全连接层</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">120</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">84</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>
        img <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        img <span class="token operator">=</span> img<span class="token punctuation">.</span>view<span class="token punctuation">(</span>img<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out
    
net <span class="token operator">=</span> LeNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 使用设备
device &#x3D; torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

mean &#x3D; 0.1307
std &#x3D; 0.3801

# 对图像变换
transform &#x3D; transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((mean,), (std,))
]
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 训练数据集, 测试数据集
train_dataset &#x3D; datasets.MNIST(&#39;..&#x2F;datasets&#x2F;MNIST&#39;, train&#x3D;True, transform&#x3D;transform, download&#x3D;True) # len 60000
test_dataset &#x3D; datasets.MNIST(&#39;..&#x2F;datasets&#x2F;MNIST&#39;, train&#x3D;False, transform&#x3D;transform, download&#x3D;True) # len 10000

# 数据迭代器
train_dataloader &#x3D; DataLoader(train_dataset, batch_size&#x3D;64, shuffle&#x3D;True)  # len 938
test_dataloader &#x3D; DataLoader(test_dataset, batch_size&#x3D;64, shuffle&#x3D;True) # len 157<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">lr <span class="token operator">=</span> <span class="token number">1e-3</span>
epochs <span class="token operator">=</span> <span class="token number">30</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> <span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.0000001</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上都是为训练模型进行准备，下面开始对模型进行训练。</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python">train_loss &#x3D; []
train_acc &#x3D; []
val_loss &#x3D; []
val_acc &#x3D; []

for epoch in tqdm(range(epochs)):
    train_losses &#x3D; 0
    train_acces &#x3D; 0
    val_losses &#x3D; 0
    val_acces &#x3D; 0
    
    for x, y in train_dataloader:
        x, y &#x3D; x.to(device), y.to(device)
        output &#x3D; net(x)
        # 计算loss
        loss &#x3D; criterion(output, y)
        # 计算预测值
        _, pred &#x3D; torch.max(output, axis&#x3D;1)
        # 计算acc
        acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

        # 反向传播
        # 梯度清零
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses +&#x3D; loss.item()
        train_acces +&#x3D; acc.item()

    train_loss.append(train_losses &#x2F; len(train_dataloader))
    train_acc.append(train_acces &#x2F; len(train_dataloader))

    # 模型评估，这里就使用测试集进行验证了，实际应该再划分验证集
    net.eval()
    with torch.no_grad():
        for x, y in test_dataloader:
            x, y &#x3D; x.to(device), y.to(device)
            output &#x3D; net(x)
            loss &#x3D; criterion(output, y)
            scheduler.step(loss)
            _, pred &#x3D; torch.max(output, axis&#x3D;1)
            acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

            val_losses +&#x3D; loss.item()
            val_acces +&#x3D; acc.item()
        
        val_loss.append(val_losses &#x2F; len(test_dataloader))
        val_acc.append(val_acces &#x2F; len(test_dataloader))

    print(f&quot;epoch:&#123;epoch+1&#125;  train_loss:&#123;train_losses &#x2F; len(train_dataloader)&#125;, train_acc:&#123;train_acces &#x2F; len(train_dataloader)&#125;, val_loss:&#123;val_losses &#x2F; len(test_dataloader)&#125;, val_acc:&#123;val_acces &#x2F; len(test_dataloader)&#125;&quot;)

plt.plot(train_loss, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train loss&#39;)
plt.plot(val_loss, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val loss&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;loss&quot;)
plt.show()


plt.plot(train_acc, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train acc&#39;)
plt.plot(val_acc, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val acc&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;acc&quot;)
plt.show()

# 保存训练好的模型
PATH &#x3D; &#39;.&#x2F;fgsm_mnist_lenet.pth&#39;
torch.save(net, PATH)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"> <span class="token number">3</span><span class="token operator">%</span><span class="token operator">|</span>▎         <span class="token operator">|</span> <span class="token number">1</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">09</span><span class="token operator">&lt;</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">37</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>58s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">1</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.23495538183736173</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9266890991471215</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06610820889117042</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9808917197452229</span>
  <span class="token number">7</span><span class="token operator">%</span><span class="token operator">|</span>▋         <span class="token operator">|</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">18</span><span class="token operator">&lt;</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">22</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>37s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">2</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.0733803818781755</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9775286513859275</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06484267477741003</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.981687898089172</span>
 <span class="token number">10</span><span class="token operator">%</span><span class="token operator">|</span>█         <span class="token operator">|</span> <span class="token number">3</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">27</span><span class="token operator">&lt;</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>26s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">3</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.07191201691368797</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9779784115138592</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.0649121593102623</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9818869426751592</span>
 <span class="token number">13</span><span class="token operator">%</span><span class="token operator">|</span>█▎        <span class="token operator">|</span> <span class="token number">4</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">36</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">56</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>08s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">4</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.07054272936885433</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9782949093816631</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06256632373674186</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9819864649681529</span>
 <span class="token number">17</span><span class="token operator">%</span><span class="token operator">|</span>█▋        <span class="token operator">|</span> <span class="token number">5</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>00s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">5</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06927903689968307</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9787446695095949</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06133736597943553</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9821855095541401</span>
 <span class="token number">20</span><span class="token operator">%</span><span class="token operator">|</span>██        <span class="token operator">|</span> <span class="token number">6</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">54</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">31</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>80s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">6</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06818971440974456</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9791611140724946</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06043949323130926</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">23</span><span class="token operator">%</span><span class="token operator">|</span>██▎       <span class="token operator">|</span> <span class="token number">7</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">03</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">27</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>01s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">7</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06716231363557422</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9793943230277186</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.06052255801631102</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9821855095541401</span>
 <span class="token number">27</span><span class="token operator">%</span><span class="token operator">|</span>██▋       <span class="token operator">|</span> <span class="token number">8</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">12</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>06s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">8</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.0663046940549541</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9796108742004265</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05889693624933197</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9826831210191083</span>
 <span class="token number">30</span><span class="token operator">%</span><span class="token operator">|</span>███       <span class="token operator">|</span> <span class="token number">9</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">20</span><span class="token operator">&lt;</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">05</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>85s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">9</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06548984157693967</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9798773987206824</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05827412447613326</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">33</span><span class="token operator">%</span><span class="token operator">|</span>███▎      <span class="token operator">|</span> <span class="token number">10</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">59</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>97s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">10</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.0647883117283339</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9800106609808102</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.058015704478261765</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9823845541401274</span>
 <span class="token number">37</span><span class="token operator">%</span><span class="token operator">|</span>███▋      <span class="token operator">|</span> <span class="token number">11</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">38</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">47</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>83s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">11</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06416832418121429</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9802105543710021</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.0571870897817989</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">40</span><span class="token operator">%</span><span class="token operator">|</span>████      <span class="token operator">|</span> <span class="token number">12</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">47</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">38</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>82s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">12</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06361952270947095</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.980410447761194</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05745712512582066</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9821855095541401</span>
 <span class="token number">43</span><span class="token operator">%</span><span class="token operator">|</span>████▎     <span class="token operator">|</span> <span class="token number">13</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">56</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>86s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">13</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06314356109725117</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.980577025586354</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.056493567623150574</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9826831210191083</span>
 <span class="token number">47</span><span class="token operator">%</span><span class="token operator">|</span>████▋     <span class="token operator">|</span> <span class="token number">14</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">06</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">25</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>07s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">14</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06276988985686144</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9806603144989339</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05619345570078037</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9825835987261147</span>
 <span class="token number">50</span><span class="token operator">%</span><span class="token operator">|</span>█████     <span class="token operator">|</span> <span class="token number">15</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">15</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">19</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>28s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">15</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.0624012095647167</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9807769189765458</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05572936845836556</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9825835987261147</span>
 <span class="token number">53</span><span class="token operator">%</span><span class="token operator">|</span>█████▎    <span class="token operator">|</span> <span class="token number">16</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">25</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>42s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">16</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06199908992553602</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9808268923240938</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.055503922487923484</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9827826433121019</span>
 <span class="token number">57</span><span class="token operator">%</span><span class="token operator">|</span>█████▋    <span class="token operator">|</span> <span class="token number">17</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">35</span><span class="token operator">&lt;</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">04</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>58s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">17</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06172961615838174</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9809101812366737</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05556283311052307</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9825835987261147</span>
 <span class="token number">60</span><span class="token operator">%</span><span class="token operator">|</span>██████    <span class="token operator">|</span> <span class="token number">18</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">45</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">57</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>77s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">18</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06138933949651065</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9810934168443497</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.054954844592198446</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">63</span><span class="token operator">%</span><span class="token operator">|</span>██████▎   <span class="token operator">|</span> <span class="token number">19</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">02</span><span class="token punctuation">:</span><span class="token number">54</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">43</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>43s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">19</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06114780887026094</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9812599946695096</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.054730413220585535</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">67</span><span class="token operator">%</span><span class="token operator">|</span>██████▋   <span class="token operator">|</span> <span class="token number">20</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">03</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">33</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>36s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">20</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.060946285610458555</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9813266257995735</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.0545463676187714</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.982484076433121</span>
 <span class="token number">70</span><span class="token operator">%</span><span class="token operator">|</span>███████   <span class="token operator">|</span> <span class="token number">21</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">13</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">25</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>45s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">21</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06072342605652736</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9813266257995735</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05441298720776845</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9826831210191083</span>
 <span class="token number">73</span><span class="token operator">%</span><span class="token operator">|</span>███████▎  <span class="token operator">|</span> <span class="token number">22</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">22</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">15</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>47s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">22</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.060515265972383304</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9814598880597015</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.054409430030092694</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9827826433121019</span>
 <span class="token number">77</span><span class="token operator">%</span><span class="token operator">|</span>███████▋  <span class="token operator">|</span> <span class="token number">23</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">31</span><span class="token operator">&lt;</span><span class="token number">01</span><span class="token punctuation">:</span><span class="token number">04</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>28s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">23</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.060337386706641426</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9815265191897654</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05422507992287161</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9827826433121019</span>
 <span class="token number">80</span><span class="token operator">%</span><span class="token operator">|</span>████████  <span class="token operator">|</span> <span class="token number">24</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">39</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">53</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>98s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">24</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.0602028726938683</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9815931503198294</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.055592933979632844</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9822850318471338</span>
 <span class="token number">83</span><span class="token operator">%</span><span class="token operator">|</span>████████▎ <span class="token operator">|</span> <span class="token number">25</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">48</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">44</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>93s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">25</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.06003174935030674</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9816098081023454</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05382291597438751</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9830812101910829</span>
 <span class="token number">87</span><span class="token operator">%</span><span class="token operator">|</span>████████▋ <span class="token operator">|</span> <span class="token number">26</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">03</span><span class="token punctuation">:</span><span class="token number">58</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">36</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>08s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">26</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.05991259947724974</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9816597814498934</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05371515328586576</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9831807324840764</span>
 <span class="token number">90</span><span class="token operator">%</span><span class="token operator">|</span>█████████ <span class="token operator">|</span> <span class="token number">27</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">06</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">26</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span>91s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">27</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.059782677139443505</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9816930970149254</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.05411682381727703</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9826831210191083</span>
 <span class="token number">93</span><span class="token operator">%</span><span class="token operator">|</span>█████████▎<span class="token operator">|</span> <span class="token number">28</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">16</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">18</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>08s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">28</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.05965107033448194</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9817097547974414</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.053595036425432015</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9829816878980892</span>
 <span class="token number">97</span><span class="token operator">%</span><span class="token operator">|</span>█████████▋<span class="token operator">|</span> <span class="token number">29</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">25</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">09</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>30s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">29</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.059546736687092164</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9816930970149254</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.0533999552309608</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9830812101910829</span>
<span class="token number">100</span><span class="token operator">%</span><span class="token operator">|</span>██████████<span class="token operator">|</span> <span class="token number">30</span><span class="token operator">/</span><span class="token number">30</span> <span class="token punctuation">[</span><span class="token number">04</span><span class="token punctuation">:</span><span class="token number">35</span><span class="token operator">&lt;</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span>19s<span class="token operator">/</span>it<span class="token punctuation">]</span>
epoch<span class="token punctuation">:</span><span class="token number">30</span>  train_loss<span class="token punctuation">:</span><span class="token number">0.05946045447769426</span><span class="token punctuation">,</span> train_acc<span class="token punctuation">:</span><span class="token number">0.9817097547974414</span><span class="token punctuation">,</span> val_loss<span class="token punctuation">:</span><span class="token number">0.053309381550925364</span><span class="token punctuation">,</span> val_acc<span class="token punctuation">:</span><span class="token number">0.9829816878980892</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>经过30轮的训练，模型的再训练集和验证集上的准确率大概达到了98%。</p>
<p>因为训练轮数很少，所以这条曲线看起来并不平滑，但基本收敛，实际应用时，可以将训练轮数增加。</p>
<img src="/posts/f891610e/2.png" class loading="lazy">

<img src="/posts/f891610e/3.png" class loading="lazy">

<p>如果已经有训练好的模型，可以使用以下代码进行加载：</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 自己根据实际情况修改模型存储路径
PATH &#x3D; &#39;.&#x2F;fgsm_mnist_lenet.pth&#39;

net &#x3D; torch.load(PATH)
net &#x3D; net.to(device)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>为了方便之后的操作，这里修改一下batch_size的大小，修改为1。</p>
<pre class="line-numbers language-none"><code class="language-none">test_dataloader &#x3D; DataLoader(test_dataset, batch_size&#x3D;1, shuffle&#x3D;True) <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>对对抗样本测试的函数</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 测试函数  FGSM</span>
<span class="token keyword">def</span> <span class="token function">test_FGSM</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> epsilons<span class="token punctuation">)</span><span class="token punctuation">:</span>
    correct <span class="token operator">=</span> <span class="token number">0</span>
    adv_examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">for</span> data<span class="token punctuation">,</span> target <span class="token keyword">in</span> test_dataloader<span class="token punctuation">:</span>
        data<span class="token punctuation">,</span> target <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># 设置张量的属性，对于攻击十分关键</span>
        data<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

        <span class="token comment"># 前向传播</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> init_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># init_pred = output.max(1, keepdim=True)[1]</span>
        <span class="token comment"># print(init_pred)</span>
        <span class="token comment"># print(target)</span>

        <span class="token comment"># 如果分类错误就不去扰动图像</span>
        <span class="token keyword">if</span> init_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> target<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># print("Original image's predict is wrong!")</span>
            <span class="token comment"># print("init_pred:", init_pred)</span>
            <span class="token comment"># print("true_pred:", target)</span>
            <span class="token keyword">continue</span>
        
        <span class="token comment"># 负对数似然损失</span>
        loss <span class="token operator">=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 收集数据损失</span>
        <span class="token comment"># data.grad 是一个完整的梯度张量，可以用于进一步的梯度计算，</span>
        <span class="token comment"># 而 data.grad.data 是梯度张量的数据内容，通常用于查看或操作梯度的具体数值，但不用于梯度计算。</span>
        data_grad <span class="token operator">=</span> data<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data
        <span class="token comment"># 使用FGSM进行攻击</span>
        perturbed_data <span class="token operator">=</span> FGSM_attack<span class="token punctuation">(</span>data<span class="token punctuation">,</span> epsilons<span class="token punctuation">,</span> data_grad<span class="token punctuation">)</span>
        <span class="token comment"># 对扰动后的图像进行重新分类</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span>perturbed_data<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> attack_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 模型对对抗样本的分类还是正确的</span>
        <span class="token keyword">if</span> attack_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> target<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            correct <span class="token operator">+=</span> <span class="token number">1</span>
            <span class="token comment"># 保存噪声为0的5个图像用于后期的可视化</span>
            <span class="token keyword">if</span><span class="token punctuation">(</span>epsilons <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                adv_ex<span class="token operator">=</span> perturbed_data<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># 保留正确标签, 攻击后标签, 攻击后图像</span>
                adv_examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>init_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attack_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> adv_ex<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 保留5个攻击后分类错误的实例</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">:</span>
                adv_ex <span class="token operator">=</span> perturbed_data<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token comment"># 同样保存正确标签，攻击后标签，攻击后图像</span>
                adv_examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>init_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attack_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> adv_ex<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 被攻击后的分类准确率</span>
    attack_acc <span class="token operator">=</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilons<span class="token punctuation">,</span> correct<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> attack_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
	
    <span class="token comment"># 返回当前保存的攻击样本的相关内容和测试准确率</span>
    <span class="token keyword">return</span> adv_examples<span class="token punctuation">,</span> attack_acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>接下来就是使用不同的eplison值生成对抗样本，并使用上述的测试函数进行测试。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">accuracies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token comment"># eps=0表示未受到攻击的测试准确性</span>
epsilons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">.2</span><span class="token punctuation">,</span> <span class="token number">.4</span><span class="token punctuation">,</span> <span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">.6</span><span class="token punctuation">,</span> <span class="token number">.65</span><span class="token punctuation">,</span> <span class="token number">.7</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> eps <span class="token keyword">in</span> epsilons<span class="token punctuation">:</span>
    ex<span class="token punctuation">,</span> acc <span class="token operator">=</span> test_FGSM<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> eps<span class="token punctuation">)</span>
    accuracies<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>
    examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ex<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/f891610e/4.png" class loading="lazy">

<p>可视化不同epsilon下的准确率</p>
<pre class="line-numbers language-none"><code class="language-none">plt.figure(figsize&#x3D;(5,5))
plt.plot(epsilons, accuracies, &quot;*-&quot;)
plt.title(&quot;Accuracy vs Epsilon -- FGSM&quot;)
plt.xlabel(&quot;Epsilon&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/f891610e/5.png" class loading="lazy">

<p>可视化保存的不同epsilon下生成的对抗样本。</p>
<pre class="line-numbers language-none"><code class="language-none">index &#x3D; 0
plt.figure(figsize&#x3D;(8, 10))

for i in range(len(epsilons)):
    for j in range(len(examples[i])):
        index +&#x3D; 1
        plt.subplot(len(epsilons), len(examples[i]), index)
        if j &#x3D;&#x3D; 0:
            plt.ylabel(&quot;Eps: &#123;&#125;&quot;.format(epsilons[i]), fontsize&#x3D;14)
        
        init_pred, attack_pred, example &#x3D; examples[i][j]
        plt.title(&quot;&#123;&#125; --&gt; &#123;&#125;&quot;.format(init_pred, attack_pred))
        plt.imshow(example)

plt.tight_layout()
# plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/f891610e/6.png" class loading="lazy">

<p>当epsilon为0时，其实就是没有做什么扰动，可以看到，随着epsilon值的提高，图像的扰动也是可以被察觉到的，值越大，扰动越明显。</p>
<h5 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h5><p><a href="https://www.cnblogs.com/tangweijqxx/p/10615950.html">2.基于梯度的攻击——FGSM - 机器学习安全小白 - 博客园 (cnblogs.com)</a>（这篇文章中有作者对使用符号函数确定方向的思考）</p>
<p><a href="https://www.cnblogs.com/hickey2048/p/15025612.html">对抗攻击(一) FGSM - HickeyZhang - 博客园 (cnblogs.com)</a>（这篇文章对FGSM中要让损失函数增加的数学解释）</p>
<p><a href="https://blog.csdn.net/weixin_41466575/article/details/116747323">对抗样本之FGSM原理&amp;coding_fgsm是有目标还是无目标的-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_45508265/article/details/118411362">GAN 系列的探索与pytorch实现 (数字对抗样本生成)_pytorch课程设计-CSDN博客</a>（由于我的能力有限，所以本文的代码主要参考参考文献中的后两篇文章）</p>
<h4 id="BIM-I-FGSM"><a href="#BIM-I-FGSM" class="headerlink" title="BIM(I-FGSM)"></a><strong>BIM(I-FGSM)</strong></h4><h5 id="I-FGSM原理"><a href="#I-FGSM原理" class="headerlink" title="I-FGSM原理"></a>I-FGSM原理</h5><p>在FGSM算法中，生成的对抗样本是通过x‘&#x3D;x+μ进行单步攻击，直接在原图像中加上扰动得到的，其中μ&#x3D;ϵ∗sign(∇xJ(x,y))，也就是说是将每个像素点都变化了ε这么多。而I-FGSM算法是使用迭代的方法，寻找各个像素点的扰动，在FGSM的基础上进行了多次迭代。</p>
<p>迭代的作用就是使新样本在旧样本的基础上每个像素点变化α，然后通过裁剪，控制得到的新样本各像素都在原始图像的ε领域内。这句话通过公式可以很好理解：</p>
<div>
$$
\boldsymbol{X_0^{adv}} = \boldsymbol{X}, \quad \boldsymbol{X_{N+1}^{adv}}=Clip_{\boldsymbol{X}, \epsilon}\{\boldsymbol{X_{N}^{adv}} + \alpha sign(\nabla_XJ(\boldsymbol{X_{N}^{adv}, y_{true}})) \} \tag{1}
$$
</div>

<h5 id="Pytorch代码实现-1"><a href="#Pytorch代码实现-1" class="headerlink" title="Pytorch代码实现"></a>Pytorch代码实现</h5><p><strong>在许多文章中，我看到对于BIM算法的实现就是简单简单的循环了FGSM算法(这样做我认为是不对的，因为忽视了ε和α参数的使用)，但是从公式中我们是可以看到，需要用α控制移动步长和ε来控制像素的变化范围，所以结合自己的理解，我修改了原始FGSM算法，得到以下I-FGSM的核心算法。</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">I_FGSM_attack</span><span class="token punctuation">(</span>ori_images<span class="token punctuation">,</span> adv_images<span class="token punctuation">,</span> epsilon<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> data_grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sign_data_grad <span class="token operator">=</span> data_grad<span class="token punctuation">.</span>sign<span class="token punctuation">(</span><span class="token punctuation">)</span>
    perturbed_image <span class="token operator">=</span> adv_images <span class="token operator">+</span> alpha <span class="token operator">*</span> sign_data_grad
    perturbed_image <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>perturbed_image<span class="token punctuation">,</span> ori_images <span class="token operator">-</span> epsilon<span class="token punctuation">,</span> ori_images <span class="token operator">+</span> epsilon<span class="token punctuation">)</span>

    <span class="token keyword">return</span> perturbed_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>相比原始的FGSM算法</p>
<pre class="line-numbers language-none"><code class="language-none">def FGSM_attack(image, epsilons, data_grad):
    sign_data_grad &#x3D; data_grad.sign()
    perturbed_image &#x3D; image + epsilons * sign_data_grad
    perturbed_image &#x3D; torch.clamp(perturbed_image, 0, 1)

    return perturbed_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>首先是传入函数参数的变化，I-FGSM函数需要计算原始图像的ε的范围，所以有参数ori_images和epsilon，然后对每次得到的样本进行扰动，最后将其范围限制在原始图像像素-ε和原始图像+ε。</p>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><p>这里就使用上文FGSM中所训练得到的模型，所以训练等代码和前面的一样，直接从测试开始。</p>
<pre class="line-numbers language-none"><code class="language-none"># 测试函数  BMI&#x2F;I-FGSM
def test_I_FGSM(model, device, test_dataloader, epsilons, alpha, iters&#x3D;40):
    correct &#x3D; 0
    adv_examples &#x3D; []

    for data, target in test_dataloader:
        data, target &#x3D; data.to(device), target.to(device)
        # 设置张量的属性，对于攻击十分关键
        data.requires_grad &#x3D; True

        # 前向传播
        output &#x3D; model(data)
        _, init_pred &#x3D; torch.max(output, axis&#x3D;1)

        # 如果分类错误就不去扰动图像
        if init_pred.item() !&#x3D; target.item():
            continue
        
        # 保存原始数据，并且不共享，用于在torch.clamp中使用
        ori_data &#x3D; data.detach().clone()

        # epsilons &#x3D;&#x3D; 0时,就是不添加任何扰动进行一次测试,反复迭代就是浪费计算资源
        if alpha !&#x3D; 0:
            for k in range(iters):
                # 负对数似然损失
                output &#x3D; model(data)
                loss &#x3D; F.nll_loss(output, target)
                model.zero_grad()
                loss.backward()

                # 收集数据损失
                # data.grad 是一个完整的梯度张量，可以用于进一步的梯度计算，
                # 而 data.grad.data 是梯度张量的数据内容，通常用于查看或操作梯度的具体数值，但不用于梯度计算。
                data_grad &#x3D; data.grad.data
                # 使用I-FGSM进行攻击, 每次都是在上一次的基础上进行扰动
                data &#x3D; I_FGSM_attack(ori_data, data, epsilons, alpha, data_grad)
                # 迭代求对抗样本中，需要及时的使用截断detach将重复使用变量，变成计算图中的叶子节点；
                # 由于变成了叶子节点，后续还需要对该变量求偏导，故添加requires_grad参数
                # 在I-FGSM中是要计算损失函数对xt的梯度，而data(就是这里的xt)是通过函数计算出来的,
                # 他的grad_fn是其对应的类型实际计算中不会计算L对xt的梯度，xt只是一个中间过程
                # 这里用detach_()把他变成一个叶子结点,那么就可以计算到L对xt的梯度
                data.detach_()
                data.requires_grad &#x3D; True

        # 对扰动后的图像进行重新分类
        output &#x3D; model(data)
        _, attack_pred &#x3D; torch.max(output, axis&#x3D;1)
        
        # 说明攻击后的分类还是正确的
        if attack_pred.item() &#x3D;&#x3D; target.item():
            correct +&#x3D; 1
            # 保存噪声为0的5个图像用于后期的可视化
            if(epsilons &#x3D;&#x3D; 0) and (len(adv_examples) &lt; 5):
                adv_ex&#x3D; data.squeeze().detach().cpu().numpy()
                # 保留正确标签, 攻击后标签, 攻击后图像
                adv_examples.append((init_pred.item(), attack_pred.item(), adv_ex))
        # 保留攻击后分类错误的实例
        else:
            if len(adv_examples) &lt; 5:
                adv_ex &#x3D; data.squeeze().detach().cpu().numpy()
                adv_examples.append((init_pred.item(), attack_pred.item(), adv_ex))
    
    # 被攻击后的分类准确率
    attack_acc &#x3D; correct &#x2F; len(test_dataloader)
    print(&quot;Alpha: &#123;:.4f&#125;\tTest Accuracy &#x3D; &#123;&#125; &#x2F; &#123;&#125; &#x3D; &#123;&#125;&quot;.format(alpha, correct, len(test_dataloader), attack_acc))

    return adv_examples, attack_acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这里为了方便，我没有调整ε的值，固定ε的值，调整了α，同时为了方便，也需要将测试数据加载器的batch_size设置为1。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">accuracies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

epsilons <span class="token operator">=</span> <span class="token number">0.3</span>
alphas <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">/</span><span class="token number">32</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> alpha <span class="token keyword">in</span> alphas<span class="token punctuation">:</span>
    ex<span class="token punctuation">,</span> acc <span class="token operator">=</span> test_I_FGSM<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> epsilons<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span>
    accuracies<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>
    examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ex<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 测试结果
Alpha: 0.0000	Test Accuracy &#x3D; 9833 &#x2F; 10000 &#x3D; 0.9833
Alpha: 0.0039	Test Accuracy &#x3D; 9378 &#x2F; 10000 &#x3D; 0.9378
Alpha: 0.0078	Test Accuracy &#x3D; 8126 &#x2F; 10000 &#x3D; 0.8126
Alpha: 0.0156	Test Accuracy &#x3D; 3150 &#x2F; 10000 &#x3D; 0.315
Alpha: 0.0312	Test Accuracy &#x3D; 2407 &#x2F; 10000 &#x3D; 0.2407
Alpha: 0.0625	Test Accuracy &#x3D; 2204 &#x2F; 10000 &#x3D; 0.2204<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可视化结果</p>
<pre class="line-numbers language-none"><code class="language-none">plt.figure(figsize&#x3D;(5,5))
plt.plot(alphas, accuracies, &quot;*-&quot;)
plt.title(&quot;Accuracy vs Alpha -- I-FGSM&quot;)
plt.xlabel(&quot;Alpha&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/f891610e/7.png" class loading="lazy">

<p>在这个结果中，α从0.0078到0.0156的过程有大量的样本被攻击成功，读者可以试着修改α，ε还有iters等参数去进行修改。</p>
<pre class="line-numbers language-none"><code class="language-none">index &#x3D; 0
plt.figure(figsize&#x3D;(8, 10))

for i in range(len(alphas)):
    for j in range(len(examples[i])):
        index +&#x3D; 1
        plt.subplot(len(alphas), len(examples[i]), index)
        if j &#x3D;&#x3D; 0:
            plt.ylabel(&quot;Alpha: &#123;:.4f&#125;&quot;.format(alphas[i]), fontsize&#x3D;14)
        
        init_pred, attack_pred, example &#x3D; examples[i][j]
        plt.title(&quot;&#123;&#125; --&gt; &#123;&#125;&quot;.format(init_pred, attack_pred))
        plt.imshow(example)

plt.tight_layout()
# plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/f891610e/8.png" class loading="lazy">

<p>从可视化的结果中也可以看出，随着alpha的增大，对图像扰动的增加也越来越明显。</p>
<h5 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h5><p><a href="https://blog.csdn.net/qq_51171586/article/details/128165071">对抗样本生成方法综述（FGSM、BIM\I-FGSM、PGD、JSMA、C&amp;W、DeepFool）-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_41466575/article/details/118928248">对抗样本之BIM原理&amp;coding_迭代攻击( bim)-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/ilalaaa/article/details/106070091">对抗样本生成算法之BIM算法_bim攻击算法-CSDN博客</a></p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>对抗样本生成与防御</tag>
        <tag>对抗攻击</tag>
      </tags>
  </entry>
  <entry>
    <title>对抗攻击:PGD算法</title>
    <url>/posts/a1b31fc5/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>之前介绍了FGSM算法还有I-FGSM算法，接下来再看看FGSM算法的拓展PGD算法。</p>
<h4 id="PGD原理"><a href="#PGD原理" class="headerlink" title="PGD原理"></a>PGD原理</h4><p>PGD算法在论文[<a href="https://arxiv.org/abs/1706.06083">1706.06083] Towards Deep Learning Models Resistant to Adversarial Attacks (arxiv.org)</a>中提出，它既是产生对抗样本的攻击算法，也是对抗训练的防御算法。除此之外，PGD算法也是一阶中的最强攻击（一阶是指利用一阶导数）</p>
<p>设想目标模型如果是一个线性模型，损失函数对输入的导数一定是一个固定值，一次迭代和多次迭代时扰动的方向都不会发生改变，但是，如果目标模型为非线性，每次迭代之间的方向都有可能会发生变化，这时FGSM的单次迭代效果肯定不如PGD的效果好。FGSM算法通过一步计算，可能达不到最优效果，而PGD算法则是每次走一小步，但是多走几次，如果超过了扰动半径为ε的空间，就重新映射回来。</p>
<p>下面来看一下PGD算法的公式：</p>
<div>
$$
x^{t+1} = \Pi_{x+S}(x^t+\alpha sgn(\nabla_xL(\theta,x,y))) \tag{1}
$$
</div>
理解了FGSM和I-FGSM以后，这个公式肯定也是非常好理解的。这里主要看一下公式最前面的投影到x+S的意思。

<p>就是通过一系列操作得到对抗样本后，将对抗样本减去原始图像得到了扰动值，然后将扰动值限制在-ε到+ε之间，得到了新的扰动值，原始图像加上新的扰动值就是最终生成的对抗样本。</p>
<p>关于我对(1)中sgn(L(θ,x,y)’)的理解可以看我在这篇文章中2.1节写的内容。<a href="https://lengnian.github.io/posts/f891610e/#FGSM%E5%8E%9F%E7%90%86">对抗攻击:FGSM和BIM算法 | WeiSJ&amp;HEXO (lengnian.github.io)</a></p>
<h4 id="Pytorch代码实现"><a href="#Pytorch代码实现" class="headerlink" title="Pytorch代码实现"></a>Pytorch代码实现</h4><p>看看PGD的核心代码：</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># PGD攻击方式,属于FGSM攻击的变体
def PGD_attack(model, image, label, epsilon&#x3D;0.8, alpha&#x3D;0.1, iters&#x3D;40):
    image &#x3D; image.to(device)
    label &#x3D; label.to(device)
    loss &#x3D; nn.CrossEntropyLoss()

    ori_image &#x3D; image.data

    for i in range(iters):
        image.requires_grad &#x3D; True
        output &#x3D; model(image)

        model.zero_grad()
        cost &#x3D; loss(output, label).to(device)
        cost.backward()

        # 对抗样本 &#x3D; 原始图像 + 扰动
        adv_image &#x3D; image + alpha * image.grad.sign()
        # 限制扰动范围
        eta &#x3D; torch.clamp(adv_image - ori_image, min&#x3D;-epsilon, max&#x3D;epsilon)
        # 进行下一轮的对抗样本生成
        image &#x3D; torch.clamp(ori_image + eta, min&#x3D;0, max&#x3D;1).detach()

    return image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="训练-攻击完整代码"><a href="#训练-攻击完整代码" class="headerlink" title="训练+攻击完整代码"></a>训练+攻击完整代码</h4><p>完整代码是使用自己在MNIST手写数据集上训练的LeNet，使用PGD算法产生对抗样本攻击模型。</p>
<p>首先是模型搭建</p>
<pre class="line-numbers language-Python" data-language="Python"><code class="language-Python"># 搭建LeNet模型
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()

        # 卷积层
        self.conv &#x3D; nn.Sequential(
            nn.Conv2d(in_channels&#x3D;1, out_channels&#x3D;6, kernel_size&#x3D;5, padding&#x3D;2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2),
            nn.Conv2d(in_channels&#x3D;6, out_channels&#x3D;16, kernel_size&#x3D;5),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size&#x3D;2, stride&#x3D;2)
        )

        # 全连接层
        self.fc &#x3D; nn.Sequential(
            nn.Linear(in_features&#x3D;16 * 5 * 5, out_features&#x3D;120),
            nn.ReLU(),
            nn.Linear(in_features&#x3D;120, out_features&#x3D;84),
            nn.ReLU(),
            nn.Linear(in_features&#x3D;84, out_features&#x3D;10)
        )

    def forward(self, img):
        img &#x3D; self.conv(img)
        img &#x3D; img.view(img.size(0), -1)
        out &#x3D; self.fc(img)
        return out


net &#x3D; LeNet()
net &#x3D; net.to(device)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">mean <span class="token operator">=</span> <span class="token number">0.1307</span>
std <span class="token operator">=</span> <span class="token number">0.3801</span>

<span class="token comment"># 对图像变换</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span>mean<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>std<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>
<span class="token punctuation">)</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 训练数据集, 测试数据集</span>
train_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'../datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 60000</span>
test_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">'../datasets/MNIST'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 10000</span>

<span class="token comment"># 数据迭代器</span>
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># len 938</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># len 157</span>

lr <span class="token operator">=</span> <span class="token number">1e-3</span>
epochs <span class="token operator">=</span> <span class="token number">30</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> <span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0.0000001</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>之后就是进行模型的训练</p>
<pre class="line-numbers language-none"><code class="language-none">train_loss &#x3D; []
train_acc &#x3D; []
val_loss &#x3D; []
val_acc &#x3D; []

for epoch in tqdm(range(epochs)):
    train_losses &#x3D; 0
    train_acces &#x3D; 0
    val_losses &#x3D; 0
    val_acces &#x3D; 0
    
    for x, y in train_dataloader:
        x, y &#x3D; x.to(device), y.to(device)
        output &#x3D; net(x)
        # 计算loss
        loss &#x3D; criterion(output, y)
        # 计算预测值
        _, pred &#x3D; torch.max(output, axis&#x3D;1)
        # 计算acc
        acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

        # 反向传播
        # 梯度清零
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_losses +&#x3D; loss.item()
        train_acces +&#x3D; acc.item()

    train_loss.append(train_losses &#x2F; len(train_dataloader))
    train_acc.append(train_acces &#x2F; len(train_dataloader))

    # 模型评估
    net.eval()
    with torch.no_grad():
        for x, y in test_dataloader:
            x, y &#x3D; x.to(device), y.to(device)
            output &#x3D; net(x)
            loss &#x3D; criterion(output, y)
            scheduler.step(loss)
            _, pred &#x3D; torch.max(output, axis&#x3D;1)
            acc &#x3D; torch.sum(y &#x3D;&#x3D; pred) &#x2F; output.shape[0]

            val_losses +&#x3D; loss.item()
            val_acces +&#x3D; acc.item()
        
        val_loss.append(val_losses &#x2F; len(test_dataloader))
        val_acc.append(val_acces &#x2F; len(test_dataloader))

    print(f&quot;epoch:&#123;epoch+1&#125;  train_loss:&#123;train_losses &#x2F; len(train_dataloader)&#125;, train_acc:&#123;train_acces &#x2F; len(train_dataloader)&#125;, val_loss:&#123;val_losses &#x2F; len(test_dataloader)&#125;, val_acc:&#123;val_acces &#x2F; len(test_dataloader)&#125;&quot;)

plt.plot(train_loss, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train loss&#39;)
plt.plot(val_loss, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val loss&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;loss&quot;)
plt.show()


plt.plot(train_acc, color&#x3D;&#39;green&#39;, label&#x3D;&#39;train acc&#39;)
plt.plot(val_acc, color&#x3D;&#39;blue&#39;, label&#x3D;&#39;val acc&#39;)
plt.legend()
plt.xlabel(&quot;epoch&quot;)
plt.ylabel(&quot;acc&quot;)
plt.show()

PATH &#x3D; &#39;.&#x2F;pgd_mnist_lenet.pth&#39;
torch.save(net, PATH)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-none"><code class="language-none"> 3%|▎         | 1&#x2F;30 [00:11&lt;05:30, 11.39s&#x2F;it]
epoch:1  train_loss:0.22698007381932217, train_acc:0.9306369936034116, val_loss:0.06039735894490057, val_acc:0.9801950636942676
  7%|▋         | 2&#x2F;30 [00:21&lt;05:05, 10.90s&#x2F;it]
epoch:2  train_loss:0.06332400290375904, train_acc:0.980527052238806, val_loss:0.059576493097694624, val_acc:0.9810907643312102
 10%|█         | 3&#x2F;30 [00:32&lt;04:48, 10.70s&#x2F;it]
epoch:3  train_loss:0.06275213833576612, train_acc:0.9806603144989339, val_loss:0.05885813971328887, val_acc:0.9812898089171974
 13%|█▎        | 4&#x2F;30 [00:42&lt;04:27, 10.28s&#x2F;it]
epoch:4  train_loss:0.0622245114612411, train_acc:0.9808768656716418, val_loss:0.05824386749778441, val_acc:0.9813893312101911
 17%|█▋        | 5&#x2F;30 [00:52&lt;04:18, 10.34s&#x2F;it]
epoch:5  train_loss:0.06163326848739151, train_acc:0.9810267857142857, val_loss:0.05831151023495254, val_acc:0.9815883757961783
 20%|██        | 6&#x2F;30 [01:03&lt;04:10, 10.45s&#x2F;it]
epoch:6  train_loss:0.06115677414128362, train_acc:0.9812766524520256, val_loss:0.05730052098222552, val_acc:0.9822850318471338
 23%|██▎       | 7&#x2F;30 [01:14&lt;04:06, 10.71s&#x2F;it]
epoch:7  train_loss:0.06073849064423077, train_acc:0.9814432302771855, val_loss:0.0568622479387292, val_acc:0.9823845541401274
 27%|██▋       | 8&#x2F;30 [01:24&lt;03:52, 10.57s&#x2F;it]
epoch:8  train_loss:0.060348112858943086, train_acc:0.9815598347547975, val_loss:0.05643120593136283, val_acc:0.9825835987261147
 30%|███       | 9&#x2F;30 [01:36&lt;03:53, 11.12s&#x2F;it]
epoch:9  train_loss:0.06001822312654399, train_acc:0.9816431236673774, val_loss:0.0560989200529067, val_acc:0.9829816878980892
 33%|███▎      | 10&#x2F;30 [01:47&lt;03:40, 11.01s&#x2F;it]
epoch:10  train_loss:0.05972875392924287, train_acc:0.9816264658848614, val_loss:0.055768566766670746, val_acc:0.9828821656050956
 37%|███▋      | 11&#x2F;30 [02:00&lt;03:37, 11.44s&#x2F;it]
epoch:11  train_loss:0.05945397531891714, train_acc:0.9817264125799574, val_loss:0.05558454929880655, val_acc:0.9829816878980892
 40%|████      | 12&#x2F;30 [02:10&lt;03:21, 11.21s&#x2F;it]
epoch:12  train_loss:0.05924657509594695, train_acc:0.9818929904051172, val_loss:0.05523945889465368, val_acc:0.9830812101910829
 43%|████▎     | 13&#x2F;30 [02:24&lt;03:22, 11.89s&#x2F;it]
epoch:13  train_loss:0.05902852281344248, train_acc:0.9819429637526652, val_loss:0.05497432513508032, val_acc:0.9830812101910829
 47%|████▋     | 14&#x2F;30 [02:39&lt;03:25, 12.82s&#x2F;it]
epoch:14  train_loss:0.05884089107651994, train_acc:0.9819929371002132, val_loss:0.05479716256581199, val_acc:0.9829816878980892
 50%|█████     | 15&#x2F;30 [02:51&lt;03:09, 12.62s&#x2F;it]
epoch:15  train_loss:0.05864403824453979, train_acc:0.9820429104477612, val_loss:0.05505548159378302, val_acc:0.9826831210191083
 53%|█████▎    | 16&#x2F;30 [03:03&lt;02:54, 12.50s&#x2F;it]
epoch:16  train_loss:0.05848915197192502, train_acc:0.9820262526652452, val_loss:0.05448126511731345, val_acc:0.9830812101910829
 57%|█████▋    | 17&#x2F;30 [03:14&lt;02:35, 11.96s&#x2F;it]
epoch:17  train_loss:0.05836289135941755, train_acc:0.9820429104477612, val_loss:0.05426665444437201, val_acc:0.9830812101910829
 60%|██████    | 18&#x2F;30 [03:26&lt;02:25, 12.09s&#x2F;it]
epoch:18  train_loss:0.05820710067130895, train_acc:0.9821761727078892, val_loss:0.054053511006377966, val_acc:0.9831807324840764
 63%|██████▎   | 19&#x2F;30 [03:38&lt;02:12, 12.01s&#x2F;it]
epoch:19  train_loss:0.058103710266032706, train_acc:0.9821761727078892, val_loss:0.05439911206745228, val_acc:0.9831807324840764
 67%|██████▋   | 20&#x2F;30 [03:49&lt;01:56, 11.64s&#x2F;it]
epoch:20  train_loss:0.058076391998888935, train_acc:0.9821595149253731, val_loss:0.05379035640181677, val_acc:0.9831807324840764
 70%|███████   | 21&#x2F;30 [04:01&lt;01:46, 11.86s&#x2F;it]
epoch:21  train_loss:0.0578732310768479, train_acc:0.9822261460554371, val_loss:0.05364397724283634, val_acc:0.98328025477707
 73%|███████▎  | 22&#x2F;30 [04:12&lt;01:32, 11.57s&#x2F;it]
epoch:22  train_loss:0.05777005526695901, train_acc:0.9823260927505331, val_loss:0.05357255679510747, val_acc:0.9834792993630573
 77%|███████▋  | 23&#x2F;30 [04:25&lt;01:24, 12.03s&#x2F;it]
epoch:23  train_loss:0.05769451945396597, train_acc:0.982359408315565, val_loss:0.05339814494749543, val_acc:0.9833797770700637
 80%|████████  | 24&#x2F;30 [04:38&lt;01:14, 12.38s&#x2F;it]
epoch:24  train_loss:0.05762437407273863, train_acc:0.982359408315565, val_loss:0.05328276433023939, val_acc:0.9833797770700637
 83%|████████▎ | 25&#x2F;30 [04:52&lt;01:03, 12.70s&#x2F;it]
epoch:25  train_loss:0.057558819814188, train_acc:0.982409381663113, val_loss:0.0532996576148898, val_acc:0.98328025477707
 87%|████████▋ | 26&#x2F;30 [05:04&lt;00:50, 12.55s&#x2F;it]
epoch:26  train_loss:0.05746660577848172, train_acc:0.9823927238805971, val_loss:0.05423711911051469, val_acc:0.9826831210191083
 90%|█████████ | 27&#x2F;30 [05:15&lt;00:35, 11.98s&#x2F;it]
epoch:27  train_loss:0.05736568021470868, train_acc:0.982442697228145, val_loss:0.053538712084139135, val_acc:0.9829816878980892
 93%|█████████▎| 28&#x2F;30 [05:25&lt;00:23, 11.61s&#x2F;it]
epoch:28  train_loss:0.057331098256998066, train_acc:0.9825093283582089, val_loss:0.053118417597120736, val_acc:0.9833797770700637
 97%|█████████▋| 29&#x2F;30 [05:36&lt;00:11, 11.26s&#x2F;it]
epoch:29  train_loss:0.05727743341136716, train_acc:0.982525986140725, val_loss:0.05280723534694985, val_acc:0.9834792993630573
100%|██████████| 30&#x2F;30 [05:48&lt;00:00, 11.62s&#x2F;it]
epoch:30  train_loss:0.05723029628682977, train_acc:0.982592617270789, val_loss:0.05286279270294935, val_acc:0.9835788216560509<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/a1b31fc5/1.png" class loading="lazy">

<img src="/posts/a1b31fc5/2.png" class loading="lazy">

<p>为了方便后续的可视化，将测试数据集加载器的bathsize设置为1。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>PGD算法</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># PGD攻击方式,属于FGSM攻击的变体</span>
<span class="token keyword">def</span> <span class="token function">PGD_attack</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> epsilon<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> iters<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    image <span class="token operator">=</span> image<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    label <span class="token operator">=</span> label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    ori_image <span class="token operator">=</span> image<span class="token punctuation">.</span>data

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        image<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

        model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        cost <span class="token operator">=</span> loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> label<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        cost<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 对抗样本 = 原始图像 + 扰动</span>
        adv_image <span class="token operator">=</span> image <span class="token operator">+</span> alpha <span class="token operator">*</span> image<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>sign<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 限制扰动范围</span>
        eta <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>adv_image <span class="token operator">-</span> ori_image<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token operator">-</span>epsilon<span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>epsilon<span class="token punctuation">)</span>
        <span class="token comment"># 进行下一轮的对抗样本生成</span>
        image <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>ori_image <span class="token operator">+</span> eta<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>测试函数</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test_PGD</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> test_dataloader<span class="token punctuation">,</span> epsilon<span class="token punctuation">,</span> alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
    correct <span class="token operator">=</span> <span class="token number">0</span>
    adv_examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword">for</span> data<span class="token punctuation">,</span> target <span class="token keyword">in</span> test_dataloader<span class="token punctuation">:</span>
        data<span class="token punctuation">,</span> target <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        data<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>

        output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> init_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># print("origin label", init_pred)</span>

        <span class="token comment"># 分类错误就不去扰动图像</span>
        <span class="token keyword">if</span> init_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> target<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">continue</span>
        
        <span class="token comment"># 使用PGD进行攻击</span>
        perturbed_image <span class="token operator">=</span> PGD_attack<span class="token punctuation">(</span>model<span class="token punctuation">,</span> data<span class="token punctuation">,</span> target<span class="token punctuation">,</span> epsilon<span class="token operator">=</span>epsilon<span class="token punctuation">,</span> alpha<span class="token operator">=</span>alpha<span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span>perturbed_image<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> attack_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># print("attack label", attack_pred)</span>

        <span class="token comment"># 扰动后还是分类正确</span>
        <span class="token keyword">if</span> attack_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> target<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            correct <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># print("classifier failed!")</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>adv_examples<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">:</span>
                adv_ex <span class="token operator">=</span> perturbed_image<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
                adv_examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>init_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> attack_pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> adv_ex<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    attack_acc <span class="token operator">=</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"    Epsilon: &#123;&#125;\tTest Accuracy = &#123;&#125; / &#123;&#125; = &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilon<span class="token punctuation">,</span> correct<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> attack_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> adv_examples<span class="token punctuation">,</span> attack_acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>使用PGD算法生成对抗样本并计算准确率，在这里我使ε和α都发生变化</p>
<pre class="line-numbers language-none"><code class="language-none">accuracies &#x3D; []
examples &#x3D; []
epsilons &#x3D; [0.4, 0.5, 0.6]
alphas &#x3D; [2&#x2F;255, 2&#x2F;128, 2&#x2F;64, 2&#x2F;32]

for alpha in alphas:
    print(&quot;Alpha:&#123;:.5f&#125;&quot;.format(alpha))
    for epsilon in epsilons:
        ex, acc &#x3D; test_PGD(net, device, test_dataloader, epsilon&#x3D;epsilon, alpha&#x3D;alpha)
        accuracies.append(acc)
        examples.append(ex)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-none"><code class="language-none"># 测试结果
Alpha:0.00784
    Epsilon: 0.4	Test Accuracy &#x3D; 8453 &#x2F; 10000 &#x3D; 0.8453
    Epsilon: 0.5	Test Accuracy &#x3D; 4201 &#x2F; 10000 &#x3D; 0.4201
    Epsilon: 0.6	Test Accuracy &#x3D; 1075 &#x2F; 10000 &#x3D; 0.1075
Alpha:0.01562
    Epsilon: 0.4	Test Accuracy &#x3D; 8113 &#x2F; 10000 &#x3D; 0.8113
    Epsilon: 0.5	Test Accuracy &#x3D; 3093 &#x2F; 10000 &#x3D; 0.3093
    Epsilon: 0.6	Test Accuracy &#x3D; 488 &#x2F; 10000 &#x3D; 0.0488
Alpha:0.03125
    Epsilon: 0.4	Test Accuracy &#x3D; 8160 &#x2F; 10000 &#x3D; 0.816
    Epsilon: 0.5	Test Accuracy &#x3D; 2991 &#x2F; 10000 &#x3D; 0.2991
    Epsilon: 0.6	Test Accuracy &#x3D; 402 &#x2F; 10000 &#x3D; 0.0402
Alpha:0.06250
    Epsilon: 0.4	Test Accuracy &#x3D; 8265 &#x2F; 10000 &#x3D; 0.8265
    Epsilon: 0.5	Test Accuracy &#x3D; 3098 &#x2F; 10000 &#x3D; 0.3098
    Epsilon: 0.6	Test Accuracy &#x3D; 406 &#x2F; 10000 &#x3D; 0.0406<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可视化测试结果</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">alpha_1 <span class="token operator">=</span> accuracies<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
alpha_2 <span class="token operator">=</span> accuracies<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span>
alpha_3 <span class="token operator">=</span> accuracies<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">9</span><span class="token punctuation">]</span>
alpha_4 <span class="token operator">=</span> accuracies<span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">:</span><span class="token punctuation">]</span>


plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epsilons<span class="token punctuation">,</span> alpha_1<span class="token punctuation">,</span> <span class="token string">"b*-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Alpha=0.0078'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epsilons<span class="token punctuation">,</span> alpha_2<span class="token punctuation">,</span> <span class="token string">"r*-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Alpha=0.0126"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epsilons<span class="token punctuation">,</span> alpha_3<span class="token punctuation">,</span> <span class="token string">"g*-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Alpha=0.0312"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epsilons<span class="token punctuation">,</span> alpha_4<span class="token punctuation">,</span> <span class="token string">"*-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Alpha=0.0625"</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Accuracy vs Epsilon and Alpha"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epsilon"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Accuracy"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/a1b31fc5/3.png" class loading="lazy">

<p>可视化不同参数下的对抗样本</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">index <span class="token operator">=</span> <span class="token number">0</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        index <span class="token operator">+=</span> <span class="token number">1</span>
        plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>
        <span class="token keyword">if</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> index <span class="token operator">&lt;=</span> <span class="token number">15</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Eps:&#123;&#125; Alpha:&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilons<span class="token punctuation">[</span>i<span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0.0078</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> index <span class="token operator">></span> <span class="token number">15</span> <span class="token keyword">and</span> index <span class="token operator">&lt;=</span> <span class="token number">30</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Eps:&#123;&#125; Alpha:&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilons<span class="token punctuation">[</span>i<span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0.0126</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> index <span class="token operator">></span> <span class="token number">30</span> <span class="token keyword">and</span> index <span class="token operator">&lt;=</span> <span class="token number">45</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Eps:&#123;&#125; Alpha:&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilons<span class="token punctuation">[</span>i<span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0.0312</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Eps:&#123;&#125; Alpha:&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epsilons<span class="token punctuation">[</span>i<span class="token operator">%</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0.0625</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

        init_pred<span class="token punctuation">,</span> attack_pred<span class="token punctuation">,</span> example <span class="token operator">=</span> examples<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span>
        plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"&#123;&#125; --> &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>init_pred<span class="token punctuation">,</span> attack_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>example<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># plt.show()</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<img src="/posts/a1b31fc5/4.png" class loading="lazy">

<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p><a href="https://blog.csdn.net/qq_40176087/article/details/121512229">对抗训练fgm、fgsm和pgd原理和源码分析-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_43381493/article/details/130192453">研究笔记（一）_pgd攻击-CSDN博客</a></p>
<p><a href="https://www.cnblogs.com/tangweijqxx/p/10617752.html">3.基于梯度的攻击——PGD - 机器学习安全小白 - 博客园 (cnblogs.com)</a></p>
<p><a href="https://muyuuuu.github.io/2021/04/26/DNN-safe-basic/">对抗攻击篇：FGSM 与 PGD 攻击算法 | Just for Life. (muyuuuu.github.io)</a>(能力有限，核心代码参考该作者)</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>对抗样本生成与防御</tag>
        <tag>对抗攻击</tag>
      </tags>
  </entry>
  <entry>
    <title>暑期游</title>
    <url>/posts/2ddb491e/</url>
    <content><![CDATA[<p>暑期去了几个地方旅游，一直拖到现在才记录，不是忘了，就是我单纯的懒。哈哈哈哈哈哈哈哈哈！！！！</p>
<h3 id="泰山行"><a href="#泰山行" class="headerlink" title="泰山行"></a>泰山行</h3><p>其实很早之前就想去爬山了，但是大四一直没找到合适的时间，正好暑期就约了同学来一场说走就走的旅游。</p>
<p>在泰安我俩的行程都挺紧张的，朋友还要赶回去监考，所以住了两晚就走了，就去了泰山和岱庙。</p>
<img src="/posts/2ddb491e/1.jpg" class loading="lazy">

<p>第二天起了个大早就去爬泰山了。大概是五点多醒的，起来简单收拾了一下，就出发准备爬泰山了，早上没啥没胃口，没吃早餐但是带了几块糖就出发了。大概六点半左右开始，一路上走走停停的爬了五个小时就到最高地方了(不是南天门那里，好像是玉皇顶还是什么呢，五岳独尊那里)。登顶之后就坐缆车下山，然后乘公交回宾馆休息去了。两个人都有些累了。哈哈哈。</p>
<img src="/posts/2ddb491e/2.png" class loading="lazy">

<p>这里再放一个当时用的登山杖。hhh</p>


<p>在山顶见了很多已经白发的老人，两人拄着拐杖，互相搀扶的登山，也有小孩子在父母的鼓励下一步一步的的爬山。无论什么年龄段，大家都在一步一个脚印往上爬。</p>
<img src="/posts/2ddb491e/4.png" class loading="lazy">

<p>登顶泰山真的是有一览众山小的感觉。</p>
<h3 id="天津行"><a href="#天津行" class="headerlink" title="天津行"></a>天津行</h3><p>其实这次没计划去天津的，但是一直不知道去哪，想去淄博、济南但是又感觉人多，没什么玩的，最后我妈挺想去天津，就选择了去天津玩。八月初定好去天津的，但是因为海河游船抢不到票，一直到八月二十号左右才出发。</p>
<p>去了解放桥、世纪路、李公楼涂鸦隧道、小白楼、五大道、西开教堂、静园、张园、瓷房子、鼓楼、古文化街、意大利风情街、狮子林桥、金汤桥等地方，吃了一些某书里推荐的食物，具体名字我好像忘了，但是印象最深的两个就是津门张记包子还有一个在犄角旮旯里的煎饼，因为是我自己起了一大早去买的，中途还走错路了。坐了海河游船，风景非常美丽，体验非常好。但是人也很多，在那侯船的时候见了很多带着孩子、老人来坐船，但是买不到票的人。</p>
<p>在五大道也买了挺多冰箱贴的，因为已经在学校了，所以照片就不放了。</p>
<p>就放一些景的照片吧。</p>
<img src="/posts/2ddb491e/5.png" class loading="lazy">

<img src="/posts/2ddb491e/6.png" class loading="lazy">

<img src="/posts/2ddb491e/7.png" class loading="lazy">

<img src="/posts/2ddb491e/8.png" class loading="lazy">

<img src="/posts/2ddb491e/9.png" class loading="lazy">

<img src="/posts/2ddb491e/10.png" class loading="lazy">

<img src="/posts/2ddb491e/11.png" class loading="lazy">

<img src="/posts/2ddb491e/12.png" class loading="lazy">

<p>这里再放一张当时吃过的刨冰，味道确实很不错，但是也确实有些小贵。</p>
<img src="/posts/2ddb491e/13.png" class loading="lazy">

<h3 id="小岛行"><a href="#小岛行" class="headerlink" title="小岛行"></a>小岛行</h3><p>最后一站便是小岛了，我们来的时候正好没什么人，先在北戴河玩了两天，之后又到市区玩了一天。本来行程第一天早上有安排，但是被突如其来的组会给打乱了，所以省略了些内容，直接去了仙螺岛坐了跨海索道，感觉非常好，但是感觉景点的保护措施做的不是很好。坐在索道上看远处俨然海天一色的样子，还可以看到海鸥飞来飞去。之后去了碧螺塔体验了一下海滩蹦迪的感觉。</p>
<p>但是我感觉鸽子窝的海滩比较好玩，可以走到很远，然后可以捡到很大的贝壳。</p>
<img src="/posts/2ddb491e/14.png" class loading="lazy">

<img src="/posts/2ddb491e/15.png" class loading="lazy">

<p>真的很美。</p>
<p>后来去了东山浴场，这里似乎没大贝壳，但是这里的沙子似乎和其他浴场不太一样，感觉比较细，绵绵的，这里海里沙子比较多，很容易小石子就卡在脚丫子缝里了。</p>
<p>去了秦皇小巷尝了尝网上推荐的内容，喝了秦甜甜，这里放张图。</p>
<img src="/posts/2ddb491e/16.png" class loading="lazy">
]]></content>
      <categories>
        <category>记随笔</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>旅游</tag>
      </tags>
  </entry>
  <entry>
    <title>电影和电视剧</title>
    <url>/posts/cb44fd80/</url>
    <content><![CDATA[<p>在这记录一些我想看的电影和电视剧，有些是因为题材去看的，有些是因为演员去看的，还有些是因为某个片段去看的。</p>
<h4 id="综艺-看综艺很少"><a href="#综艺-看综艺很少" class="headerlink" title="综艺(看综艺很少)"></a>综艺(看综艺很少)</h4><ul>
<li>心动的信号6(完全是为了磕cp)√</li>
</ul>
<h4 id="电影"><a href="#电影" class="headerlink" title="电影"></a>电影</h4><ul>
<li><p>太平轮上下</p>
</li>
<li><p>默杀√</p>
</li>
<li><p>抓娃娃√</p>
</li>
<li><p>第二十条</p>
</li>
<li><p>周处除三害</p>
</li>
<li><p>东哥闯东北</p>
</li>
<li><p>来福大酒店√</p>
</li>
<li><p>西施新传√</p>
</li>
<li><p>宠爱</p>
</li>
<li><p>欲火之路</p>
</li>
<li><p>出入平安</p>
</li>
<li><p>焚城</p>
</li>
<li><p>守阙者</p>
</li>
<li><p>风平浪静</p>
</li>
<li><p>危机航线</p>
</li>
<li><p>乌海√</p>
</li>
<li><p>好东西</p>
</li>
<li><p>大风杀</p>
</li>
<li><p>走走停停</p>
</li>
<li><p>风林火山</p>
</li>
<li><p>风中有朵雨做的云</p>
</li>
<li><p>苦尽柑来遇见你</p>
</li>
<li><p>纸钞屋</p>
</li>
<li><p>天生一对</p>
</li>
<li></li>
</ul>
<h4 id="电视剧"><a href="#电视剧" class="headerlink" title="电视剧"></a>电视剧</h4><ul>
<li><p>黑暗荣耀 √</p>
</li>
<li><p>尘封十三载√</p>
</li>
<li><p>漫长的季节√</p>
</li>
<li><p>清明上河图密码√</p>
</li>
<li><p>玫瑰的故事√</p>
</li>
<li><p>九部的故事√</p>
</li>
<li><p>人民的名义√</p>
</li>
<li><p>雪迷宫√</p>
</li>
<li><p>隐瞒之事</p>
</li>
<li><p>冰雪谣√</p>
</li>
<li><p>亢奋√</p>
</li>
<li><p>宿敌√</p>
</li>
<li><p>好团圆√</p>
</li>
<li><p>太阳星辰√</p>
</li>
<li><p>念念勿忘</p>
</li>
<li><p>黑白诀√</p>
</li>
<li><p>隐身的名字</p>
</li>
<li><p>凡人歌√</p>
</li>
<li><p>阳光之下√</p>
</li>
<li><p>蔷薇风暴</p>
</li>
<li><p>掌心√</p>
</li>
<li><p>半熟男女√</p>
</li>
<li><p>漂白√</p>
</li>
<li><p>余烬之上√</p>
</li>
<li><p>黄雀</p>
</li>
<li><p>势在必行</p>
</li>
<li><p>亦舞之城</p>
</li>
<li><p>成家</p>
</li>
<li><p>陷入我们的热恋√</p>
</li>
<li><p>绝密较量√</p>
</li>
<li><p>隐娘</p>
</li>
<li><p>灿烂的转身</p>
</li>
<li><p>平等之门</p>
</li>
<li><p>扫毒风暴 √</p>
</li>
<li><p>低智商犯罪</p>
</li>
<li><p>沙尘暴</p>
</li>
<li><p>剥茧</p>
</li>
<li><p>隐瞒之事</p>
</li>
<li><p>七根心简</p>
</li>
<li><p>北上</p>
</li>
<li><p>风声</p>
</li>
<li><p>人民警察√</p>
</li>
<li><p>目之所及</p>
</li>
<li><p>归队</p>
</li>
<li><p>生万物</p>
</li>
<li><p>蝴蝶风暴</p>
</li>
<li><p>黑白局</p>
</li>
<li><p>不眠日</p>
</li>
<li><p>斯巴达克斯</p>
</li>
<li><p>方名三九</p>
</li>
</ul>
<h4 id="动漫"><a href="#动漫" class="headerlink" title="动漫"></a>动漫</h4><ul>
<li>云深不知梦</li>
</ul>
]]></content>
      <categories>
        <category>记随笔</category>
      </categories>
      <tags>
        <tag>追剧</tag>
      </tags>
  </entry>
  <entry>
    <title>我的2023——谨以此纪念我的考研</title>
    <url>/posts/ace8cace/</url>
    <content><![CDATA[<p>很久之前就想写这篇文章了，但是一直拖着没有完成（其实自己就是非常懒）。近期正好没什么事情，而且再不写感觉就要忘记了。</p>
<p>我本身就不是一个聪明的人，甚至有些迟钝，很多问题别人感觉轻而易举就可以拿下，但我总要反应一阵子。我一贯认为，天赋决定上限，而努力可以决定下限，所以也从来没有抱怨过自己的愚钝，只是告诉自己，努力的还不够。我是2024考研，但实际备考年份就是2023年，所以我的标题就叫成了我的2023。下面就记录一下这一整年吧。</p>
<p>我的专业是计算机科学与技术，在考研专业课方面自然而然就有统考（408）和自命题之分。其实，当初萌生考研这个想法时，自己就决定了要考11408。一来是认为11408的难度较大，竞争的人可能会少一些，二来自己并不排斥数学，所以感觉可以考数学一，三来，通过系统的复习408，可以把整个计算机的内部工作原理弄清楚，一定会有助于之后的学习（但这个想法似乎是错误的）。所以，自己就坚定不移的选择了11408。计算机专业的同学想必都知道，408专业课有四本书分别是数据结构、计算机组成原理、操作系统、计算机网络。东西有多又难，再加上一门数一，备考的难度可想而知（曾看到数一被称为最难的里面东西最多的，408被称为最多的里面最难的）。由于此，所以我决定2023年的寒假就开始着手复习。</p>
<p>我记得2022年12月初我们学校就放假了，但是由于xx（懂的都懂）原因，我在学校当地的宾馆住了几天才能回家，回家以后就中招了，大约到12月底才恢复。自己当时计划寒假复习，所以提前就预订了一个自习室，一直到1月8号，大家都一轮之后，我才正式去自习室学习。当时给自己订的计划是寒假开始复习数学，并且背英语单词。当时是刚开始复习，再加上临近过年，也是在家很放松，所以每天计划的时间根本做不到，经常早上十点才能到，然后十一点半左右就要回家，下午也常常一觉睡到下午三点。（但是我记得很有意思的一件事就是，当时流行电视剧《狂飙》，所以在自习室我就看到了一位初试结束的女生每天就在自己座位上追剧。）总之，寒假的复习状态感觉并不是很好，每天也就是睡醒会有负罪感，醒来就赶紧去自习室。但好在，寒假还是有些收获。</p>
<p>我们开始是2月16、17号左右，但我是18号去的学习。因为涉及到考研教室占座的问题，所以就委托了同学帮我占，但是大家对考研的热情太高了，我也不太好意思麻烦同学一间教室一间教室的看是否有我心仪的位置，所以18号下午到校，晚上就赶紧去教学楼每层楼去找我心仪的座位（靠窗且在后排），但是爬遍了八层楼都没找到合适的，最后只占到了一间前排但靠窗的座位。占教室期间，还碰到了我的好朋友，一开学已经开始卷了（他也成功上岸了心仪院校）。开学之后，就每天正常上课，下课之后就去考研教室复习。我们大三下基本就是每天一节课，偶尔有两节课，大部分课都是早八，所以早上下课以后大概去了教室，看会手机，十点开始学习，一直到十二点二十左右去吃饭（避开十二点下课的同学们），吃完饭回宿舍睡觉，大概下午两点半就去教室继续学习，一直到下午五点，我回去操场跑5km，大概就是跑二休一，这段时间，我就是强迫自己一定要5km跑进25min以内，所以跑的很累（这可能也为后续受伤埋下了伏笔）。这段时间我都是早上复习数学，下午专业课，晚上也是看数学，和背单词。一直到四月多（记不清了，大概是）开始学英语语法，晚上就只看一小会数学。每天就这样按部就班。</p>
<p>直到5月1号这个假期，我印象也是很深刻，我和几个朋友约好了5.1下午出去吃自助水饺，但是在4.30吃晚饭的时候，碰到了一个同学f（f就是约好去吃水饺的同学之一），然后我和f就在食堂吃了鸭腿饭，这一吃不要紧，5.1号早晨6点左右，我就被s意搞醒，紧接着就开始窜x，一直到八点半，我一直处在在床上和卫生间奔波的状态。整个人无精打采的，在群里一问，f同学也在cuan，于是我俩就很确定，昨晚吃的饭有问题。（可能是因为5.1放假，食堂吃的人很少，所以他们备餐不多，食材不太新鲜了）起来吃了些药，感觉好些，就去校园里走走，总不能一直在床上躺着，走着走着发现大家都是去教室学习的，我也就去了（当时备考的东西就直接全放到考研教室了）。学习的时候没啥感觉，但是等我中午回了宿舍又开始窜（我记得5.1我舍友大部分都去旅游、回家），所以宿舍算上我就两个人，不会出现抢不到厕所的情况。就这样，中午也没睡好，下午到了考研教室，直接爬桌子上就睡着了。因为我个人性格原因，也不愿意和朋友们说改天再去，所以下午就又和他们去自助水饺了，可能是吃过药的原因，下午和晚上就没有什么不适感了，只是偶尔会有些恶心。</p>
<p>再补充一个，因为当时是各自占各自的座位，你并不知道你的前后左右是什么人，后来有一天，我突然发现，我后面那位女生，可能和我认识（我记不清她长什么样子了，但是我应该是有她的微信和qq，当初参加比赛加上的（我是混子的那种比赛，经过别人引荐））之后就没有什么小插曲了，一直到暑期放假吧。</p>
<p>具体放暑假的日期记不住了，应该是六月低或者六月中旬吧（我们因为要实训，所以八月中旬就会开学），我也是申请了留校，当时有小道消息说留校学生为了集中管理，会让住在一起，意思就是你宿舍如果只有一个人留校，那你就要去宿舍只有一个人不留校的宿舍，凑满一个宿舍，方便阿姨管理，好在这是假的。但是假期宿舍关门时间提前了一个小时，也就是十点宿舍就关门了（九点半图书馆赶人）。我平时都是十点之后背单词，现在既然回宿舍我学不进去，不妨早点睡觉，所以我暑期晚上睡觉提早了一个小时，但是白天也会早起一个小时，大概是五点四十五左右，起来去跑步，跑步结束后背单词。暑期学校东区食堂一共三层楼，只有一层零零碎碎的几个窗口开着，甚至这几个窗口的饭都不是很好吃，我记得米饭窗口只有一个，一个粥，一个掉渣饼，剩下的记不清了，不过也差不多就这几个窗口吧。暑期，我直接把我的吃饭时间改到了十二点四十，这个时候去食堂基本没人，但是饭菜基本没有了，甚至还有些凉。暑期中午我在食堂吃饭，下午就到校外去吃碗面（无意间发现的一个面馆，真的好好吃，甚至下大雨我都要撑伞去吃）。本来是计划着暑期不回家，但是父母一直希望回家，正好当时身体有点不舒服就想着回家看一下病，不然之后严重了反而影响复习，就这样我就回家了大概十天左右。带了很多书回来，结果一点书没看（回家根本没有看书的欲望），回家正好算着自己生日的时候，妈妈给我买了个大蛋糕，我记得我还许了成功上岸的愿望。暑期回家吃吃喝喝玩了一个星期多，老感觉有负罪感，一打开书确怎么也看不进去。</p>
<p>呆了一周多在父母一直说再多住几天吧，我还是买了票。忘了补充一点，我们暑期教学楼封楼，考研教室就不能去了，所以暑期都是在图书馆学习。图书馆本来是不允许占座位的，但是暑期管的不是很严，同学们还是会把放到座位然后偷偷塞到桌底下，管理员也是睁一只眼闭一只眼。因为回家，我自然把占座的书都拿回去了，然后就导致我再回学校之前的座位被占掉了，然后因为图书馆的空调太冷我受不了了，很多地方我都不能坐，我同学告了我一个座位，但是那个座位旁边有个人一直喘气，抖腿（正常抖腿我可以接受，但是他裤子和凳子摩擦很乱），坐了一两天接受不了我就走了。后来我起了个大早，去了我原座位旁边（原座位被占了），原来那坐了对小情侣。第一天挺正常的，他俩面对面坐，但是第二天那个男生突然就坐我对面了，还和我面对面，中午吃完饭我就直接搬走了，（当天晚上我听到了我同学和我说昨天晚上他回宿舍听见这俩人在楼梯说我怎么怎么样，想必是不想让我坐他们旁边吧）就这样我搬到了图书馆五楼，五楼是那种独立的教室，我到了一个几乎没人的教室，这里空调温度开的也不低。就这样一直呆在这了，大概八月中下旬，我在看概率论的强化课，感觉自己进度和别人相比有点慢了，感觉十月份都开不了真题，莫名就哭了，就是边哭边学（但是，现在突然意识到和别人比真是一点用都没有，但是我感觉还是，能尽早就尽早吧）。暑期结束以后，我们也有实训，还是请的校外导师给我们授课，当时因为考研，对实训的热情也不大，所以通常都是老师在那里讲，我们在底下做自己的事，老师也通情达理，说你们可以做自己的，只有不说话，不扰乱课堂纪律即可。20天的实训，在团队大佬的带领下，一周后就完成了实训项目，并且验收（意味着我们可以不来了）。但是这个验收有个小插曲，当时我们用的yolo做的检测，验收老师会提问代码，可能另外两位组员着重看了项目的整体逻辑，没有看yolo的实现细节，但是验收时，老师反而问了yolo的实现细节，而我由于专门看了yolo的实现细节，所以最后的分数我反而比写代码的同学的分数高。对了，忘说了，快要开学时，就提前去占教室了，但是有些同学没在学校，所以这个座位相当于全部打乱了，我自然而然占了我原来的座位，但是我的前后的同学都变了，反正我自己感觉很难受，但是我这个教室大部分人都没变，变了就是没变那些人的同学们也都来了（不知道我这样说，大家是否看的明白）。后来因为某种缘由（外部因素+个人原因+同学劝说），我就下定决心搬出教室改去图书馆了。大四上学期基本就没什么课了，每天也就是八点开门去排队占座，一直到晚上九点半闭馆，秋天的图书馆不冷不热，我就一直在二楼呆着了。</p>
<p>在图书馆二楼这里我右边有个女生这里就称他为w女生吧（后面会提到），到了10月中旬，我同学和我说他们宿舍熄灯太迟了，每天都两点多才能睡，他要租房子，其实当时我想和他合租，想着和家里面商量一下，结果中午他和我说要租房，当天下午他告诉我他已经找好房子了，不过是单人那种，有一张书桌，一张床，但是没有卫生间（11月初有人退房他再搬到那边去）。之后的一周，每天中午吃饭都能碰到他，我也就问问他在外面住的怎么样，他也一直和我说感觉挺好的，弄的我心里也直痒痒，后来我问他要了房东微信，考虑了几天也决定搬出去住。当时我要租的话房子肯定也没有洗手间，我和同学商量了一下，我洗漱洗澡就去他屋子里，所以11.1号我就搬到了校外住。刚搬过去我非常满意，一间长条形的屋子，床靠着窗户，进门有衣架，有书桌，还有一个梳妆台，一个小茶几，但是住了几天我发现我这屋子窗户漏风，晚上吹的我脑袋难受，甚至，白天走了，晚上回来屋子里非常冷，我就和房东说我同学搬动有卫生间的屋子后我搬去我同学屋，但是他说有个人想租，他再给我联系一下，如果不租就可以给我换，谢天谢地，那个人没有租，所以当天晚上回去我就立马把东西搬过去了。（我原来在四楼住，现在搬到了二楼，同学搬到了三楼）搬到新屋子确实暖和了不少，晚上回来一点不冷，但是好景不长，大概到十一月中旬（北方）我们这里就非常冷了，而租的屋子竟然没有暖气（北方的同学肯定知道没暖气意为着什么）。这个时候图书馆也开始很冷了，早上刚去还号，大概呆一个半小时左右，脚就开始非常冷，中午吃完饭就好些，然后下午再呆一个多小时，又开始很冷，晚上吃完饭又好些。晚上回了屋子也开始很冷。大概就是这段时间，我晚上睡觉也不会脱衣服（穿着保暖裤，厚背心睡觉）。比较离谱的是，图书馆不让开空调，因为学校只买了制冷（有了解这些的可以评论一下这是真的吗？），但是图书馆冻的大家真的都很冷，所以总还是有人每天都去开空调，当然，管理员很快就会来关掉，其实也没什么用。就这样就每天在图书馆也冻着，回了出租屋也冻着，到了十二月初，我和同学说我真的每天在图书馆冷的不行了，他说然我去他那里试试，他那不太冷。我第二天就去了，果然，一点都不冷，可以说非常暖和，甚至有点热。因为这个位置位于图书馆东面和西面的链接的一个走廊，然后学校支了些桌子，这里早上可以晒到太阳，可以说早上十点左右到下午三点都是太阳直晒的，所以我就一直留在这里了，直到考研结束。但是这里也有弊端，就是这里的插座非常少， 而且离桌子非常远，即使你有插排充电都是很不方便。</p>
<p>十二月初这段时间，我们这里连着下了几天暴雪！！！非常大，漫天雪花，气温也是非常低，到了-15℃~-25℃，晚上睡觉我索性直接带一个针织帽，再把一间羊绒的那种毛毛衣服盖在脑袋上，再把自己的羽绒服，放在被子上面，晚上睡觉甚至要穿两双袜子。每天回了出租屋学习时，也非常冷，只能穿着加绒的靴子，羽绒服。忘说了，家里专门给我买了电热毯，本来要买一个小太阳，我想着又耗电（在这里一度电要一块钱），晚上睡觉开着的话也不安全，就买了个电热毯，晚上睡觉还好点。十二月初还有个小插曲，就是这里有空调，但是因为电费很贵，所以我没怎么用过，晚上回来也只是充充台灯，房东一月一查电表，发现我一个月都没用了一度电，一度怀疑电表坏了，中午把我叫回来，开了空调测试了电表，发现没有坏，就单纯是我用的比较少而已，说到这，我想起来一点，我这个屋子的门关不严实，他有很宽的一道缝，也能吹进来很大的风&#x2F;(ㄒoㄒ)&#x2F;~~。大概到十二中旬，怕考试那几天感冒，所以洗澡前会把屋子里的空调开了，等洗澡回来屋子里也是热的，但是空调总会自动关掉，所以最后一周大概也就只洗了一次澡（这是一篇有味道的文章，哈哈哈）。</p>
<p>又忘了一个点，前面提到了我每天都会跑步，大概到九月低，突然大腿根本外侧很疼，就休息了几天，但是之后继续跑，还是会疼，索性就想着多休息一段时间，但是直到十月中旬我们体测，跑完1000米之后我还是疼，我干脆就没跑了，直到考研结束吧。国庆的时候，我妈妈来学校看我了，和我聊了聊报考学校的事情，在市区逛了逛，给我买了几件衣服，就回去了。走后我的情绪有几天也不太高。</p>
<p>再回到十二月份，因为冷，还买了暖宝宝、发热鞋垫等。十二月份明显能感到学习的动力没那么强烈了，以前吃饭回来可以立即进入学习状态，现在不玩会手机就难受的慌，晚上睡前不玩会手机也很难受，根本控制不住自己，所以经常拖动十二点半才睡觉。我们九点半闭馆，十点左右回到出租屋，洗漱完十点二十，回了自己房间还要玩会手机，一直到十点四十甚至十点五十才能开始学习。</p>
<p>就这样机械式的浑浑噩噩过了十二月份20多天，就迎来了考研那两天，非常不幸的是，我在的那个考试教室是阴面，根本晒不到太阳，所以冷是可想而知的了。我和我同学干脆拿暖宝宝把鞋的一周都贴满了，相当于整个鞋面都是暖宝宝，连监考老师都问这是在干嘛，哈哈哈。考场应该是按地区，地区高校划分的吧，我们这间屋都是11408，坐在第一个的也是报考的北京大学，之后是北京理工……第一门是政治，感觉单选很简单，多选很难，有多一半我都不确定，下午是英语，我个人感觉英语阅读做的还是很顺的，卡壳的地方也不太多。第一天考完试还是蛮高兴的，所以这天晚上也不太能复习进去，看了看知识点、易错题、易错点、公式就睡了。没想到第二点的数学和408直接给我做傻了。到了第二天考数学的时候，我记得第一道选择题就和平时的题很不一样，直接就懵了，后来算了算还是能算出来的，就这样会做的做，不会做的就空下，选择填空就做了很久，还空了好几道，心态感觉受到了很大的影响。第一道大题就做错了，后面的大题应该是没有一道能完整答出来的，当时感觉考试时脑子里一片空白（其实十二月做数学模拟卷就感觉整个数学框架多乱掉了，可能是基础没打捞的缘故吧）。出了考场，也不太敢和同学多聊数学考试的内容，中午因为回出租屋睡觉很不方便，所以这两天中午也都是趴在图书馆睡了，这天中午也没睡着，一趴在那闭上眼，就开始算数学能打多少分，下午去考场的路上，同学说他也是这样。下午考的408也是一言难尽，试卷感觉和之前的完全不同，大题都是整页的字，介绍出题背景、题目描述。做选择题时，就能感觉出来，出的题很偏，个人感觉考了很多冷门知识点，甚至有些题型我都没有见过，有些知识点我都没听过，做到大题的时候感觉脑子也是一片空白，当时，真的很想一走了之，我感觉当时答题的状态就是完全不知道在写什么，完全不知道自己在算什么，就是机械式的机械记忆去作答。考场一直提醒自己，沉下心来，仔细读题，总算是坚持了下来，把自己会做的有点思路的不管正确与否都写了上去。答题铃声响了，出考场那一刻似乎也没有想象中的那么兴奋，只是很平静的和同学一起回了出租屋。回去点了外卖，玩了会手机，我们便开始对答案。其实当时我心里对数学和408一点底没有，感觉自己应该时完蛋了，应该是没学上了，后来想了想已经考完了，不妨就对答案试试。也就按照考试顺序对了对，我记得政治单选似乎就错了一道，但是多选没对几道，最后选择应该就是30分左右，英语做的比较好，阅读只错了两个，完型错了6个，另一个阅读题型我忘了应该是全对。数学对了对答案，就感觉数学应该不会很高，此时心里有点难受了已经，直到对408的答案，就是压垮我的最后一根稻草。因为我房间的桌子很小，只够一个人，所以对答案是在我同学屋里进行的，当天考完408，晚上王道就出了文档，里面有部分选择题的答案，我记得好像还直播讲解了，我记得当时文档里似乎只有20多不到30道选择题的答案吧，我对完感觉自己已经错了20多道，这么一算已经扣了40多分了，再加上大题做的也很差，我记得计组有道大题又臭又长，我根本是瞎写，网络的大题也不会，就直接躺同学床上了，躺在那越想越难受，我索性就告他我回去了。一回到我屋我忘了是我给我妈打电话了还是我妈给我打电话了，只记得一接电话就开始哭，哭了得有十来二十分钟吧，和妈妈聊了会，我也就开始看电影了，我记得那天晚上看了俄罗斯列车大劫案那个电影，然后就睡觉了。</p>
<p>第二天我们要把东西搬回宿舍了，我彷佛也接受了我估分240这个事实了（虽然我同学一点都不信，但是我就是估了这个分数）。花了一早上整理，然后往宿舍搬，一回宿舍就感觉真的好暖和，哈哈哈。这天下午去了ktv唱歌，唱了三个小时。在宿舍大概浑浑噩噩的两三天，我就又开始跑步了，但是我跑的很慢，而且要做足热身才开始，有一次跑步我舅舅给我打电话，问了问我考试情况，我说我考了240（之前我告诉了我妈我估分240，她肯定和家里说过了），然后我舅舅建议我可以报名公务员试试，当时的我其实非常抵触公务员考试，只是嘴上随便应付过去了（现在才明白宇宙的尽头就是编制这个道理）。但其实心里感觉自己应该是能过国家线的，因为我估分比较紧吧，高考就估分和真实差了70分。然后我也开始去图书馆看看复试的课了。复试有四门专业课，有一门我甚至还没学过，但是即便去了图书馆，也根本学不进去，每天呆在那玩玩手机，看几页书，就回宿舍打游戏了。因为冷，所以我还是想去我备考那个座位，但是我已经考完研了，我肯定不可能每天八点去占座位，可能起床就九点多了，于是每天都在找哪里有空座位。就这样大四上学期就结束了，我也就放寒假回家了。当时回家仍然想的，回家要看看复试的东西，结果呢可想而知，也没怎么看。</p>
<p>我没记错的话应该是2024.2.26号出的分。当时想的即便能过线，那肯定也不会高出线太高，如果提前联系老师估计老师们也肯定不会想要我，所以想的如果等复试结束再联系老师（现在看来，这个想法就是错的），其实2.25晚上就比较煎熬了，到了2.26早上就能看到大家陆续出分了，我们当时是下午三点可以查询，这一早上就等待的非常狡猾，中午吃饭的时候也不太想吃，一直在关注着群里的消息，突然有人就说学校网站有bug可以用去年的链接查出来，我也就立马跑回卧室，查分。当时真的心跳非常快，并且可以很明显的感觉到，查询界面反应很快，还容不得我挡屏幕，分数就出来了，我立马去看我的总分。我也立马大叫起来，不仅看到了总分，也看到了初试排名。比我的预估高了86分，排名也很靠前，我知道我可以提前联系导师了，赶紧又扒拉了两口饭，就开始问问学长哪些老师比较不错，总之一下午都在准备邮件模板，选择老师。我记得考研出分后一天早上是可以查四六级成绩，所以我决定查出来是否过了六级，并且修改简历后再发给老师，所以，26号一下午，一晚上都在为联系老师做准备。应该是弄到了27号凌晨一点多。第二天起了个大早，查了六级成绩，修改了简历，八点多美滋滋的给老师发了邮件，但是两天多老师都没有回消息，我使用的是学生邮箱，看不到老师是否已经读我的邮件，所以我不知道是老师没看到还是看到了不想要我，所以我又给老师发了一封邮件，又是两天，老师仍然没回我，这下子我认为应该是老师不想要我，所以我就给另一位老师发了邮件。早上九点左右发了邮件，然后老师十一点就给我回了信，并且加了微信和我进行了通话。通话中感觉老师很和蔼，非常随和，平易近人，老师问了问我的学习情况，同时也鼓励我努力准备复试，非常欢迎我加入他的课题组。</p>
<p>时间也到了2024年3月初，联系好老师以后，我就返校并且开始准备复试了。因为复试有四门专业课（一门笔试，三门面试），还有一门外语笔试，所以按照以往的复试时间，只有不到一个月，我还是去图书馆的二楼，之前最开始的地方，出乎意料的是w女生还在那复习（前后呼应上了），哈哈哈。然后一直到复试前一周，学习发了复试通知，我也订了酒店，到复试前一天，我就去了学校，然后探了路，就准备复试了。（写道这里不想写了，哈哈哈）</p>
<p>复试两天很快就过去了，就是面试当天很紧张，我属于这组中间的部分，面试结束以后，我就在学校里面逛了逛，去看了看学校的音乐厅和美术馆，看了看学校的小湖和亭子，看了学校的网红图书馆，看了学校的标志性建筑物，看了学校的天桥……然后过了好几天学校才出了复试查分通知，让自行申诉，又是好几天才出了复试名单，我记得出名单那天我一天都在等，然后八点多去吃了晚饭，回学校后去操场溜达的过程中看到学校发了录取名单，看到录取名单中我的名字，我揪着的心总算放了下来，自己曾幻想过，如果看到我被录取，自己会不会尖叫，会不会哭，实际上就是很平淡，看到结果，我立即给家人打了电话，给他们分享了这个好消息。自此，我的考研也算是划上了一个完美的句号，也就正式结束了。</p>
<p>我深知自己的写作水平有待提高，自己回头看一看自己写的内容，就和流水账一样，如果你看到了这里，非常感谢你愿意看我写的一些废话，谢谢。</p>
<p>写这篇文章就是纯粹的想记录一下我的整个考研历程，我没有写我到底是怎么复习，就是记录了这一年多来的部分时间节点所发生的事情。回看这一年，似乎没有什么大起大落，反而这一年多过的很平淡，每天起床就是去锻炼身体，然后就开始学习，吃饭，午睡，学习，吃饭，睡觉。每天都是这样。备考这一年，时间过得飞快，一眨眼间，就过去了。这一年，我谈不上很努力，甚至经常学习时走神，或者想放弃，我也想过去旅游，去打游戏，去追剧，去谈恋爱，可能这就需要自己的自制力，把各种事情的优先级安排好。很多人说考研难，他确实难，但是，我觉得难在你需要用你的意志去克服你的惰性，你需要日复一日坚持的去做同一件事情。当你发现英语阅读看不懂，数学不会做，政治背不会，各种各样的问题时，不能气馁，不能退缩，而是要迎难而上，这个知识点我没搞清，我就再去听课理解，刷题巩固，这个单词一遍背不会，我就背两遍、三遍直到记住他。考研真的可以磨砺我们的意志，但是总会有苦尽甘来的那一天。我身边备考的同学很多，三天打鱼两天晒网的有，日复一日坚持下来的也有，每天起早贪黑但没上岸的也有。我想考研的成功结合了各方面的因素，不仅仅要求你认真的复习，每一个时间节点的选择都是不可忽略的，比如择校，报考科目，报考专业等都需要与自己的实力相匹配，否则就只能是螳臂当车，拜拜浪费时间。有的人会说，我就是要上985，211，非它不可，非它不上。我承认名校的教育平台确实比普通院校高的不是一大截，拿我的本科学校和我同学上的一本来说差距就很大，我是切身实地的可以体会到，但是择校真正的需要结合自身实际，不可盲目。身边很多考研失败的很大原因都是择校出了问题。另外就是，复习过程也需要克制自己玩手机的欲望，图书馆中玩一整天手机的大有人在。</p>
<p>纵观自己的二十多年，没有值得夸赞的成功，也没有大的失败，最起码在一些时间节点上没有失败罢。网上最近很流行的一句话，我在这里借用一下，世界的边角料，父母的小骄傲，我直到自己的研究生只是一个双非，但是我也能看到妈妈是真的很开心，这一年很多时候想放弃的时候，我其实都告诉自己，一定要成为妈妈的骄傲，现在最起码在考研这件事上，我真的可以成为妈妈的骄傲。</p>
<p>我一直都相信，只要扎扎实实地复习好，做好择校等各方面地准备，一定是考得上的，愿每一个考研人都对得起自己一年来的努力，也希望大家的梦想都能实现，在各自的领域中闪闪发光。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024.07.26<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我的书桌前</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>回忆录</tag>
      </tags>
  </entry>
  <entry>
    <title>目标计数简洁框架</title>
    <url>/posts/9ec2453a/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h4><p>前阵子看了些目标计数的综述性文章，然后自己大概整理了个框架，可以供想要快速了解目标计数的阅读。</p>
<h4 id="目标计数"><a href="#目标计数" class="headerlink" title="目标计数"></a><strong>目标计数</strong></h4><p><strong>目标计数：</strong>目标计数就是指对给定的图像或者视频进行分析，从中估计指定目标的数量。通过目标数量的空间分布信息，就可以推测给定场景中关注度高的区域，从而对图像或者视频进行合理的分析和解读。</p>
<h4 id="目标计数模型分类-根据模型任务属性的角度分类"><a href="#目标计数模型分类-根据模型任务属性的角度分类" class="headerlink" title="目标计数模型分类(根据模型任务属性的角度分类)"></a><strong>目标计数模型分类(根据模型任务属性的角度分类)</strong></h4><h5 id="基于回归的计数模型"><a href="#基于回归的计数模型" class="headerlink" title="基于回归的计数模型"></a>基于回归的计数模型</h5><p>模型通过学习输入图像直接输出待计数目标的数量。这种方法只能提供场景内目标的数量信息，空间上目标数量的分布情况可以提供更加复杂的信息，同时，基于传统图像处理的目标计数方法所面临的问题就是复杂背景的干扰和，目标重叠。所以研究中更倾向于通过估计密度图来间接获得目标数量。</p>
<h5 id="基于密度图估计的计数模型"><a href="#基于密度图估计的计数模型" class="headerlink" title="基于密度图估计的计数模型"></a>基于密度图估计的计数模型</h5><p>种计数模型的输出是一张密度图，密度图中的每一个元素的值反应了当前位置上包含目标的数量。同时，密度图整体上也可以反映目标的空间分布，对密度图中任意区域内的元素进行积分，就可得到该区域中的目标数量。 密度图生成的前提就是需要对目标样本逐个进行标记。密度图真实值生成过程就是对于一张给定的包含目标的图像，先对每个目标都用一个靠近目标(人头、细胞等)中心的点进行标注，然后采用归一化高斯函数对其进行卷积。重叠区域输出的密度图是每个目标经卷积后的叠加结果。</p>
<h5 id="多任务模型-模型有多种输出"><a href="#多任务模型-模型有多种输出" class="headerlink" title="多任务模型(模型有多种输出)"></a>多任务模型(模型有多种输出)</h5><p>以密度图估计任务为主，其他任务(目标数量估计、密度等级划分、图像分割等任务)为辅来帮助提高计数任务的准确性。</p>
<h4 id="常用目标计数数据集"><a href="#常用目标计数数据集" class="headerlink" title="常用目标计数数据集"></a><strong>常用目标计数数据集</strong></h4><p>目标计数的应用范围十分广泛，如人群计数、车辆计数、细胞计数、野生动物计数等。</p>
<h5 id="人群计数常用数据集"><a href="#人群计数常用数据集" class="headerlink" title="人群计数常用数据集"></a>人群计数常用数据集</h5><ul>
<li><p>UCSD：该数据集是第一个用于人群计数的数据集，记录了固定场景下的人群流动情况，包含2000张分辨率为158×238的图片，共包含了49885个行人。</p>
</li>
<li><p>Mall：该数据集来源于一个商业公共区域的监控视频，包含2000张分辨率大小为240×320的图片，共有62325个行人。其记录场景相对复杂，存在物体遮挡问题。</p>
</li>
<li><p>UCF_CC_50：该数据集覆盖了音乐会、体育场等不同的场景，但是只包含50张图片，平均每张图片包含1280个人。</p>
</li>
<li><p>WorldExpo’10：该数据集包含108个摄像头的1132个视频序列，分辨率大小为576×720，覆盖了大量不同场景，共计199923人。</p>
</li>
<li><p>Shanghai Tech：该数据集由PartA和PartB两部分组成，A部分来自网络，包含482张大小不一致的图片，一共包含241677人。B部分拍摄了来自上海主城区的繁华街头场景，包含716张分辨率大小为768×1024张图片，共计88488人。</p>
</li>
</ul>
<h5 id="细胞计数常用数据集"><a href="#细胞计数常用数据集" class="headerlink" title="细胞计数常用数据集"></a>细胞计数常用数据集</h5><ul>
<li><p>VGG Cells：该数据集是一个荧光细胞数据集，模拟了真实图片中经常会出现的一系列问题，包含200张分辨率大小为256×256的图片。</p>
</li>
<li><p>MBM Cells：该数据集记录了8个不同病人的健康骨髓细胞，共有11张分辨率大小为1200×1200的图片。对错误订成后裁剪为44张分辨率大小为600×600的图片。</p>
</li>
<li><p>Adipocyte Cells：人体皮下脂肪细胞数据集，每张图片分辨率为150×150。</p>
</li>
</ul>
<h5 id="车辆计数常用数据集"><a href="#车辆计数常用数据集" class="headerlink" title="车辆计数常用数据集"></a>车辆计数常用数据集</h5><ul>
<li><p>WebCamT：该数据集由多台不同的交通摄像机记录了不同天气条件、不同时刻的路况。训练集包含了45480张图片，测试集包含了14150张图片，场景分为市区道路和公园道路。</p>
</li>
<li><p>TRANCOS：该数据集记录了多种不同的道路交通场景，共标记了46796辆车辆，共1244张图片。</p>
</li>
</ul>
<h4 id="人群计数方法"><a href="#人群计数方法" class="headerlink" title="人群计数方法"></a><strong>人群计数方法</strong></h4><h5 id="传统人群计数方法"><a href="#传统人群计数方法" class="headerlink" title="传统人群计数方法"></a>传统人群计数方法</h5><ul>
<li><p>检测法：检测图像的个体并统计人数，当人群密集时就无法得到准确结果。</p>
</li>
<li><p>回归法：建立手工提取特征到图像人数之间的映射来完成计数。</p>
</li>
<li><p>密度估计方法：学习提取特征与密度图之间的映射关系。</p>
</li>
</ul>
<h5 id="基于深度学习的人群计数方法"><a href="#基于深度学习的人群计数方法" class="headerlink" title="基于深度学习的人群计数方法"></a>基于深度学习的人群计数方法</h5><ul>
<li><p>多列CNN模型：多列网络是指用不同的列对应于不同感受野的多尺度信息。但是这种网络模型参数较多，训练困难，计数的实时性较差，网络结构冗余度较高。</p>
</li>
<li><p>单列CNN模型：单列网络仅存在单个深度网络，不会增加网络的复杂性，结构简单，模型训练容易。但是这种网络不能有效分析任意场景的透视信息，对尺度变化的处理效率低。</p>
</li>
</ul>
<h4 id="人群计数损失函数"><a href="#人群计数损失函数" class="headerlink" title="人群计数损失函数"></a><strong>人群计数损失函数</strong></h4><p>评价模型的预测值与真实值ground-truth的一致程度，通过定义损失函数，可以将人群密度图的映射关系学习转化为一个最优化问题。</p>
<ul>
<li><p>欧式距离损失：采用像素级的欧式距离，度量估计密度图与真实密度图之间的差距。但是这种方法的鲁棒性差，很容易因为个别像素点的极端情况影响整体的计数效果，并且由于取了平均值，所以其并不关注结构化信息，生成的密度图模糊，细节不清晰。</p>
</li>
<li><p>结构相似性损失(SSIM)：这种方法从图像的亮度、对比度和结构这三方面度量图像相似性，并通过均值、方差、协方差3个局部统计量计算图像之间的相似性，其取值范围在-1和1之间，值越大表示图像之间越相似。这种方法更好地关注图像间对应局部块的差异，更好的生成密度图。</p>
</li>
<li><p>生成对抗损失：使用对抗损失函数，通过对抗的方式对生成图片进行矫正，避免密度图模糊的情况。</p>
</li>
</ul>
<h4 id="人群计数ground-truth密度图生成方法"><a href="#人群计数ground-truth密度图生成方法" class="headerlink" title="人群计数ground-truth密度图生成方法"></a><strong>人群计数ground-truth密度图生成方法</strong></h4><p>为图片中的每个人头标注中心坐标，然后再利用高斯核将坐标图转化为ground-truth人群密度图。ground-truth密度图生成的关键在于如何选择高斯核。</p>
<ul>
<li>几何自适应法：由于存在透视效应，人群图片中远近景目标的尺寸差异较大，不同位置人头对应着不同大小的像素区域，所以考虑到透视畸变的影响，大人头采用大尺寸高斯核，小人头采用小尺寸高斯核。</li>
<li>固定高斯核法：这种方法忽略了人头尺寸差异以及自身与邻居的相似性，无论图片中哪个位置的人头均采用方差大小固定的高斯核对每个人头进行高斯模糊。但是对于远处的人头高斯核尺寸较大，可能会出现重叠，导致密度图质量的降低。</li>
<li>内容感知标注法：首先使用暴力最近邻算法定位最近的头部，再用无监督分割算法分割出头部区域，然后依据邻居头部的大小计算高斯核尺寸。</li>
</ul>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>目标计数</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>LearningToCountEverying论文阅读与源码理解</title>
    <url>/posts/74d83a5b/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近读了小样本计数的开山之作Learning To Count Everying，并对代码进行了一下复现，在这里做一个简短的记录。</p>
<p>原文链接：[<a href="https://arxiv.org/abs/2104.08391">2104.08391] Learning To Count Everything (arxiv.org)</a></p>
<p>代码链接：<a href="https://github.com/cvlab-stonybrook/LearningToCountEverything">cvlab-stonybrook&#x2F;LearningToCountEverything (github.com)</a></p>
<p>我对本文代码进行调试，并且增加了注释也就是我的理解，放到了github，可以直接下载查看：[PaperRecurrent&#x2F;FamNet–Learning To Count Everything at main · LengNian&#x2F;PaperRecurrent (github.com)](<a href="https://github.com/LengNian/PaperRecurrent/tree/main/FamNet--Learning">https://github.com/LengNian/PaperRecurrent/tree/main/FamNet--Learning</a> To Count Everything)但是这里需要自己下载数据集和密度图保存才可以运行</p>
<h3 id="环境调试"><a href="#环境调试" class="headerlink" title="环境调试"></a>环境调试</h3><p>在我印象中，根据readme进行就可以了，只需要修改一下train.py中的</p>
<pre class="line-numbers language-none"><code class="language-none">parser.add_argument(&quot;-dp&quot;, &quot;--data_path&quot;, type&#x3D;str, default&#x3D;&#39;&#x2F;home&#x2F;hoai&#x2F;DataSets&#x2F;AgnosticCounting&#x2F;FSC147_384_V2&#x2F;&#39;, help&#x3D;&quot;Path to the FSC147 dataset&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>将其改为</p>
<pre class="line-numbers language-none"><code class="language-none">parser.add_argument(&quot;-dp&quot;, &quot;--data_path&quot;, type&#x3D;str, default&#x3D;&#39;.&#x2F;data&#x2F;&#39;, help&#x3D;&quot;Path to the FSC147 dataset&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>就可以了，同时要把自己下载的FSC-147数据集放到data文件夹下，也就是</p>
<p>—data</p>
<p>​	—image_384_VarV2</p>
<p>​		—x.jpg(图片)</p>
<p>​	—其余自带的文件</p>
<p>数据集可以在readme中的链接下载</p>
<p>当我们想要测试val和test的结果时可以使用</p>
<pre class="line-numbers language-none"><code class="language-none">python test.py --test_split val
python test.py --test_split test<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>在使用demo自己画框进行测试时，也就是使用</p>
<pre class="line-numbers language-none"><code class="language-none">python demo.py --input-image orange.jpg<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>进行测试时，按n可以进行画框，按空格或者回车进行保存，按q或者ESC代表画框结束，进行预测。</p>
<p>可能按q结束时会报错</p>
<pre class="line-numbers language-none"><code class="language-none">cv2.error: OpenCV(4.10.0) D:\a\opencv-python\opencv-python\opencv\modules\highgui\src\window_w32.cpp:1261: error: (-27:Null pointer) NULL window: &#39;Image&#39; in function &#39;cvDestroyWindow&#39;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这时候把demo.py中的</p>
<pre class="line-numbers language-none"><code class="language-none">cv2.destroyWindow(&quot;Image&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>修改为</p>
<pre class="line-numbers language-none"><code class="language-none">cv2.destroyWindow(&quot;image&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>因为这篇论文是近年的，所以没有很多版本不兼容的问题。</p>
<h3 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a>读论文</h3><p>现有的计数都是针对某一个确定类别的方法进行，比如人群计数，车辆计数等，但是本篇论文提出的方法与类别无关，而是只要给出几个特定类别的示例图像，就可以在图像中对该示例类别进行计数。同时该论文也提出一个小样本计数数据集FSC-147。</p>
<p>本文提出的FamNet网络，包括两个部分，分别是特征提取模块和密度预测模块。</p>
<p><strong>本文是通过给定很少的几个样本，从而得到图像中给定样本的数目，比如一副图像中有40个苹果，通过给定3个苹果的边界框，从而可以对图像中的40个苹果计数。这里特别说明给定的3个苹果称为示例图像或者示例类别，苹果则是感兴趣的类别。</strong></p>
<p><strong>为了更好的介绍之后的工作，这里先对FSC-147数据集进行介绍，该数据分为train、val和test。每张图片作者都有三个示例类别即对其标注了边界框以及对每张图片中感兴趣的类别进行了点注释。</strong></p>
<p>特征提取模块：结构就是ResNet的前四个块。使用ResNet的前四个块提取特征，然后保存第三和第四个块得到的特征图。</p>
<p>得到特征图以后会对示例特征与图像特征之间进行操作(我的理解就是进行了卷积操作)，得到相关图。为了解释不同比例下的对象，作者这里使用了三种不同的缩放尺度对示例特征进行缩放，即0.9，1，1.1。也就是会得到这三个不同比例下的示例特征和图像特征的相关图。看到这里可能会不懂到底是怎么操作，别急，只要知道得到了相关图就可以了，请继续往下看。</p>
<p>密度预测模块：密度预测模块的输入就是上面提到的相关图，会输出密度图。</p>
<p>下面我介绍一下得到示例特征和图像特征相关图的流程：</p>
<p><strong>上面提到会保存ResNet中第三和第四个块的特征图。以第三个特征图为例，此时第三个特征图就是图像特征，因为给出了示例图像的边界框，所以根据边界框我们就可以得到给出的示例图像所对应的示例特征，也就是在图像特征图上根据边界框截取对应部分(截取过程中还涉及调整大小以及插值等问题，这里就不赘述，主要讲整个流程)。得到三个边界框对应的示例特征以后，会堆叠起来，此时把堆叠起来的示例特征图的形状就是[3, channel, height, width]，前面那个3就代表三个边界框所堆叠起来。之后怎么得到相关图呢，就是对图像特征进行卷积操作，卷积核就是我们得到的示例特征。然后上面也提到会有不同的缩放尺度，所以这里对特征图进行缩放，缩放比列分别是0.9和1.1。然后再使用缩放后的示例特征对图像特征进行卷积操作，就得到了不同缩放尺度下的相关图。将三个不同尺度下的相关图再次堆叠。此时的相关图形状为[3, 3, height, width]。这就是得到部分相关图的流程，然后重复上述流程对ResNet第四个块中的特征图进行相同的操作，也会得到一个相同形状的相关图，再将这两个相关图堆叠，得到的最终相关图形状就为[3, 6, height, width] (这里还涉及到了交换维度顺序等内容，所以是[3,6,h,w]而不是[6,3,h,w]，详细可见源码)</strong></p>
<p>这是我认为文中蛮重要一个点，接下来我将解释另一个点Test-time adaptation。</p>
<p>这里定义几个变量B是边界框的集合，b是单个边界框，Z为密度图，$ Z_b $就是边界框所对应的那块区域的密度图。</p>
<p>作者提出了两个损失：最小计数损失和扰动损失，分别介绍。</p>
<p>最小计数损失(Min-Count Loss)：</p>
<div>
$$
\cal {L}_{MinCount} = \sum_{b\in \rm B} max(0, 1-\rVert \rm Z_b \rVert_1) \tag{1}
$$
</div>
作者对数据集进行标注，每个边界框内至少会有一个物体，但是该物体可能与附近物体重叠，所以$ Z_b $的总和总是应该大于等于1。对于上面的式子，如果$Z_b$总和小于1，就会得到一个正的损失值，表示模型预测的密度没有达到最小计数的要求，如果$Z_b$总和大于等于1，损失值就为0，符号最小计数的要求。

<p>扰动损失(Perturbation Loss):</p>
<div>
$$
\cal{L}_{\rm{Per}} = \sum_{b\in \rm{B}} \rVert \rm Z_b - \rm{G}_{h×w} \rVert_2^2 \tag{2}
$$
</div>
在边界框的确切位置应该有较大的相应，而在受扰动位置的响应应较低。这里的$G_{h×w}$应该是理想化的高斯分布。

<p>示例图像周围密度值在理想情况下应该类似于高斯分布，所以通过计算预测密度图与理想情况下的高斯分布的差异。</p>
<p>The combined adaption Loss，将这两种损失进行结合：</p>
<div>
$$
\cal{L}_{\it Adapt} = \lambda_1 \cal{L}_{\it MinCount} + \lambda_2 \cal {L}_{\it Per} \tag{3}
$$
</div>
这就是测试时使用的最终函数，结合了上述两个损失得到的最终损失。注意，该损失函数只在测试阶段使用。

<p>在测试进行时，使用这种方法能够使网络更适应与当前的目标类别，在测试阶段会进行100次迭代，传递梯度，优化参数，从而使网络能够更适应当前预测的类别。</p>
<h3 id="源码理解"><a href="#源码理解" class="headerlink" title="源码理解"></a>源码理解</h3><p>代码主要关注utils.py中的内容。</p>
<p>如果想仔细看源码的理解，可以去参考我的注释[PaperRecurrent&#x2F;FamNet–Learning To Count Everything at main · LengNian&#x2F;PaperRecurrent (github.com)](<a href="https://github.com/LengNian/PaperRecurrent/tree/main/FamNet--Learning">https://github.com/LengNian/PaperRecurrent/tree/main/FamNet--Learning</a> To Count Everything)。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="https://www.cnblogs.com/dxmstudy/p/17241194.html">CVPR论文解读《Learning To Count Everything》 - 同淋雪 - 博客园 (cnblogs.com)</a></p>
<p><a href="https://blog.csdn.net/qq_35599200/article/details/121373114">2021论文解读：Learning To Count Everything_微小物体计数识别论文-CSDN博客</a></p>
]]></content>
      <categories>
        <category>小样本计数</category>
        <category>论文复现</category>
        <category>目标计数</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Github克隆本地仓库再次上传文件</title>
    <url>/posts/effa8074/</url>
    <content><![CDATA[<p>写这个主要是方便自己复现文章以后上传，不需要每次再去找，可以直接在自己的博客中看到。</p>
<p>也供和我有相同处境的使用。</p>
<p>1.在本地建立一个空白文件夹，用于克隆仓库</p>
<p>2.在git中克隆仓库</p>
<pre class="line-numbers language-Git" data-language="Git"><code class="language-Git">git clone + 仓库地址<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>3.把要上传的文件夹放到克隆的仓库内</p>
<p>4.依次使用以下命令</p>
<pre class="line-numbers language-git" data-language="git"><code class="language-git">cd 仓库名
git init
git add .
git commit -m <span class="token string">"upload messages"</span>
git push<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>再次刷新就可以了。</p>
]]></content>
      <categories>
        <category>解问题</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer学习记录</title>
    <url>/posts/3b51a2ff/</url>
    <content><![CDATA[<p>本文用于记录自己在学习Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。</p>
<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><h4 id="Personal-understand"><a href="#Personal-understand" class="headerlink" title="Personal understand"></a>Personal understand</h4><p>self-attention可以和fc交替使用，输入几个vector，就会输出几个vector。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131028202.png" alt="image-20241206131028202" loading="lazy"></p>
<p>默认计算注意力的方法使用Dot-product</p>
<p>假设在a之前还有其它运算，所以命名为 $ a^1, a^2,…$。</p>
<ul>
<li><p>如图，query，key，value是如何产生的？</p>
<p>有一个$ W^q $矩阵,通过和输入$a^i$相乘,就能得到$q^i$,也就是query。</p>
<p>同理。也存在一个$W^k, W^v$矩阵，和输入$a^i$相乘，就得到 $k^i, k^v$,也就是key, value。</p>
<p>$query和key进行一个dot product就得到了注意力分数也就是图中所示的\alpha$</p>
<p>以$a^1$为例，$q^1$ dot product  $k^1以后得到了\alpha_{1,1}, q^1和k^2得到了\alpha_{1,2}…$</p>
<p>**我的理解就是$\alpha_{1,j}$就是特征1和特征1、特诊2…特征j之间的注意力分数，为了更好运算，将这些进行一个softmax，统一到(0, 1)之间，得到了 **${\alpha _{1, j}}^{‘}$。</p>
</li>
</ul>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206131316164.png" alt="image-20241206131316164" loading="lazy"></p>
<p>之后将$\alpha^{‘}_{1,j}和对应的v^j相乘，就得到了b^1$。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206132747390.png" alt="image-20241206132747390" loading="lazy"></p>
<p> 上面的两张图是对输入$a^1$进行计算的一个过程。其他的输入也一样。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133116827.png" alt="image-20241206133116827" loading="lazy"></p>
<h4 id="Matrix-Multiply-show"><a href="#Matrix-Multiply-show" class="headerlink" title="Matrix Multiply show"></a>Matrix Multiply show</h4><p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133314813.png" alt="image-20241206133314813" loading="lazy"></p>
<p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133534659.png" alt="image-20241206133534659" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133708410.png" alt="image-20241206133708410" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206133823384.png" alt="image-20241206133823384" loading="lazy"></p>
<p><strong>在整个self-attention过程中，只有</strong>   $W^k,W^v,W^q$ <strong>是需要通过训练学习的</strong>。</p>
<h3 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h3><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151609909.png" alt="image-20241206151609909" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206151742513.png" alt="image-20241206151742513" loading="lazy"></p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>位置编码通过一种特殊的方式产生，不一定要通过sin&#x2F;cos产生，也可以通过网络学习产生。</p>
<h3 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h3><p>self-attention就是复杂的CNN</p>
<p>普通的RNN只考虑了左边最近的一个词，而且无法并行处理，而self-attention可以并行处理，并且考虑了所有词之间的关系。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155029432.png" alt="image-20241206155029432" loading="lazy"><br>Encoder可以简化成这种结构 ，里面包含</p>
<ul>
<li><p>residual connection </p>
</li>
<li><p>layer norm</p>
</li>
</ul>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155823308.png" alt="image-20241206155823308" loading="lazy"></p>
<p>每个block的结构又如下图所示<br><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155316616.png" alt="image-20241206155316616" loading="lazy"></p>
<p> <img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206155536850.png" alt="image-20241206155536850" loading="lazy"></p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><h5 id="Input-Output"><a href="#Input-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h5><p> voc_size:就是可能输出的单词总数，比如26个字母，或者2000个汉字</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161249953.png" alt="image-20241206161249953" loading="lazy"></p>
<p>这里的distribution经过一个softmax，加起来的总和是1，选择一个概率最大的，作为decoder的下一个输入，Decoder看到的输入其实就是自己上一次的输出，所以是可能看到错误的。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161453825.png" alt="image-20241206161453825" loading="lazy"></p>
<h5 id="Masked-multi-head-attention"><a href="#Masked-multi-head-attention" class="headerlink" title="Masked multi-head-attention"></a>Masked multi-head-attention</h5><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161732336.png" alt="image-20241206161732336" loading="lazy"></p>
<p>可以注意到Decoder中有一个Masked Multi-Head Attention，这里的masked的意思就是看不到在输入之后的内容</p>
<p>比如处理 $a^1$时，只能和$a^1自己计算得到b^1$,处理$a^2时，只能和a^1,a^2进行计算得到b^2,在处理a^4时，则是和a^1,a^2,a^3,a^4进行计算得到b^4$。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206161950727.png" alt="image-20241206161950727" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206162515398.png" alt="image-20241206162515398" loading="lazy"></p>
<p><strong>为什么要用masked？</strong></p>
<p>本文刚开始提到的self-attention的输入是一次性给进去的，所以计算注意力时可以使用后面的，而transformer中的输出其实是一步一步产生，也就是处理$a1$时，并不知道$a^2$的值，处理$a^2时，只能用a^2,a^1,而并不知道a^3,a^4$。所以这里就要用masked。</p>
<p>这里要有特殊符号，BEGIN和END，用于开始开始和结束。 </p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206163352894.png" alt="image-20241206163352894" loading="lazy"></p>
<h5 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross attention"></a>Cross attention</h5><p>Decoder提供一个query，Encoder提供key和value，使用q和k计算注意力分数以后，再和c相乘，就是交叉注意力机制的过程。</p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206164332001.png" alt="image-20241206164332001" loading="lazy"></p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165024419.png" alt="image-20241206165024419" loading="lazy"></p>
<p><img src="/Transformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241206165109208.png" alt="image-20241206165109208" loading="lazy"></p>
<p>Decoder每一个输出都有一个cross entropy，希望cross entropy的总和最少，同时模型还要能够输出结束标记END。</p>
<h3 id="参考文章-视频"><a href="#参考文章-视频" class="headerlink" title="参考文章(视频)"></a>参考文章(视频)</h3><p><a href="https://www.bilibili.com/video/BV1o2421A7Dr/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">【研1基本功 （真的很简单）注意力机制】手写多头注意力机制_哔哩哔哩_bilibili</a></p>
<p><a href="https://www.bilibili.com/video/BV1cV411X7XN/?spm_id_from=333.337.search-card.all.click&vd_source=dff3ad76ed17ff6ff5fee17de98f73c5">Transformer终于有拿得出手得教程了！ 台大李宏毅自注意力机制和Transformer详解！通俗易懂，草履虫都学的会！_哔哩哔哩_bilibili</a></p>
<p><a href="https://www.bilibili.com/video/BV13z421U7cs/?spm_id_from=333.337.search-card.all.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">【官方双语】GPT是什么？直观解释Transformer | 深度学习第5章_哔哩哔哩_bilibili</a></p>
<p><a href="https://www.bilibili.com/video/BV1XH4y1T76e/?spm_id_from=333.337.search-card.all.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)_哔哩哔哩_bilibili</a></p>
<p> <a href="https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>写笔记</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2024研究生科研素养提升公益讲座笔记</title>
    <url>/posts/877ae5d4/</url>
    <content><![CDATA[<h3 id="科研信息检索与利用-–李修波"><a href="#科研信息检索与利用-–李修波" class="headerlink" title="科研信息检索与利用  –李修波"></a>科研信息检索与利用  –李修波</h3><p>科研信息检索与利用可以学知识，思路方法启发，跨学科融合创新。</p>
<p><strong>科研信息的搜集：</strong> </p>
<p>中文资源：知网、万方、维普      建议修课：信息检索</p>
<p>外文资源：Web of Science、Compendex、INSPEC、PubMed</p>


<img src="/posts/877ae5d4/2.png" class loading="lazy">

<img src="/posts/877ae5d4/3.png" class loading="lazy">

<img src="/posts/877ae5d4/4.png" class loading="lazy">

<p>如果将危害作为检索词，会降低查全率。</p>
<p>引文分析要弄清楚影响力评测范围，引文分析工具，检索用词，</p>
<p>如何扩充关键词？</p>
<img src="/posts/877ae5d4/5.png" class loading="lazy">

<img src="/posts/877ae5d4/6.png" class loading="lazy">



<h3 id="科技论文选题-–李垚"><a href="#科技论文选题-–李垚" class="headerlink" title="科技论文选题  –李垚"></a>科技论文选题  –李垚</h3><p>高阶专业知识(前沿科技动态，多学科交叉只是)，科学思维能力(发现<strong>问题意识</strong>与能力，辨证批判思维)，成果展示能力(选题，文字，表述，学术论文与学位论文)</p>
<img src="/posts/877ae5d4/7.png" class loading="lazy">

<p><strong>找准科学问题和凝练选题直接关系到研究的深度、广度、创新性和最终成果的价值</strong></p>
<p><strong>问题意识</strong> ：发现知识缺口，人们在认识活动中，实践中遇到的一些疑难困惑的现实或理论问题及深入思考、积极探究的心理活动。</p>
<ul>
<li>有什么新问题需要被回答？</li>
<li>已有的回答，是否可以被继续深化？</li>
<li>已有的回答，是否可以被覆盖？</li>
</ul>
<img src="/posts/877ae5d4/8.png" class loading="lazy">

<p>找准科学问题途径：</p>
<ul>
<li><p>保持好奇心与敏锐洞察力</p>
</li>
<li><p>在经典文献细读中发现科学问题</p>
<ul>
<li><p>中文文献以摘要为主，储备知识</p>
</li>
<li><p>英文文献细致掌握最新研究动态，寻找研究空白、</p>
</li>
<li><p>学术报告掌握最新问题</p>
</li>
</ul>
</li>
<li><p>批判性思维深挖问题</p>
</li>
</ul>
<img src="/posts/877ae5d4/9.png" class loading="lazy">
<ul>
<li><p>与导师和同行讨论中发现科学问题</p>
</li>
<li><p>跨学科交流合作中发现科学问题</p>
</li>
<li><p>将实际应用需求转化为科学问题的思考</p>
</li>
</ul>


<p>选题原则：</p>
<ul>
<li><p>兴趣与热情</p>
</li>
<li><p>重要性和意义</p>
</li>
<li><p>知识空白和研究需求</p>
</li>
<li><p>可行性和可操作性</p>
</li>
<li><p>可复制性和可验证性</p>
</li>
<li><p>独创性和创新性</p>
</li>
<li><p>可发表性和学术影响</p>
</li>
</ul>
 

 

 

<h3 id="文献阅读与论文写作-–杨军"><a href="#文献阅读与论文写作-–杨军" class="headerlink" title="文献阅读与论文写作  –杨军"></a>文献阅读与论文写作  –杨军</h3><img src="/posts/877ae5d4/15.png" class loading="lazy"> 

<img src="/posts/877ae5d4/16.png" class loading="lazy"> 

<img src="/posts/877ae5d4/17.png" class loading="lazy"> 

<img src="/posts/877ae5d4/18.png" class loading="lazy"> 

<img src="/posts/877ae5d4/19.png" class loading="lazy"> 

<p><strong>论文前言一定要读</strong></p>
<img src="/posts/877ae5d4/20.png" class loading="lazy"> 

<p>只有和自己研究非常相近才需要仔细读。</p>
<p>自己的实验记录(<strong>及时用本记录</strong>)-材料、过程、现象、步骤、总结(只是记录，不要额外添加其他内容)</p>
<p>自己的研究计划–短期的，根据今天的实验结果，去想一下明天应该怎么做(实验内容、材料方法、特殊情况和对策、预期结果，没达到预期的情况、数据整理)。</p>
<img src="/posts/877ae5d4/21.png" class loading="lazy"> 

<p>带着思考读文献，<strong>有思路随手记一下，有时间看看是否需要做实验验证</strong></p>
<img src="/posts/877ae5d4/22.png" class loading="lazy"> 

<p>某种方法尝试能否进行拓展，不要急于发表，如果无法扩展再发表也不迟。</p>
<img src="/posts/877ae5d4/23.png" class loading="lazy"> 

<img src="/posts/877ae5d4/24.png" class loading="lazy"> 

<img src="/posts/877ae5d4/25.png" class loading="lazy"> 

<p><strong>把最重要的贡献体现在题目中</strong></p>
<img src="/posts/877ae5d4/26.png" class loading="lazy"> 

<p><strong>注意摘要和题目的一致性，摘要第一句话至少应该出现题目中三分之一的关键词，摘要是你要做，还没做的，摘要的时态是现在进行时</strong></p>
<img src="/posts/877ae5d4/27.png" class loading="lazy"> 

<img src="/posts/877ae5d4/28.png" class loading="lazy"> 

<img src="/posts/877ae5d4/29.png" class loading="lazy"> 

<img src="/posts/877ae5d4/30.png" class loading="lazy"> 

<img src="/posts/877ae5d4/31.png" class loading="lazy"> 

<p><strong>前言总结的问题就是正文要讨论的问题</strong></p>
<img src="/posts/877ae5d4/32.png" class loading="lazy"> 

<img src="/posts/877ae5d4/33.png" class loading="lazy"> 

<p><strong>结果讨论唯一目的就是让审稿人信服</strong></p>
<img src="/posts/877ae5d4/34.png" class loading="lazy"> 

<p>结论和摘要内容相近，但要注意时态，结论通常为现在完成时。</p>
<p>参考文献反应论文趋势。</p>
<img src="/posts/877ae5d4/35.png" class loading="lazy"> 

<img src="/posts/877ae5d4/36.png" class loading="lazy"> 

<p>选择熟悉的稿件格式，写完再去改格式也不迟。</p>
<h3 id="研究生学术生涯七个方面-–吴子牛"><a href="#研究生学术生涯七个方面-–吴子牛" class="headerlink" title="研究生学术生涯七个方面 –吴子牛"></a>研究生学术生涯七个方面 –吴子牛</h3><img src="/posts/877ae5d4/38.png" class loading="lazy"> 

<img src="/posts/877ae5d4/37.png" class loading="lazy"> 

<img src="/posts/877ae5d4/39.png" class loading="lazy"> 

<img src="/posts/877ae5d4/40.png" class loading="lazy"> 

<img src="/posts/877ae5d4/41.png" class loading="lazy"> 

<img src="/posts/877ae5d4/42.png" class loading="lazy"> 

<img src="/posts/877ae5d4/43.png" class loading="lazy"> 

<img src="/posts/877ae5d4/44.png" class loading="lazy"> 

<img src="/posts/877ae5d4/45.png" class loading="lazy"> 



<h3 id="SCI写作与投稿-–任驰"><a href="#SCI写作与投稿-–任驰" class="headerlink" title="SCI写作与投稿 –任驰"></a>SCI写作与投稿 –任驰</h3><p><strong>SCI写作步骤：</strong></p>
<img src="/posts/877ae5d4/46.png" class loading="lazy"> 

<img src="/posts/877ae5d4/47.png" class loading="lazy"> 

<img src="/posts/877ae5d4/48.png" class loading="lazy"> 

<img src="/posts/877ae5d4/49.png" class loading="lazy"> 

<img src="/posts/877ae5d4/50.png" class loading="lazy"> 

<img src="/posts/877ae5d4/51.png" class loading="lazy">

<img src="/posts/877ae5d4/52.png" class loading="lazy">  

<img src="/posts/877ae5d4/53.png" class loading="lazy">  

<img src="/posts/877ae5d4/54.png" class loading="lazy">

<p>养成实验记录习惯，避免忘记细节</p>
<img src="/posts/877ae5d4/55.png" class loading="lazy">

<p><strong>投稿前工作：</strong></p>
<img src="/posts/877ae5d4/56.png" class loading="lazy">

<img src="/posts/877ae5d4/57.png" class loading="lazy">

<p>结果和讨论是一篇文章中原创的内容</p>
<p><strong>投稿流程：</strong></p>
<img src="/posts/877ae5d4/58.png" class loading="lazy">



<h3 id="立足工程需求，矢志科研创新-–孙俊杰"><a href="#立足工程需求，矢志科研创新-–孙俊杰" class="headerlink" title="立足工程需求，矢志科研创新 –孙俊杰"></a>立足工程需求，矢志科研创新 –孙俊杰</h3><img src="/posts/877ae5d4/59.png" class loading="lazy">

<img src="/posts/877ae5d4/60.png" class loading="lazy">

<img src="/posts/877ae5d4/61.png" class loading="lazy">

<img src="/posts/877ae5d4/62.png" class loading="lazy">

<img src="/posts/877ae5d4/63.png" class loading="lazy">

<img src="/posts/877ae5d4/64.png" class loading="lazy">

<img src="/posts/877ae5d4/65.png" class loading="lazy">

<img src="/posts/877ae5d4/66.png" class loading="lazy">

<img src="/posts/877ae5d4/67.png" class loading="lazy">

<p>自己多尝试，多和导师，师兄师姐沟通，才能确定自己到底适合什么。</p>
]]></content>
      <categories>
        <category>记笔记</category>
      </categories>
      <tags>
        <tag>科研讲座</tag>
      </tags>
  </entry>
  <entry>
    <title>CounTR-论文阅读与源码理解</title>
    <url>/posts/6c13436b/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>这篇论文很久之前就看过一次，但当时也没有看太懂，正好最近在看Transformer的相关内容，所以就翻出来再读一遍。对部分内容做一个记录。本文内容为个人理解，如有错误，欢迎指正。</p>
<p>原文链接：[<a href="https://arxiv.org/abs/2208.13721">2208.13721] CounTR: Transformer-based Generalised Visual Counting</a></p>
<p>代码链接：<a href="https://github.com/Verg-Avesta/CounTR">Verg-Avesta&#x2F;CounTR: CounTR: Transformer-based Generalised Visual Counting</a></p>
<p>我对论文进行了复现，然后代码中有些可能与现版本不兼容的内容进行了修改，同时对源码增加了中文注释，放在github，链接为：<a href="https://github.com/LengNian/PaperRecurrent/tree/main/CounTR--Transformer-based">https://github.com/LengNian/PaperRecurrent/tree/main/CounTR--Transformer-based</a> Generalised Visual Counting(不要点链接，复制到浏览器)</p>
<p>本文需要用到的权重文件，可以在作者的Readme文件中找到。</p>
<h3 id="论文内容"><a href="#论文内容" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="两阶段训练"><a href="#两阶段训练" class="headerlink" title="两阶段训练"></a>两阶段训练</h4><p>本文采用一种两阶段的训练方式，即现在FSC147训练集上进行预训练，然后在下游任务上进行微调。</p>
<h5 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h5><p>预训练过程中使用MAE自监督训练，首先将输入图像分成不重叠的patch作为模型的输入，加入位置编码以后进行一个随机掩码，用于遮挡输入图像并送入编码器，然后通过解码器图像，得到对patch的恢复。然后通过计算恢复图像和原始图像的损失，来调整参数优化模型。</p>
<h5 id="微调阶段"><a href="#微调阶段" class="headerlink" title="微调阶段"></a>微调阶段</h5><p>在微调阶段，会用预训练的权重初始化图像的编码器，然后对模型再进行微调。该阶段使用的模型如图所示。</p>


<p><img src="/CounTR-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/1.png" alt="image-20241211192338128" loading="lazy"></p>
<p>作者将查询图像和示例图像都进行编码，查询图像编码器采用12层Transformer中的编码层$ \Phi_{VIT-ENC}$，该编码器采用Cross VIT网络。示例图像的编码器$ \Phi_{CNN-ENC}$用卷积层和全局池化层的组合。经过编码器后将查询图像编码后的的向量作为Query，示例图像编码后的向量作为Key和value送入到特征交互模块$\Phi_{FIM}$。特征交互模块由TYransformer的解码层构成。</p>
<p>解码器$\Phi_{DECODER}$使用一种渐进的上采样设计方法，一工会经过四次上采样，上采样操作包含了卷积层和2$\times$双线性插值组成。最后得到输出密度图。</p>
<p>在这个阶段，损失计算的是预测人数和实际人数之间的损失(与预训练阶段不同)。</p>
<h4 id="Mosaic图像增强"><a href="#Mosaic图像增强" class="headerlink" title="Mosaic图像增强"></a>Mosaic图像增强</h4><p>**为了解决长尾问题(重要的示例无法多次出现)**，所以提出了Mosaic图像增强，分为两个步骤，拼接和混合。</p>
<h5 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h5><p>拼接是从图像中裁剪出正方形区域，然后统一大小后，拼接起来，同时更新对应的密度图。文章提到了两种不同的拼接方式，分别是使用四张图或一张图(根据这幅图像的点注释来判断)，如果大于某个阈值，就用一张图，即将图像重复四次，否则使用四张图像，使用四张图像会用一张原图中裁剪出来的图像，其余从训练集中选择，如果类别一致就继续拼接，调整密度图，否则就只拼贴，密度图设置为全0。</p>
<h5 id="混合"><a href="#混合" class="headerlink" title="混合"></a>混合</h5><p>因为直接拼贴无法完美合成，所以会有一个混合宽度，就是用比拼接图像块固定尺寸稍微大一些，多出来的部分用于通道混合，使拼贴得到的图像更加逼真。</p>
<h4 id="测试时归一化"><a href="#测试时归一化" class="headerlink" title="测试时归一化"></a>测试时归一化</h4><p>在测试时，会使用测试时归一化。通过这种方法可以校准输出的密度图。</p>
<p>滑动窗口预测，如果边界框长宽都小于10像素，将会图像分成9块，每块是原图的九分之一，通过滑动窗口累计每个小块的计数和，计数和就是最终的计数结果。否则，直接利用滑动窗口预测。</p>
<h3 id="源码理解"><a href="#源码理解" class="headerlink" title="源码理解"></a>源码理解</h3><p>我对作者提供的代码中对FSC147数据集进行处理的内容以及模型文件做了注释。包括预训练、微调和测试部分。</p>
<p>具体内容可以看</p>
<p><a href="https://github.com/LengNian/PaperRecurrent/tree/main/CounTR--Transformer-based">https://github.com/LengNian/PaperRecurrent/tree/main/CounTR--Transformer-based</a> Generalised Visual Counting(不要点链接，复制到浏览器)</p>
<h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="https://0809zheng.github.io/2023/05/10/countr.html">CounTR: Transformer-based Generalised Visual Counting - 郑之杰的个人网站</a></p>
<p><a href="https://blog.csdn.net/qq_46981910/article/details/141871648">《CounTR: Transformer-based Generalised Visual Counting》CVPR2023-CSDN博客</a></p>
]]></content>
      <categories>
        <category>小样本计数</category>
        <category>目标计数</category>
        <category>论文复现</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>SwinTransformer学习记录</title>
    <url>/posts/1defb2fa/</url>
    <content><![CDATA[<p>本文用于记录自己在学习Swin Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。</p>
<h3 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h3><h4 id="Compare-Vit-and-Swin-Transformer"><a href="#Compare-Vit-and-Swin-Transformer" class="headerlink" title="Compare Vit and Swin-Transformer"></a>Compare Vit and Swin-Transformer</h4><ul>
<li><p>Swin-Transformer具有层次性，随着层次加深，高宽减小，而Transformer是直接下采样16倍，之后保持不变；</p>
</li>
<li><p>Swin-Transformer是将特征图分割，并且没有重叠，而transformer中没有分割(可以降低参数量 )；</p>
</li>
</ul>
<p> <img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215223036660.png" alt="image-20241215223036660" loading="lazy"></p>
<h4 id="Swin-transformer-network"><a href="#Swin-transformer-network" class="headerlink" title="Swin transformer network"></a>Swin transformer network</h4><p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215223210273.png" alt="image-20241215223210273" loading="lazy"></p>
<p>每经过一个stage，宽和高下采样两倍，但是channel也会扩大两倍。</p>
<p>Patch Partition和Linear Embedding(包含Layer Norm)都可以通过卷积层完成</p>
<p>每个Swin Transformer Block都是重复偶数次，因为每个Swin Transformer Block中都是右侧两个Block重复使用(将多头自注意力MSA替换为W-MAS和SW-MAS。</p>
<h4 id="Patch-Partition"><a href="#Patch-Partition" class="headerlink" title="Patch Partition"></a>Patch Partition</h4><p>将一个通道上的内容，用patch_size为4*4进行分割，然后在通道维再进行展平，</p>
<p>Patch Partition和Linear Embedding(包含Layer Norm)都可以通过卷积层完成。</p>
<h4 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h4><p>以4 * 4 * 1图像为例，窗口大小为2 * 2，会将每个窗口的第i个元素取出来，然后在通道方向堆叠。并在通道方向进行layer nrom。相比输入，长宽缩小二倍，通道扩大二倍。</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215224801017.png" alt="image-20241215224801017" loading="lazy"></p>
<h4 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h4><p> MSA是对特征图中的每个像素计算与其它像素之间的QKV，而W-MS A是对每个窗口中的每个像素计算QKV,而窗口和窗口之间没有任何交互。窗口之间无法进行交互会导致感受野变小，无法看到全局视野。 </p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215225126960.png" alt="image-20241215225126960" loading="lazy"></p>
<h4 id="Shifted-Windows-MSA"><a href="#Shifted-Windows-MSA" class="headerlink" title="Shifted Windows MSA"></a>Shifted Windows MSA</h4><p>实现不同窗口之间的信息交互，如果layeri用的MSA，那么layeri+1就要用SW-MSA。</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215230713869.png" alt="image-20241215230713869" loading="lazy"></p>
<p>其实就是将每个window向右、向下移动两个像素，这样再次划分的窗口就会不同。  </p>
<p>移动像素之后对新的特征图再次划分窗口，可以看到划分以后的新的特征图中窗口数变多，并且尺寸不同，如果想要进行并行计算，就必须把小于4*4的窗口重现填充为，这样就相当于计算9个windows，计算量又增加。</p>
<p>所以，作者提出一种高效的方法。</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215232051146.png" alt="image-20241215232051146" loading="lazy"></p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215232130757.png" alt="image-20241215232130757" loading="lazy"></p>
<p>对划分window后的区域重新分类，如上面第一张图，然后把1，2合并起来称为C，把3，6合并起来称为B。最后吧第一行移动到最下面，再把最左边一行移动到最右面，就是上面第二张图的效果。此时，将3，5合并，8，6，2，0合并，1，7合并就是四个4*4的window。这样子的计算量就和之前一样。</p>
<p>但是又有新的问题，以3，5为例，这里把3，5强行看成一个区域，原本这两个区域就不是相邻的，直接计算QKV是有问题的。 所以作者引入mask去解决这个问题。</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241215232720343.png" alt="image-20241215232720343" loading="lazy"></p>
<p>先依次计算q，可以看到 $ \alpha_{0，2}，\alpha_{0，3}，…$等是5和3之间不需要的内容，这里对他们的值-100，之后再通过softmax对应位置的值就会变为0，这样就达到了同时只计算5区域和3区域内的Q，其它区域同理。注意，计算完之后还要把对应数据还原。</p>
<h4 id="Relative-position-bias"><a href="#Relative-position-bias" class="headerlink" title="Relative position bias"></a>Relative position bias</h4><p>$ Attention(Q,K,V) &#x3D; SoftMax(QK^{T}&#x2F;\sqrt{d}+B)V$</p>
<h5 id="Relative-position-index"><a href="#Relative-position-index" class="headerlink" title="Relative position index"></a>Relative position index</h5><p> 在此之前先要介绍一下相对位置索引</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216094122036.png" alt="image-20241216094122036" loading="lazy"></p>
<p>以window_size&#x3D;2为例，绝对位置索引就是用行，列代表，而相对位置索引就是用当前匹配的像素位置索引为参考点，减去其它像素的位置索引，如上图所示。然后将其在行方向上展平。</p>
<p>但是这里使用的是二维坐标，而作者在原论文中使用的是一维坐标。如果只是简单的行列相加，就会令(0，-1)和(-1，0)原本不是同一位置的地方取到相同的值。作者是这样做的：(M代表窗口大小)</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216094627443.png" alt="image-20241216094627443" loading="lazy"></p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216094655799.png" alt="image-20241216094655799" loading="lazy"></p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216094709682.png" alt="image-20241216094709682" loading="lazy"> </p>
<p> 经过这样的操作，可以看到原本位置相同仍然可以取到相同的索引，位置不同但行列和相同的取到了不同的坐标。</p>
<p><img src="/SwinTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216095309203.png" alt="image-20241216095309203" loading="lazy"> </p>
<p>然后就可以根据relative position index从中取到bias。</p>
<p>为什么 relative position bias table的长度是(2M-1)*(2M-1)呢？</p>
<p>对于window_size&#x3D;M，能取到的索引点的最大位置就是[-M+1， M-1],所以，行&#x2F;列索引可取到的数就有2M-1种可能，所以最后relative position bias table的长度就是(2M-1)*(2M-1)。</p>
<p>相对位置索引是为了在相对位置偏执中取参数，训练过程中真正训练的是relative position bias table中的内容。</p>
<h3 id="参考文章-视频"><a href="#参考文章-视频" class="headerlink" title="参考文章(视频)"></a>参考文章(视频)</h3><p><a href="https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.337.search-card.all.click&vd_source=ce2ba30706a07f3300142a42d4e3023f">12.1 Swin-Transformer网络结构详解_哔哩哔哩_bilibili</a></p>
<p><a href="https://blog.csdn.net/qq_37541097/article/details/121119988?spm=1001.2014.3001.5501">Swin-Transformer网络结构详解_swin transformer-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/367111046">图解Swin Transformer - 知乎</a></p>
<p><a href="https://www.bilibili.com/video/BV1AL411W7dT?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">11.2 使用pytorch搭建Vision Transformer(vit)模型_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>写笔记</category>
      </categories>
      <tags>
        <tag>Swin-Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>VisionTransformer学习记录</title>
    <url>/posts/2d942648/</url>
    <content><![CDATA[<p>本文用于记录自己在学习Vision Transformer中理解，方便日后复习，文章中所使用图片全部来自于参考文章中所列文章或视频，仅用于个人学习使用。</p>
<h3 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h3><p><img src="/VisionTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216115355450.png" alt="image-20241216115355450" loading="lazy"></p>
<p>这里的输入把图像分成大小为16*16的patch，然后将每个patch输入到Linear Projection of Flattened Patches(Embedding层)，就能得到token。在这些token之前会再增加一个class token，并且加入位置编码Position Embedding。再输入到Transformer Encoder中，最后用MLP Head进行分类。</p>
<h4 id="Embedding层"><a href="#Embedding层" class="headerlink" title="Embedding层"></a>Embedding层</h4><p>对于标准的Transformer模块，输入应该是token序列，[num_token, token_dim]。</p>
<p>实际操作中，Linear Projection of Flattened Patches是直接通过一个卷积层来实现的。，以ViT-B&#x2F;16为例，使用卷积核大小为16*16，stride为16，卷积核个数为768，这里的768就是token_dim。</p>
<p>图像尺寸的变化就是[224, 224, 3] –&gt; [14, 14, 768] –&gt; [196, 768]，先通过卷积层，再把长宽摊平。(14 &#x3D; 224&#x2F;16, 196&#x3D;14*14)</p>
<p>在输入到Transformer Encoder之前需要加上class token以及position Embedding(这些都是可训练参数)</p>
<p>拼接class token： Cat([1, 768], [196, 768]) –&gt; ([197, 768])</p>
<p>叠加Position Embedding: [197, 768] -&gt; [197, 768] (加数值，所以维度没有发生变化)</p>
<p>位置编码的余弦相似度：</p>
<p><img src="/VisionTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216124045127.png" alt="image-20241216124045127" loading="lazy"></p>
<h4 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h4><p><img src="/VisionTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216130617836.png" alt="image-20241216130617836" loading="lazy"></p>
<h4 id="MLP-Head"><a href="#MLP-Head" class="headerlink" title="MLP Head"></a>MLP Head</h4><p> MLP Head可以简单理解为就是一个Linear层。</p>
<p><img src="/VisionTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20241216131240947.png" alt="image-20241216131240947" loading="lazy"></p>
<h3 id="ViT-B-16结构"><a href="#ViT-B-16结构" class="headerlink" title="ViT-B&#x2F;16结构"></a>ViT-B&#x2F;16结构</h3><p><img src="/VisionTransformer%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/721d65bffe3543a7e87b970a4a15eb54.png" alt="img" loading="lazy"></p>
<h3 id="参考文章-视频"><a href="#参考文章-视频" class="headerlink" title="参考文章(视频)"></a>参考文章(视频)</h3><p><a href="https://www.bilibili.com/video/BV1Jh411Y7WQ?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">11.1 Vision Transformer(vit)网络详解_哔哩哔哩_bilibili</a></p>
<p><a href="https://www.bilibili.com/video/BV1AL411W7dT?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">11.2 使用pytorch搭建Vision Transformer(vit)模型_哔哩哔哩_bilibili</a></p>
<p><a href="https://blog.csdn.net/qq_37541097/article/details/118242600">Vision Transformer详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/weixin_42392454/article/details/122667271">狗都能看懂的Vision Transformer的讲解和代码实现_vision transformer代码-CSDN博客</a></p>
]]></content>
      <categories>
        <category>写笔记</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo上传文章时图片文件夹被随意截断问题</title>
    <url>/posts/4522cfbc/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>对于自己的博客，上传图片都是通过hexo-asset-image插件进行上传，然后在文章中使用固定格式 <img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108214823646.png" alt="image-20250108214823646" loading="lazy"> 就可以上传了，当时具体的解决步骤在这里<a href="https://lengnian.github.io/posts/95fa3d73/">Hexo-GitHub Pages解决上传文章图片不显示的问题 | WeiSJ&amp;HEXO</a>，感兴趣的可以去看一下。</p>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>但是用了两个月以后，就出现了仍然按照之前的方法，但是部署的时候就会看到图片的上传路径明显出现问题。比如我的文章名为visionTransformer学习记录。那么按照我上面链接中的方法，在文件夹中会有这样的格式</p>
<pre class="line-numbers language-none"><code class="language-none">_posts
|--visionTransformer学习记录.md  # 文章编辑
|--visionTransformer学习记录  # 存放文章编辑中用到的图片<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>然后正常上传以后，在github的仓库中这篇文章下就是有一个index.html和用到的图片，如下</p>
<p><img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108210548851.png" alt="image-20250108210548851" loading="lazy"></p>
<p>而现在呢，只会有包含文章的index.html，图片会被随意上传到某个文件夹，比如这个样子:</p>
<p><img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108210757998.png" alt="image-20250108210757998" loading="lazy"></p>
<p>部署时可以看到这样的东西(马赛克遮住了我发表在CSDN上的问题)：</p>
<p><img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108211334687.png" alt="image-20250108211334687" loading="lazy"> </p>
<p>可以看到生成时文件夹就出现了问题，然后后面也是一连串的问题。</p>
<p>最初我以为是使用主题的问题，后来，我试着更换了主题，然后上传时还是这样的问题，也可能是插件的问题，我又卸载插件重新安装等一系列方法，都是不行。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>后来我无意看到一种方法，这种方法将 source&#x2F;_posts&#x2F; 目录下的所有子目录中的 img 文件夹复制到 public&#x2F; 目录中。</p>
<pre class="line-numbers language-hexo" data-language="hexo"><code class="language-hexo">hexo g
cp -r source&#x2F;_posts&#x2F;*&#x2F; public&#x2F;
hexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>根据我的存放结果修改为</p>
<pre class="line-numbers language-none"><code class="language-none">hexo g
cp -r source&#x2F;_post&#x2F;*&#x2F; public&#x2F;
hexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>cp -r source&#x2F;_posts&#x2F;*&#x2F; public&#x2F;（代码框里的source有问题，直接复制这里的。）</p>
<p>再次部署，图片成功上传，并且这种方法不需要使用这种固定格式</p>
<p><img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108214837054.png" alt="image-20250108214837054" loading="lazy">，可以直接把图片黏贴到markdown文件中，如下。</p>
<p><img src="/hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E7%AB%A0%E6%97%B6%E5%9B%BE%E7%89%87%E6%96%87%E4%BB%B6%E5%A4%B9%E8%A2%AB%E9%9A%8F%E6%84%8F%E6%88%AA%E6%96%AD%E9%97%AE%E9%A2%98/image-20250108212801193.png" alt="image-20250108212801193" loading="lazy"></p>
<p>但是，这样部署以后图片文件夹全部跑到了github仓库中首页而不是post下对应的文件夹中。</p>
<p>虽然还是有点瑕疵，但总归解决了问题。之后如果有新的问题再去看看其它的解决方法。</p>
]]></content>
      <categories>
        <category>解问题</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github Pages</tag>
      </tags>
  </entry>
  <entry>
    <title>《小小的我》观后感</title>
    <url>/posts/90f0037e/</url>
    <content><![CDATA[<p>今天去看了电影《小小的我》，自己很久没有这么冲动的想去看一部电影。但是因为最近有很多事情，一直拖到了6号才去看。2号有几门蛮重要的考试，然后3，4，5号自己最好的朋友来找我玩，6号早上考完试，中午就买了票下午就去了电影院。</p>
<p>为什么会特别想要去看这部电影呢？因为我身边就有脑瘫病人。研究生阶段有一位同学应该就患有脑瘫，但是可能症状比电影中轻一些。每次看到他，总是会感到他真的很勇敢，一个人来学校上学，不顾及周围人的眼光。因为上大课的缘故，所以自己也近距离接触过他，印象最深的一次就是一门课的考试，当时老师要求写结课论文，他带着手套拿着笔一直坚持写的那一刻，我真的泪目了。相比他而言，自己已经非常幸运了，最起码我有健全的身体。难以想象，他在考研备考时会有多么不被他人看好，他又是怎么坚持下来的，正如易烊千玺在电影中说的一样,”我是一个正常的成年男性”，我想身残志坚在那一刻真的具象化了。除了这位同学之外，在我家附近，还有一位脑瘫患者。她是一位女孩，应该小我几岁，因为我家周围就是体育场，她家周围应该也在附近，所以从小我总是能见到她和她的父母推着一个辅助走路的架子在体育场里面练习，到后面我上初中的时候，也能见到她自己一个人推着架子独自上下学，印象中，这个小女孩总是笑嘻嘻的，自己一步一个脚印坚持着，老天爷有时候真的蛮不公平的，给予了如此好的性格，但又给予了她身体上的残疾。上高中以后，我们就搬到新家了，虽然中午也还会去吃饭，但是印象中基本没有见过这个女孩了，更别谈上了大学，更是没有见过她，也不知道她恢复的怎么样。无论如何，希望她还在上学，希望她还在坚持着与命运进行抗争。</p>
<p>以上其实就是我去看这部电影最大的原因，我想要去看看易烊千玺表演下的脑瘫病人是如何的，也想通过这部电影对脑瘫病人有更深入的了解。至于这部电影的题材少见其实完全不是我有冲动去电影院观看的理由。</p>
<p>电影整体剧情其实没什么不合理的地方，网上其实看了很多影评认为女孩那段没什么意思，但是我觉得女孩那段导演就是想表达出脑瘫病人是一个正常人，也会对爱情充满向往。</p>
<p>印象蛮深刻的两个点就是拿到通知书外婆那段表演，以及和母亲冰释前嫌那部分，当时自己有些想哭，我也能听到后面观众在抽纸擦泪。</p>
<p>写了半天突然就不知道写什么了，虽然题目是观后感，但其实写了一大堆自己为什么去看电影，可能我本身就是想写为什么要去看这个电影，有点不想写了，那就到这里吧。</p>
]]></content>
      <categories>
        <category>记随笔</category>
      </categories>
      <tags>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title>LOCA论文阅读与源码理解</title>
    <url>/posts/9bf517d2/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>这是第三遍读这篇论文了，同时自己计划对着源码把每次变换的形状弄明白，所以再看一遍论文的细节，并且对一些数据维度进行一下标注。</p>
<p>论文链接：[<a href="https://arxiv.org/abs/2211.08217">2211.08217] A Low-Shot Object Counting Network With Iterative Prototype Adaptation</a></p>
<p>我对论文代码进行了注释，并且对数据在变换过程中的形状变换进行了标注，可以在[PaperRecurrent&#x2F;LOCA–A Low-Shot Object Counting Network With Iterative Prototype Adaptation at main · LengNian&#x2F;PaperRecurrent](<a href="https://github.com/LengNian/PaperRecurrent/tree/main/LOCA--A">https://github.com/LengNian/PaperRecurrent/tree/main/LOCA--A</a> Low-Shot Object Counting Network With Iterative Prototype Adaptation)中找到。此外，我修改了代码，使其可以成功运行，同时作者给出的代码是在多卡上运行，我进行修改后，可以在自己笔记本上单卡运行(按照作者给的训练指令，只需把卡数变成1即可)。</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p>(1) 输入图像的大小调整为$ H_{in} \times W_{in}$，使用ResNet50编码，从ResNet50第二、三、四个block中提取到的多尺度特征也调整大小到$ h \times w$，并使用1*1卷积将通道变为d维，然后通过一个全局自注意力块(3层的Transformer encoder)加强编码特征，产生了图像特征$f^E \in R^{h \times w \times d}$;</p>
<p>(2) 将边界框和图像特征$f^E$送入物体原型提取模块(OPE)，就会得到n个物体原型$ q_i^{O} \in  R^{s \times s \times d}$，图像特征和原型深度关联得到了多通道的相似性张量$\tilde{R}_i, \tilde{R}_i &#x3D; f^E * q_i^O $。通过每个通道，每个像素进行最大值运算，将n个单独的相似性向量$\tilde{R}$融合在一起形成一个联合相应张量$\tilde{R} \in R^{h \times w \times d}$;</p>
<p>（3) 使用一个回归头预测最后的二维密度图$R \in R^{H_{IN} \times R_{IN}}$。回归头由三个卷积层组成，包括128，64，32通道，每一个卷积层后面跟随一个Leaky ReLU和一个2$\times$双线性采样层和一个线性1*1卷积层，后面再跟一个Leaky ReLU。</p>
<p><img src="/LOCA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/image-20250111152213842.png" alt="image-20250111152213842" loading="lazy"></p>
<h3 id="Object-prototype-extraction-module-OPE"><a href="#Object-prototype-extraction-module-OPE" class="headerlink" title="Object prototype extraction module(OPE)"></a>Object prototype extraction module(OPE)</h3><p><img src="/LOCA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/image-20250111152236220.png" alt="image-20250111152236220" loading="lazy"></p>
<p>OPE使用图像$f^E和对应的n个边界框$${b_i}_{i&#x3D;1:n}$生成n个对象原型$ q_i^{O} \in  R^{s \times s \times d}$</p>
<h4 id="获得外观和形状查询"><a href="#获得外观和形状查询" class="headerlink" title="获得外观和形状查询"></a>获得外观和形状查询</h4><p>外观查询$q_i^A \in R^{s \times s \times d}$被提取通过RoI池化操作从图像特征$f^E$中提取，转换为s*s的张量。<strong>因为池化把特征图转化为相同大小，所以形状信息在这个过程中被忽略了。</strong>形状信息用示例原型的高和宽进行初始化，第i个边界框对应的形状查询是通过将其宽度和高度进行非线性映射成为高维张量($R^2 -&gt; R^{s \times s \times d}$)，即$q_i^S &#x3D; \phi([b_i^W, b_i^H])$**(这里说每个形状信息是通过初始化示例的高和宽，所以相当于就是按照边界框的高和宽进行初始化。)**，映射$\phi$是一个三层的FFN组成，每个linear后有一个ReLU。</p>
<h4 id="外观和形状查询注入到对象原型-通过迭代适应模块"><a href="#外观和形状查询注入到对象原型-通过迭代适应模块" class="headerlink" title="外观和形状查询注入到对象原型(通过迭代适应模块)"></a>外观和形状查询注入到对象原型(通过迭代适应模块)</h4><p><img src="/LOCA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/image-20250111152254657.png" alt="image-20250111152254657" loading="lazy"></p>
<p>首先将形状查询调整形状成为一个矩阵$Q^S \in R^{ns^2 \times d}$，外观查询和特征图也同理得到$ Q^A \in R^{ns^2 \times d}$和$F^E \in R^{hw \times d}$。</p>
<h5 id="少样本"><a href="#少样本" class="headerlink" title="少样本"></a>少样本</h5><p>然后通过一个循环的交叉注意力块将外观查询和和形状查询注入到对象原型中，在这里的交叉注意力块包括形状查询和外观查询进行多头交叉注意力以及特征图和上一个多头交叉注意力的结果再进行多头交叉注意力，最后再通过前馈传播网络，得到一次注入的结果$Q^{l} \in R^{ns^2 \times d}，l \in {1,…,L}$，如此迭代L轮。最后迭代适应模块的输出$Q^L$重新调整形状为一组n个对象原型$q_i^O \in  R^{s \times s \times d}$。</p>
<h5 id="零样本"><a href="#零样本" class="headerlink" title="零样本"></a>零样本</h5><p>如果是零样本的情况下，就没有形状查询和外观查询，所以就初始化一个可以学习的参数来代替最初的$Q^l$。</p>
<h3 id="训练损失"><a href="#训练损失" class="headerlink" title="训练损失"></a>训练损失</h3><p>同大多数的工作一样，使用L2 loss，但是这里增加了一个辅助损失，用于更好地监督迭代适应模块的训练。</p>
<p><strong>(从原文的图中可以看出，迭代适应模块的结果经过一系列最大化的操作，就得到了响应图。再经过回归头就得到了预测密度图，所以这里把每次迭代适应的输出通过max和regress head就得到了当前迭代输出的结果所对应的预测密度图，利用这个密度图与ground truth进行L2 loss，这就是辅助损失)</strong></p>
<p>最终的损失就是$ L &#x3D; L_{OSE} + \lambda_{AUX}L_{AUX}$</p>
<h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>$H_{IN} &#x3D; W_{IN} &#x3D; 512, h &#x3D; w &#x3D; 64, d&#x3D;256 $</p>
<p>迭代适应模块中的MHA包含8个头，隐藏维d&#x3D;256，FFN的隐藏维为1024；进行3次迭代，即L&#x3D;3，s &#x3D; 3</p>
]]></content>
      <categories>
        <category>小样本计数</category>
        <category>论文复现</category>
        <category>目标计数</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>BMNet论文阅读与源码理解</title>
    <url>/posts/f9d2e46c/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近重新读了BMNet这篇论文，细扒一下论文的细节，同时将细节部分和代码进行一下对应。我对论文关键部分进行了注释，并且注释了数据在网络中的形状变化，方便更好的理解数据是如何被处理的。</p>
<p>论文地址：[<a href="https://arxiv.org/abs/2203.08354">2203.08354] Represent, Compare, and Learn: A Similarity-Aware Framework for Class-Agnostic Counting</a></p>
<p>源码地址：<a href="https://github.com/KevinJ-Huang/BMNet">KevinJ-Huang&#x2F;BMNet: CVPR 2022 (Official implementation of “Bijective Mapping Network for Shadow Removal”)</a></p>
<p>注释过代码地址：[PaperRecurrent&#x2F;BMNet–Represent, Compare, and Learn A Similarity-Aware Framework for Class-Agnostic Counting at main · LengNian&#x2F;PaperRecurrent](<a href="https://github.com/LengNian/PaperRecurrent/tree/main/BMNet--Represent%2C">https://github.com/LengNian/PaperRecurrent/tree/main/BMNet--Represent%2C</a> Compare%2C and Learn A Similarity-Aware Framework for Class-Agnostic Counting)</p>
<h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p><img src="/BMNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/image-20250113193226233.png" alt="image-20250113193226233" loading="lazy"></p>
<h4 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h4><p>对于查询图像X，将其作为输入，经过由卷积层组成的feature extractor，会将其映射为通道数为d的特征，即通过下采样得到$F(x)\in R^{d \times \ h_x \times w_x}$，而对于exemplar Z，会通过全局平均池化得到一个特征向量$F(Z) \in R^d$。</p>
<h4 id="Learning-Bilinear-Similarity-Metric"><a href="#Learning-Bilinear-Similarity-Metric" class="headerlink" title="Learning Bilinear Similarity Metric"></a>Learning Bilinear Similarity Metric</h4><p>先前的方法通过固定的内积来计算两个特征向量之间的相似性，作者认为这种方法不能充分的模拟无类别之间的相似性。所以作者将最初的内积扩展到了一种可学习的双线性相似性。</p>
<p><strong>抛开论文不谈，我们需要先知道一下什么是双线性度量。双线性度量是一种用于计算两个特征向量之间相似度或相关性的方法，其思想就是通过一个可学习的权重矩阵W，将两个特征向量x和y映射到一个标量值，用于表示他们的相似度或相关性。</strong></p>
<p>$ Similarity(x, y) &#x3D; x^T W y $</p>
<p>这里的x和y就是两个输入特征向量，而W是一个可学习的权重矩阵，用于捕获x和y之间的交互关系。</p>
<p>我们再回到论文中，上面可以看到查询图像$F(x) \in R^{d \times h_x \times w_x}$，这里把$F_{ij}(X) \in R^d$ 取出来，$F_{ij}$ 其实就是一个$h_x \times w_x$的特征图。把(i, j)看作是特征图的空间位置。同时定义$x_{ij} &#x3D; F_{ij}(x)$和$z&#x3D;F(Z)$。那么双线性度量就可以表示为$x_{ij}^TWz$。论文中又提到提出一种可学曦的双线性度量，所以将W分解并添加偏置，就得到了论文中的公式，即：$S_{ij}(x,z) &#x3D; (Px_{ij}+b_x)^T (Qz+b_z)$，S是相似图，P和Q就是可学习的指标，b就是偏置，并且$P,Q \in R^{d \times d}，b_x,b_z \in R^{d \times 1}$(将W分解为P,Q并且P,Q分别i与query和exemplar对应)。对于n个exemplars，我们可以得到n个相似图，最后对这些相似图进行平均，得到最后的相似图S。</p>
<h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h4><p>把特征图F(X)和相似图S堆叠起来我们就得到了Counter的输入，Counter会预测出密度图$D_{pr}$。Counter是由卷积层和双线性上采样层组成。</p>
<h3 id="Learning-Dynamic-Similarity-Metric"><a href="#Learning-Dynamic-Similarity-Metric" class="headerlink" title="Learning Dynamic Similarity Metric"></a>Learning Dynamic Similarity Metric</h3><p><strong>在上面我们提到的双线性度量在训练后就固定不变，无法根据样本的特定模式动态调整的，所以作者提出一种动态相似度度量</strong>，通过特征选择模块生成针对特定样本的度量。通过SENet的思想，将$Qz+b_z$作为条件，学习一种动态通道权重a，更新后的计算公式即：</p>
<p>$S_{ij}(x,z) &#x3D; [(Px_{ij}+b_x)]^T [a \circ (Qz+b_z)]，\circ$表示哈达玛积，也就是逐元素相乘。  </p>
<p>下面这段黑体不需要看之前去看Supervising the Similarity Map这一小节，是我自己没看完代码当时的困惑，而且自己看的过程忽略了相似图的计算并不是在自注意力部分，而是在matcher部分，因为这也是自己的思考，所以就不删除了。</p>
<p>（<strong>但这里我有一个不明白的地方，如果将P,Q以及b看作是Linear中学习的参数，作者公式中写的是分别对query的特征图向量和exemplar投影后的向量进行运算，然后对exemplar的运算结果加权，但是代码实现过程中，是将特征图向量和exemplar向量堆叠在一起，再分别利用Linear的参数和偏置进行运算，我认为代码中表达出来的计算公式应该为</strong>$S_{ij}(x,z) &#x3D; [(PU+b_x)]^T [a \circ (QU+b_z)],U&#x3D;concat[x_{ij}, z] $。，忽略掉）</p>
<h3 id="Supervising-the-Similarity-Map"><a href="#Supervising-the-Similarity-Map" class="headerlink" title="Supervising the Similarity Map"></a>Supervising the Similarity Map</h3><p>大部分方法在训练过程中都只使用计数损失进行监督，<strong>而直接监督相似性匹配可以使相似性更好的建模</strong>，所以作者提出一种监督方式可以指导相似度的建模。假设相似图的尺寸是queryd的1&#x2F;r，那么S中的一个点就代表query中的r * r个点。对于S中的每一个点如果他所包含query中的r * r个点中包含了超过一个或多个的目标，那么就给他指派一个正标签，否则就是负标签，我们就得到了相似性度量损失。</p>
<p>$ L_{sim} &#x3D; -log \frac{\sum_{i \in pos}exp(S_i)}{\sum_{i \in pos}exp(S_i) + \sum_{j \in neg}exp(S_j)} $，通过这个公式，我们将最大化正标签在所有标签中的比例。</p>
<h3 id="Self-Similarity-Module"><a href="#Self-Similarity-Module" class="headerlink" title="Self-Similarity Module"></a>Self-Similarity Module</h3><p><strong>同一类别外形等各方面也会有各种不同的变化等，比如姿势，比例，这又给相似度匹配带来挑战</strong>，所以作者提出自注意力模块解决该问题。从query中提取每个特征向量$F_{ij}(x)$，将示例特征F(Z)和其收集到一个特征集合中，然后进行自注意力，自注意力之后的结果会与原始特征进行加权融合即$F_{final} &#x3D; F_{original} + \gamma F_{update} $。然后将加权融合以后的特征再重新变为exemplar和query，并且这里的$\gamma $也是一个可学习的参数。</p>
<p><strong>这里作者说是将收集特征图向量(也就是$F_{ij}X \in R^{d} $) 和exemplar特征组成一个特征集，实际在代码中就是通过堆叠query的特征图和exemplar映射成的特征向量，然后通过矩阵乘法达到每个向量之间计算相似度的目的</strong></p>
<p><img src="/BMNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%8E%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/image-20250113193249321.png" alt="image-20250113193249321" loading="lazy"></p>
<h4 id="scale-embed"><a href="#scale-embed" class="headerlink" title="scale embed"></a>scale embed</h4><p><strong>对图像大小的调整和特征提取过程中的池化操作损失了scale信息。</strong>所以作者提出了使用对应的scale embedding去弥补这部分丢失的信息。</p>
<p>我的理解是将尺度空间离散化为多个等级$l_{total}$，不同等级对应着一个d维的嵌入向量，那么总共就会有$l_{total}$个嵌入向量。</p>
<p>对于Query X和exemplar Z，会计算exemplar的尺度等级l(Z)，$ l(Z) &#x3D; min(l_{total}-1, \lfloor(\frac{h_Z}{2h_X}+\frac{w_Z}{2w_Z} \cdot ) \cdot l_{total} \rfloor )$，得到尺度等级l(Z)以后，就可以选择对应的d维嵌入向量添加到exemplar中以增强其特征表示。	</p>
<h3 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h3><p>限制query的大小，将其限制在[384， 1584]以内，exemplar的大小调整为128 * 128。</p>
<p>特征提取器的输出通道为1024，对于query，使用1 * 1卷积将其通道减小为256，对于exemplar使用全局平均池化和线性层得到一个256维的张量。</p>
<p>Counter包含了一系列双线性上采样层和卷积层，最后的密度图和输入图像的大小是保持一致的。</p>
]]></content>
      <categories>
        <category>小样本计数</category>
        <category>目标计数</category>
        <category>论文复现</category>
      </categories>
      <tags>
        <tag>读论文</tag>
      </tags>
  </entry>
  <entry>
    <title>2024年总结</title>
    <url>/posts/795668d8/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>虽然这是2024年总结，但是我一直拖到现在才写，哈哈哈，实在是太烂了。</p>
<p>回看整个2024年，于我而言也是蛮重要的一年，最重要的一件事就是完成了从本科生到研究生身份的转变，其次这一年我也陪着妈妈去了两个城市旅游，我们大家都知道，一旦进入大学，和父母能在一起的时间就过了多半，所以我也是格外珍惜这次机会。这也是我觉着2024年最重要的两个事情了吧，剩下的就按时间顺序去写吧！！！</p>
<h4 id="2024-01-2024-03"><a href="#2024-01-2024-03" class="headerlink" title="2024.01-2024.03"></a>2024.01-2024.03</h4><p>这两个月应该是考完研以后到开学那段时间，依稀感觉考研似乎仍然在昨天，我记得考完研之后我们学校当时有一个实训的验收，所以1月份首先我应该是先完成了一个实训验收，记得做的项目是基于yolov5的安全帽检测，当时刚考完试估完分，记得自己大哭了一场，状态也并不是很好。验收过后，记得每天还是去看看复试要考的课，但实际一去图书馆就想玩手机，根本学不进去。对了。这段时间还有毕业设计的选题，当时的指导老师方向是NLP，但我更新换CV一些，所以当时并没有做老师给出的题目，而是自己选择题目。因为状态并不在线，所以看论文也是囫囵吞枣的，最后听老师的建议，选了公共卫生方面得题目，具体题目在后面再说吧。然后选好题我应该就回家了吧。我记得大概是1月19号回的家。回家也就是疯狂的玩，根本没有心思看复试的内容，反正及时控制不了自己，想着开了学再去学应该也来得及。一直玩到2月末准备开学，记得当时出考研成绩是在开学以后，当时我妈强烈要求我在家查成绩，而我自己也不太想开学，所以就买了三月一号的票，等查完成绩再去学校。再之后就是考研查成绩，查成绩前一天我也并没有什么食欲，当天也没有食欲，还记得查成绩本来是下午三点，但是中午吃饭的时候有人说有bug，所以当时也顾不得吃饭了，立马就去电脑前等着了。查出成绩来感觉还不错，随便扒拉几口饭就准备联系老师，完善自己的简历（当时的简历基本做的差不多了，并且老师也筛了一轮，但是并没有问过学长学姐之类的，所以一下午一晚上都在各种询问，一直忙活到晚上一点多），我记得当时还有一个情况就是考研出分的第二天早上六点会出六级成绩，所以我当天并没有发邮件，而是第二天查到成绩后再发的。</p>
<h4 id="2024-03-2024-04"><a href="#2024-03-2024-04" class="headerlink" title="2024.03-2024.04"></a>2024.03-2024.04</h4><p>三月回校之后就是准备复试了，好几门课，也是背来背去的，白天背专业课，准备自己面试要回答的问题，晚上背英语，一直坚持了一个月，当时三月份开题，也是随便搞了搞，和老师说先准备复试了。具体的复试时间我记不清了，不是三月底就是四月初，当时复试了两天，一天笔试，一天面试，笔试三门，面试当天，整个走廊似乎也就我一个人穿了西装，好尴尬，哈哈哈。</p>
<h4 id="2024-04-2024-06"><a href="#2024-04-2024-06" class="headerlink" title="2024.04-2024.06"></a>2024.04-2024.06</h4><p>四月初这段时间还是蛮煎熬的，毕竟在等复试结果，等的每天都是在床上，没心思做毕设，也没心思玩，就是隔一阵看看群，刷新刷新官网，哈哈哈。但是我记得出成绩那天，我也是煎熬了一整天，然后到晚上八点了出去吃饭，吃完饭回来的路上就看见群里说出名单了，哈哈哈哈，一高兴就又叫上朋友去好想来买了一大堆零食（有点报复性消费的感觉）。</p>
<p>清明我记得和朋友去做了蛋糕，然后去了北京动物园，回学校就安心准备毕业设计了，记得我的毕设是处理肺炎的，属于改进网络结构那方面，总是感觉太简单，隔两天就去给老师点压力，让她给我交个底，看看能不能过，哈哈哈，经常从晚上七八点聊到十点多。哈哈哈哈。不过我的毕设老师也是蛮好的，非常有耐心，也很感谢老师给我的鼓励。基本上到中期的时候我的网络结构方面的工作就已经全部完成了，但是中期验收，其它老师要求做成个系统，方便展示，虽然很不情愿，但是也用了几天立马搭建了一个展示系统，算是代码部分就完成了吧。整个论文写了很久，得有一个半月，我四月份还在做实验的时候就开始写了，但是老师也没基本没有给我修改就让我直接查重了，也算是没白耗费时间。查重第一次好像得有40，然后老师让我弄到15以下，查重花了好几百。做毕设那段时间总是焦虑，我也不太像去回忆，就写这么多吧，直接快进到毕业吧。</p>
<h4 id="2024-06-2024-09"><a href="#2024-06-2024-09" class="headerlink" title="2024.06-2024.09"></a>2024.06-2024.09</h4><p>毕业就到六月份了，当时和朋友拍了一下午照，然后毕业典礼那天我妈妈也来参加了，后来在那玩了一天就回家了，我的大学也算是彻底结束了。</p>
<p>假期也是酷酷玩，和我妈妈去了秦皇岛，天津这两个地方，然后自己也和朋友去爬了一次泰山（旅游的详细之前的文章已经写过了，这里就不赘述了，在这里<a href="https://lengnian.github.io/posts/2ddb491e/?highlight=%E6%9A%91%E6%9C%9F">暑期游 | WeiSJ&amp;HEXO</a>）。</p>
<p>对了，暑假看到别人都有自己的博客，所以我也尝试自己弄了属于自己的博客，可以自己写一些东西，哈哈哈。</p>
<h4 id="2024-09-2024-12"><a href="#2024-09-2024-12" class="headerlink" title="2024.09-2024.12"></a>2024.09-2024.12</h4><p>开学是超级不情愿的，可能是因为山西人本就特别恋家，再加上我的家庭原因，可能我就更会恋家一些，但是也没有办法。我们学校是规定研一上学期要修完所有课的，所以研一上学期的主要任务就是上课了，把所有课程都通过是首要任务。因为要上的数学课比较多，所以大部分时间都花在上课，剩余的空闲时间可能会看看论文，总之，这个学期论文看了大概25篇左右，可能复现了几篇代码，但也只是跑通，并没有真正钻研每句代码的作用啥的。</p>
<p>这四个月似乎没有什么想要特别去记录的，就是按部就班地进行着，组会是固定时间段进行，如果有课可以不参加，所以基本上整个上学期也没有参加过几次组会，剩下的感觉生活就和大学一样，没什么区别。</p>
<h4 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h4><p>哈哈哈，总结其实没什么写的，毕竟我不是什么大佬，可能就只能记录一些自己生活中印象深刻的事情，记录一些时间节点吧，我觉着这可能才是普通学生的真实写照。</p>
<p>关于2025，嗯….</p>
<p>首先，希望我和我的家人都健健康康的，平安顺利，快快乐乐；</p>
<p>其次，希望自己早点可以发出自己的论文，一篇应该是最低要求了，我给自己定的目标是要努力朝着两篇论文出发（写总结的时候，人已经非常焦虑，毕竟现在研二的师兄已经人手两篇，并且同门还超级超级卷！！！！！）</p>
<p>对于我自己来说，可能今年论文是最重要的事情吧，因为对于学硕来说，毕业条件就得有一篇论文，有点难搞。</p>
<p>有点烦，就这样吧，自己一定要加油！加油！再加油！！！</p>
]]></content>
      <categories>
        <category>写文章</category>
      </categories>
      <tags>
        <tag>回忆录</tag>
      </tags>
  </entry>
  <entry>
    <title>Mamba学习记录</title>
    <url>/posts/e0dc302e/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近我想使用Mamba去代替一下Transformer，所以在b站看了耿直哥老师的讲解视频，前半段所有内容几乎全部来自视频内容。</p>
<p>如果想看我自己总结的部分，可以直接跳转到补充部分</p>
<p>最近自己又看到几篇文章感觉讲的非常好，也在这里一同推荐给大家。</p>
<p><a href="https://zhuanlan.zhihu.com/p/694695534">Mamba详解(一)之什么是SSM？ - 知乎</a> 这篇文章的一系列感觉都很清晰，可以在作者文章中找到其它部分</p>
<p><a href="https://srush.github.io/annotated-s4/">The Annotated S4</a></p>
<p><a href="https://blog.csdn.net/v_JULY_v/article/details/134923301">一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba(被誉为Mamba最佳解读)_mamba模型-CSDN博客</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.bilibili.com/video/BV1Xn4y1o7TE?spm_id_from=333.788.videopod.sections&vd_source=ce2ba30706a07f3300142a42d4e3023f">AI大讲堂：革了Transformer的小命？专业拆解【Mamba模型】_哔哩哔哩_bilibili</a></p>
<p> <a href="https://www.maartengrootendorst.com/blog/mamba/">A Visual Guide to Mamba and State Space Models - Maarten Grootendorst</a></p>
<p><a href="https://blog.csdn.net/weixin_44162361/article/details/144591024">图文并茂【Mamba模型】详解-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/v_JULY_v/article/details/134923301">一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba(被誉为Mamba最佳解读)_mamba模型-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/694695534">Mamba详解(一)之什么是SSM？ - 知乎</a></p>
<p>关于安装mamba_ssm我主要参考了这几篇文章</p>
<p><a href="https://jishuzhan.net/article/1808857327386759169">Ubuntu和Windows系统之Mamba_ssm安装 - 技术栈</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/686355774">配置mamba-ssm环境的乱七八糟的可能有用的操作 - 知乎</a></p>
<p><a href="https://blog.csdn.net/m0_55684014/article/details/145939814">mamba_ssm和causal-conv1d详细安装教程_causal-conv1d离线安装包-CSDN博客</a></p>
<h2 id="Transformer缺点"><a href="#Transformer缺点" class="headerlink" title="Transformer缺点"></a>Transformer缺点</h2><p>  位置编码：把时序内容空间化   </p>
<p>对于transformer来说，其中的自注意力机制存在一个天然的缺陷，就是其自注意力机制的计算范围仅仅局限在了窗口内，而忽略了窗口外的元素，这就造成了视野狭窄，缺乏了全局观。如果增加窗口的长度，那么计算量会呈平方增长。</p>
<p><strong>本质上说，Transformer就是通过位置编码，将序列数据空间化，然后通过计算空间相关度方向建模时序相关度，这个过程忽视了数据内在结构的关联关系。</strong> (我的理解就是对于输入数据，无论其是否冗余或者是否重要，都统一进行位置编码，然后将其空间化，计算其空间相关度)。但是这种做法是在当年为了充分利用GPU的并行能力，SSM类模型(时序状态空间模型SSM)就是让长序列数据建模回归传统，这是其思考问题的初衷和视角。</p>
<h2 id="时序状态空间模型SSM"><a href="#时序状态空间模型SSM" class="headerlink" title="时序状态空间模型SSM"></a>时序状态空间模型SSM</h2><h3 id="连续空间的时序建模"><a href="#连续空间的时序建模" class="headerlink" title="连续空间的时序建模"></a>连续空间的时序建模</h3><p>Mamba是基于结构化状态空间序列模型(SSMs),对应论文[<a href="https://arxiv.org/abs/2110.13985">2110.13985] Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers</a></p>
<p> 上面这篇论文本质也是RNN模型。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320151906758.png" alt="image-20250320151906758" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320152156279.png" alt="image-20250320152156279" loading="lazy"></p>
<p>这一段其实就是解释了上图中Continuous-time所对应的内容。绝大部分情况下，都是时变的，是动态的，而非时不变的。</p>
<h3 id="时序离散化与RNN"><a href="#时序离散化与RNN" class="headerlink" title="时序离散化与RNN"></a>时序离散化与RNN</h3><p>其对应Recurrent这部分。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320153231028.png" alt="image-20250320153231028" loading="lazy"></p>
<p>所谓离散化就是上图中连续的数据变成了离散化的数据</p>
<p>而零阶保持则是变成离散化以后，将数据变化变成了阶跃式的，保持当前当前时间的状态。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320153315530.png" alt="image-20250320153315530" loading="lazy"></p>
<h3 id="并行化处理与CNN"><a href="#并行化处理与CNN" class="headerlink" title="并行化处理与CNN"></a>并行化处理与CNN</h3><p> 对应于最上面图中的Convolutional， 对于SSM而言，就是通过卷积实现了并行化。<strong>其核心思想就是使用CNN对时序数据进行建模，借助不同尺度的卷积核，从不同时间尺度上捕获时序特征。</strong></p>
<p> <img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320162416870.png" alt="image-20250320162416870" loading="lazy"></p>
<p>k可以理解为一个伸缩窗口，当前状态可以用之前输出的加权和来表征，再把 $ h_t$代入到输出的式子，与卷积的公式进行对比。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320162222937.png" alt="image-20250320162222937" loading="lazy"></p>
<p>在实际问题中，会对上述的AB矩阵进一步简化，会将其设置为对角阵，这就是结构化SSM，S4模型。</p>
<p>对于SSM模型，要记住，这里有两个强假设：<strong>线性+时不变</strong>，这两个假设极大的限制了其应用范围。而Mamba本质上就是对SSM模型的改进，其不再考虑这两个约束。</p>
<h2 id="Mamba"><a href="#Mamba" class="headerlink" title="Mamba"></a>Mamba</h2><p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320163937323.png" alt="image-20250320163937323" loading="lazy"></p>
<p>Mamba的设计机制让状态空间具备了选择性，同时在序列长度上实现了线性扩展。  </p>
<p>看这幅图的中间部分，BC都变成为了带有t的时变参数，A虽然没有加t，但其实也是时变的，因为会将Δt加入到A中，这里的Δt是一个非线性的，</p>
<p>Δt可以看作一个总开关，$B_t,C_t$就是旋钮。总开关 + 若干个旋钮 &#x3D; 非线性时变系统</p>
<h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>序列建模的核心就是研究如何将长序列的上下文信息压缩到一个较小的状态中。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320170719312.png" alt="image-20250320170719312" loading="lazy"> </p>
<p>作者希望可以关注两种能力：</p>
<p>1.选择性复制任务(抓重点的能力)</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320171055772.png" alt="image-20250320171055772" loading="lazy"></p>
<p>2.诱导头任务(上下文推理能力)</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320171115823.png" alt="image-20250320171115823" loading="lazy"></p>
<h3 id="怎么增加选择性？"><a href="#怎么增加选择性？" class="headerlink" title="怎么增加选择性？"></a>怎么增加选择性？</h3><p>让B和C由原来固定的变为了可变的，根据$x_t$和其压缩投影学习可变参数。上图中蓝色部分(包括投影和连线)就是所谓的选择机制。<strong>目的是根据输入内容选择性地记忆和处理信息，从而提高对复杂序列数据的适应能力。</strong></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320174904901.png" alt="image-20250320174904901" loading="lazy"></p>
<p>这里面的Δ是前面离散化计算时的参数，投影出来的三条蓝线其实就是$s_B,s_C和s_Δ三个选择函数$，共享一个投影模块(project)，主要是为了实现参数共享和计算效率。</p>
<p>B：batch size，L：Sequence length，N：Feature dimension，D：input feature dimension。</p>
<p> <img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320175824740.png" alt="image-20250320175824740" loading="lazy"></p>
<p>上述所提到的$s_B(x)&#x3D;Linear_N(x), s_C(x)&#x3D;Linear_N(x), s_Δ(x)&#x3D;Broadcast_D(Linear_1(x)), \tau_Δ&#x3D;softplus$</p>
<p>这样设计的效果如图所示：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320180310474.png" alt="image-20250320180310474" loading="lazy"></p>
<p> 这也达到了注意力的效果。</p>
<h3 id="核心原理"><a href="#核心原理" class="headerlink" title="核心原理"></a>核心原理</h3><p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320180647275.png" alt="image-20250320180647275" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320181221559.png" alt="image-20250320181221559" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320181254994.png" alt="image-20250320181254994" loading="lazy"></p>
<h3 id="Mamba结构"><a href="#Mamba结构" class="headerlink" title="Mamba结构"></a>Mamba结构</h3><p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320181852261.png" alt="image-20250320181852261" loading="lazy"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250320183146006.png" alt="image-20250320183146006" loading="lazy"></p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><h3 id="Part1-Transformer-and-RNN"><a href="#Part1-Transformer-and-RNN" class="headerlink" title="Part1: Transformer and RNN"></a>Part1: Transformer and RNN</h3><p>对于transformer来说，无论接收到什么输入，它都可以回溯序列中的任何早期标记，从而推导出其自己的表示。尽管已经生成了一些token，但是当生成下一个token时，transformer任然需要重新计算整个序列的attention，这就导致了二次方的计算复杂度，当序列长度增加，其代价也就越大。</p>
<p>RNN有两个输入，一个是当前时间步的输入，另一个则是先前时间步的隐藏状态。利用这两个输入，RNN和产生下一个隐藏状态以及预测输出。<strong>当预测时，RNN避免重新计算先前所有的隐藏状态(这正是transformer想要做的)。</strong>这就意味着RNN在推理时非常快，因为其线性的尺度，也意味着在理论上RNN可以处理无限长的文本长度。</p>
<p>但是RNN也存在一些问题，因为他只考虑上一个时间步的隐藏状态，所以随着时间推荐，其会忘掉一些重要信息(如下图，当处理到“Maarten”时，会遗忘掉“Hello”)；此外，RNN无法并行进行，因为它需要随着时间逐时间步的去进行处理。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327163227219.png" alt="image-20250327163227219" loading="lazy"></p>
<p>这时，RNN和Transformer所面临的优缺点就非常明显了。我们如何才能找到一种折中的方法呢？<strong>又可以像Transformer一样并行化处理，又可以随着序列长度进行线性扩展的推理。</strong>没错，就是Mamba。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327163554566.png" alt="image-20250327163554566" loading="lazy"></p>
<h3 id="Part2-The-State-Space-Model-SSM"><a href="#Part2-The-State-Space-Model-SSM" class="headerlink" title="Part2: The State Space Model(SSM)"></a>Part2: The State Space Model(SSM)</h3><p>状态空间就是能够充分描述一个系统所包含的最小变量数，其也是通过定义系统内可能的状态去数学化的代表一个问题。</p>
<p>在传统的状态空间中，对于时间t，存在输入序列x(t)，潜在的状态空间表示h(t)，预测输出序列y(t)。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327164433811.png" alt="image-20250327164433811" loading="lazy"></p>
<p>通过两个等式，就可以预测输出序列y(t)</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327164612517.png" alt="image-20250327164612517" loading="lazy"></p>
<p>这两个等式就是SSM的核心部分，(上面的式子被称为状态等式，下面的式子被称为输出等式)。</p>
<p>通过下图理解状态等式，<strong>可以看出隐藏状态是如何进行改变的(通过矩阵A)，以及输入是如何影响状态(通过矩阵B)。</strong></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327165316851.png" alt="image-20250327165316851" loading="lazy"></p>
<p>同理，对于输出等式，<strong>可以看出状态是如何转换成输出(通过矩阵C)，以及输入如何影响输出（通过矩阵D）。</strong></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327165526496.png" alt="image-20250327165526496" loading="lazy"></p>
<p>综上，我们可以得出以下结构（D也被叫做跳跃连接，这也是原因有时SSM会忽略掉这个跳跃连接）。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327170404078.png" alt="image-20250327170404078" loading="lazy"></p>
<p>忽略跳跃连接以后的结构如下图所示。在这里我们仍然需要注意一点，<strong>输入和输出还都是连续的</strong>，但是我们经常处理的都是离散的token，所以就需要将连续变为离散。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327170432725.png" alt="image-20250327170432725" loading="lazy"></p>
<h4 id="From-a-Continuous-to-a-Discrete-Signal"><a href="#From-a-Continuous-to-a-Discrete-Signal" class="headerlink" title="From a Continuous to a Discrete Signal"></a>From a Continuous to a Discrete Signal</h4><p>怎么将连续的信号变为离散的呢？这里使用的方法是零阶保持（Zero-order）。具体表现就是当我们接收到一个离散信号时，会一直保持其值，直到再次接受到一个新的离散信号，这个过程给SMM创造出了可以使用的连续信号。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327170954398.png" alt="image-20250327170954398" loading="lazy"></p>
<p>零阶保持用数学公式的表达如下：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327171657694.png" alt="image-20250327171657694" loading="lazy"></p>
<p>这里我看到了一个推导方法，可以帮助理解：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327185113603.png" alt="image-20250327185113603" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327185134507.png" alt="image-20250327185134507" loading="lazy"></p>
<p>这个离散数值的保持时间由一个新的可学习的参数Δ来表示，其也代表了输入的分辨率。</p>
<p><strong>现在我们有了连续的输入信号，就可以生成连续的输出，只需根据输入的时间步长对数值进行采样即可，采样值就是我们的离散化输出！！！</strong></p>
<p>经过上述处理，我们可以从连续 SSM 变为离散 SSM，其表述方式不再是函数对函数，即 x(t) → y(t)，而是序列对序列 $x_k  → y_k$，如下图所示，这里的矩阵A和B目前代表模型的离散化参数。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327171842868.png" alt="image-20250327171842868" loading="lazy"></p>
<p><strong>注意：在训练过程种，保存的是矩阵A的连续形式，而不是离散形式，在训练时，连续的表示形式会被离散化。</strong></p>
<h4 id="The-Recurrent-Representation-The-Convolution-Representation"><a href="#The-Recurrent-Representation-The-Convolution-Representation" class="headerlink" title="The Recurrent Representation &amp;&amp; The Convolution  Representation"></a>The Recurrent Representation &amp;&amp; The Convolution  Representation</h4><p>现在，我们可以考虑如何在模型种进行计算。在每个时间步，我们计算当前输入如何影响先前的隐藏状态，并且计算所预测的输出。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327172637659.png" alt="image-20250327172637659" loading="lazy"></p>
<p>这是之前所看过的RNN是比较相似的，如下图：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327172710306.png" alt="image-20250327172710306" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327172718840.png" alt="image-20250327172718840" loading="lazy"></p>
<p>现在我们的计算方法类似RNN，其在推理时非常快速，但是训练时比较慢！</p>
<p>我们也可以使用另一种表示方法就是卷积，因为我们需要处理的是序列，而不是图像，所以需要使用的也就是1维卷积。</p>
<p>此时，我们使用的卷积核来自于SSM</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327173711824.png" alt="image-20250327173711824" loading="lazy"></p>
<p>卷积核的推导如下所示（需要结合上面的状态等式和输出等式一起看）：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327173756898.png" alt="image-20250327173756898" loading="lazy"></p>
<p>这样我们就以卷积的方式来<strong>并行</strong>计算。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327173907263.png" alt="image-20250327173907263" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327173914065.png" alt="image-20250327173914065" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327173925414.png" alt="image-20250327173925414" loading="lazy"></p>
<h4 id="HiPPO-sequence"><a href="#HiPPO-sequence" class="headerlink" title="HiPPO sequence"></a>HiPPO sequence</h4><p>对于矩阵A来说，其是SSM中最重要的一个部分，因为其对上一个时间步的隐藏状态进行处理。矩阵A需要记住其之前所看过的所有标记之间的差异。那么该如何创建矩阵A，用于保留较大的内存去存储上下文呢？</p>
<p>这里使用HiPPO（High-order Polynomial Projection Operators），HiPPO尝试将所有的输入信号压缩成为一个系数向量。它可以很好地捕捉最近的token并且弱化之前的token，表示如下图。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327180300544.png" alt="image-20250327180300544" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327180321463.png" alt="image-20250327180321463" loading="lazy"></p>
<p><strong>此时，我们的状态空间就从SSM转变成为了S4模型。其有一个重要特性就是线性时间不变性（Linear TIME Invariance, LTI）其意味着，对于一个给定的SSM，矩阵A, B, C都是保持固定的，是静态的，这也说明无论你向SSM中提供什么序列，A,B,C的值并不会随输入的改变而改变，这就说明暂时其并不具备内容感知的能力。</strong></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327180434153.png" alt="image-20250327180434153" loading="lazy"></p>
<h3 id="Part3-Mamba-A-Selective-SSM"><a href="#Part3-Mamba-A-Selective-SSM" class="headerlink" title="Part3: Mamba - A Selective SSM"></a>Part3: Mamba - A Selective SSM</h3><p>对于Mamba来说，其有两个主要贡献：</p>
<ul>
<li>选择扫描算法（selective scan algorithm）: 这允许模型去过滤相关或者不相关的信息。</li>
<li>硬件感知算法（hardware-aware algorithm）：通过并行扫描、核融合和重新计算高效地存储中间结果。</li>
</ul>
<p>具体表述如下图：<br><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327181507179.png" alt="image-20250327181507179" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327181530737.png" alt="image-20250327181530737" loading="lazy"></p>
<p>对于SSM以及S4，其对关注或忽略特定输入的内容等任务上表现并不好，在mamba中使用选择性复制（selective copying ）和感应头（induction heads）。</p>
<p>由于我们上面提到的线性时间不变性导致了ssm无法进行内容感觉推理,但是我们希望ssm可以对输入进行推理，相比之下，transformer由于会根据输入序列动态改变注意力，其可以有选择地“注意”序列中的不同部分，所以在文本等任务上表现更好。这就说明矩阵A,B,C固有的LIM造成了无法进行内容感知的问题。</p>
<p>对于SSM和S4，A, B, C三个矩阵和输入独立，不会随着输入的改变而发生改变。与之相反地，mamba使A, B, C，甚至Δ都依赖于输入的序列长度和batch大小。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327183127351.png" alt="image-20250327183127351" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327183144420.png" alt="image-20250327183144420" loading="lazy"></p>
<p>这意味着，对于不同的输入，就有不同的矩阵B和C，这就解决了所面临的无法进行内容感知的问题。</p>
<p><strong>注意：矩阵A是静态的，因为我们希望状态是静态的，但是A又通过B,C影响，所以A又是动态的！！！</strong></p>
<p>这些矩阵共同选择将哪些内容保留在隐藏状态，哪些内容忽略不计，因为它们现在依赖于输入内容。</p>
<p>较小的步长 ∆ 会导致忽略特定的单词，而更多地使用以前的上下文，而较大的步长 ∆ 则更多地关注输入的单词而不是上下文。</p>
<h4 id="The-scan-operation"><a href="#The-scan-operation" class="headerlink" title="The scan operation"></a>The scan operation</h4><p>由于这些矩阵现在是动态的，因此无法使用卷积表示法进行计算，因为它假定了一个固定的核。我们只能使用递归表示法，而失去了卷积所提供的并行化功能。</p>
<p>每个状态都是前一个状态（乘以 A）加上当前输入（乘以 B）的总和。这就是所谓的扫描运算，可以用 for 循环轻松计算。相比之下，并行化似乎是不可能的，因为只有当我们拥有前一个状态时，才能计算出每个状态。然而，Mamba 通过<em>并行扫描</em>算法实现了这一点。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327183921685.png" alt="image-20250327183921685" loading="lazy"></p>
<p>动态矩阵 B 和 C 以及并行扫描算法共同创建了选择性扫描算法</p>
<h4 id="Hardware-aware-Algorithm"><a href="#Hardware-aware-Algorithm" class="headerlink" title="Hardware-aware Algorithm"></a>Hardware-aware Algorithm</h4><p>最近推出的 GPU 的一个缺点是其小型但高效的 SRAM 与大型但效率稍低的 DRAM 之间的传输（IO）速度有限。经常在 SRAM 和 DRAM 之间复制信息会成为瓶颈。<br>Mamba 和 Flash Attention 一样，试图限制从 DRAM 到 SRAM 以及从 SRAM 到 DRAM 的次数。它通过内核融合来实现这一目标，使模型能够防止写入中间结果，并持续执行计算，直到完成为止。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184406194.png" alt="image-20250327184406194" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184412584.png" alt="image-20250327184412584" loading="lazy"></p>
<p>我们可以通过可视化 Mamba 的基本架构来查看 DRAM 和 SRAM 分配的具体实例：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184534071.png" alt="image-20250327184534071" loading="lazy"></p>
<p>硬件感知算法的最后一个环节是重新计算。</p>
<p>中间状态不会被保存，但却是后向计算梯度所必需的。相反，作者在后向计算过程中重新计算了这些中间状态。</p>
<p>虽然这看起来效率不高，但比从相对较慢的 DRAM 中读取所有这些中间状态的成本要低得多。</p>
<p>整个Mamba的过程如下所示：</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184639573.png" alt="image-20250327184639573" loading="lazy"></p>
<p>对于整个Mamba块的清晰表达如下图所示，它也可以通过堆叠多次来完成特定任务。</p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184818979.png" alt="image-20250327184818979" loading="lazy"></p>
<p><img src="/Mamba%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/image-20250327184908764.png" alt="image-20250327184908764" loading="lazy"></p>
]]></content>
      <categories>
        <category>写笔记</category>
      </categories>
      <tags>
        <tag>Mamba</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer总结与理解</title>
    <url>/posts/6e9b5a85/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近在看一些few shot的文章时，发现将target和query作为q和kv以及kv和q会取得完全不同的效果。</p>
<p>为了加深自己对transformer的理解，所以决定阅读<a href="https://www.cnblogs.com/rossiXYZ/p/18785601%E4%B8%AD%E7%9A%84%E6%96%87%E7%AB%A0%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%80%9D%E8%80%83%EF%BC%8C%E5%B9%B6%E5%9C%A8%E8%BF%99%E9%87%8C%E8%AE%B0%E5%BD%95%E9%98%85%E8%AF%BB%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%89%80%E6%80%9D%E6%89%80%E6%83%B3,%E5%B9%B6%E5%AF%B9%E4%BD%9C%E8%80%85%E7%9A%84%E8%A7%82%E7%82%B9%E8%BF%9B%E8%A1%8C%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93,%E4%BE%9B%E8%87%AA%E5%B7%B1%E6%97%A5%E5%90%8E%E9%98%85%E8%AF%BB%E3%80%82">https://www.cnblogs.com/rossiXYZ/p/18785601中的文章并进行思考，并在这里记录阅读过程中的所思所想,并对作者的观点进行归纳总结,供自己日后阅读。</a></p>
<hr>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4><p>对于transformer的产生背景：对于文本生成任务来说，自回归模型 –&gt; 隐变量自回归模型 –&gt; 编码器-解码器模型</p>
<p>从宏观角度来看，文本生成任务就是通过总结上文，来对下文进行预测，但是怎么对长文本的序列压缩到一个较小状态值得考虑。人们使用神经网络进行拟合，即CNN,RNN和Transformer。</p>
<p>对于<strong>序列转换</strong>任务来说面临着两个挑战：<strong>对齐和长依赖</strong>。</p>
<p>对齐：英文单词和中文翻译必然不是一对一的关系，<strong>这就会导致我们在时刻t，无法确定此时模型是否已经获得了输出正确结果所需要的所有信息。</strong>所以人们通过将上文信息编码成为隐状态，这能够保证模型接受所有信息，但是，<strong>解码时无法确认每个词之间的贡献度，而是将其默认看作贡献度是一致的。</strong></p>
<p>长依赖：作者在文章中举了一个例子”秋蝉的衰弱的残声，更是北国的特产，因为北平处处全长着树，屋子又低，所以无论在什么地方，都听得见它们的啼唱。”从这个句子中，我们可以看出”它们”是指代”秋蝉”，而模型则需要依赖交互关系建模，神经网络很难处理这里依赖关系，尤其是随着两个位置之间间隔越长，这种依赖越难学习，从而导致长时信息丢失，也就是遗忘问题。</p>
<p>对于CNN来说，众所周知，其通过卷积核去捕获空间中的局部依赖关系，但是由于卷积核的尺寸是有限的，并且CNN对相对位置比较敏感，而对绝对位置不敏感，所以其很难提取长距离依赖关系。人们则是通过堆叠更深的卷积去将局部感受野扩大，不同深度的所看到的内容范围不同，在最顶层的卷积理论上是被看作将所有的序列信息压缩到了一个卷积窗口中的。但是<strong>这种逐深的去传播往往会导致信息只有部分被保留下来，导致模型性能下降。</strong>所以通常不会使用CNN去处理长文本信息。</p>
<p>(我自己在读论文的过程中，其实经常可以看到Transformer+CNN这两种结构结合的模型，其立意通常就是Transformer去捕获长程依赖，而CNN去捕获局部信息，两种互相增强。)</p>
<p>对于RNN来说，其本身就是一种时序结构，后面的时刻天然的就依赖于前一时刻的输出。其记忆功能，又使得它能够记住之前时刻的信息。所以<strong>在理论上，RNN可以利用先前的信息去预测无限长的文本。</strong>RNN在处理序列时也使用了权重共享的策略，这可以减少模型的参数量，但是也埋下了雷。在RNN中，其使用同一个函数将上文压缩成<strong>固定长度</strong>的隐向量，依赖这个固定长度的隐向量去预测下文毫无疑问会造成信息遗失的问题。很简单，因为长度固定，当信息越多，其丢失的细节一定也会更多，所以RNN中的隐向量对于编码上文的细节能力在本质上是有限的。同样地，在预测过程中，权重共享会导致对输入单词赋予相同权重，这显然无法区分其重要程度。对于RNN中的信息遗失问题，如果关键信息出现的序列起点，那么就很容易被忽略，并且其会优先关注尾部的输入，这其实也导致了难以捕获长距离依赖关系，这也让RNN形成一种有序假设，会让其无法平带看待每个输入的顺序。此外，RNN依赖先前隐状态和当前输入，导致其无法并行，训练效率极低，并且其信息传输通路只有一条，那么随着时间步的增加，其在反向传播时一定会导致指数级的衰减或爆炸。当提取消失时，信息无法及时传递，导致RNN难以学习长距离依赖关系，当梯度爆炸时，又会导致网络不稳定。</p>
<p>综上所述，RNN和CNN都无法解决上面所提到的两个问题–对齐和长依赖。</p>
<hr>
<p>注意力机制本质上就是要像人类一样进行感知，可以选择性地优先关注相关信息，忽略或者抑制不相关信息。</p>
<p><strong>注意力机制本质是上下文决定一切，注意力机制是一种资源分配方案，注意力机制是信息交换，是”全局信息查询”</strong></p>
<p>论文”A General Survey on Attention Mechanisms in Deep Learning”中给出了注意力模型的通用结构，并将其称作任务模型。</p>
<p>任务模型中包括四个部分：</p>
<ul>
<li>特征模型：特征模型就是将任务模型的输入X转换为特征向量F;</li>
<li>查询模型: 查询模型产生查询向量q，q就可以理解为<strong>哪个特征向量中包含对q最重要的信息</strong>；</li>
<li>注意力模型:注意力模型会输出上下文向量c，其输入是特征向量F和查询向量q<ul>
<li>在注意力模型中，从输入特征向量F生成Key和Value；</li>
<li>利用q和Key计算相似度分数，产生相似度向量e，  $ e_l $  ** 就代表Key中第l列对q的重要性。**</li>
<li>对相似度分数进行处理得到注意力权重a；</li>
<li>利用权重a对Value进行计算，得到上下文向量c</li>
</ul>
</li>
<li>输出模型: 利用上下文向量c将各个部分进行加权。</li>
</ul>
<p>注意力模型中不可避免的要提到QKV这三个关键术语，对于注意力模型中的两个输入q和F，可以将其理解为q(正在处理的序列&#x2F;目标序列)和F(被关注的序列&#x2F;源序列)</p>
<ul>
<li>Q 查询矩阵：其针对目标序列q，可以理解为<strong>某个单词向其它单词发出询问</strong>，目标序列中的每个元素把自己的特征总结到一个向量query中(我理解为线性映射变为query)，所有元素的query就共同构成了查询矩阵Q；</li>
<li>K 键矩阵：其针对源序列F，可以理解为<strong>某个单词依据自身特征对查询单词的回答</strong>，同理，源序列中的每个元素将自己的特征总结到一个向量key中，所有元素的key就共同构成了键矩阵K；</li>
<li>V 值矩阵：其针对源序列F，每个元素的实际值是向量value，所有元素的value就构成了值矩阵V。</li>
</ul>
<p><strong>实际中，我们的KV是来自同一个特征，但是千万不能将其视作相同，K和V分别代表着不同的含义，而这个含义就是通过线性投影去映射出来的</strong>。</p>
<p><strong>我对QKV的理解就把Q当作一个询问者，询问我和谁像，而K只是用来计算与Q是否像，V则是用于被取出，其对应的就是QK算出来像所对应的值</strong></p>
<p>作者给出一种比喻：query是你要找的内容，key是字典的索引（字典里面有什么样的信息），value是对应的信息。普通的字典查找是精确匹配，即依据匹配的键来返回其对应的值。而注意力机制是向量化+模糊匹配+信息合并。注意力机制不仅查找最佳匹配，还要依据匹配程度做加权求和。源序列每个元素转化为&lt;key,value&gt;对，这就构成了源序列的字典。目标序列每个元素提出了query，这就是要查询的内容。在查找中，目标序列中每个元素会用自己的query去和目标序列每个元素的key计算得到对齐系数。这个对齐系数就是元素之间的相似度或者相关性。query和key越相似就代表value对query的影响力越大，query越需要吸收value的信息。随后query会根据两个词之间的亲密关系来决定从V中提取出多少信息出来融入到自身。</p>
<p><strong>我认为注意力机制中比较重要的一点就是在解码过程中，会计算与每一个输入部分进行计算，其并不是像RNN存储一个包含上文信息的固定长度的隐状态，而是存储每个上文信息的隐状态，这使每个输入的隐状态没有被压缩，并且也忽略了距离的影响。</strong></p>
<p>对于注意力中的加权求和，可以拆分成两部分：加权和求和：</p>
<ul>
<li>加权：CNN和RNN的权重会被固定，在测试阶段使用固定的权重，而<strong>注意力机制则是会动态地计算当前应该关注哪些输入</strong></li>
<li>求和：对数据进行融合，不是按同等贡献，而是依据相似度有侧重点的进行融合。</li>
</ul>
<p>对比注意力机制和CNN，RNN，不难看出：</p>
<ul>
<li>对齐：注意力机制中允许对输入的不同部分计算相关性，这使不同输入与输出的对齐得到了控制，而不再是同等的对齐贡献</li>
<li>长距离依赖：注意力机制中存储了每个输入的隐状态，这就避免了头部信息随着时间序列的增大而消失，并且消除了距离的概念，使RNN中的有序被克服，Transformer可以平等地看待每一个输入的隐状态。</li>
</ul>
<p>虽然注意力机制可以很好的克服CNN和RNN的缺陷，但是内存以及算例的需求也是急剧增长。</p>
<h4 id="总结架构"><a href="#总结架构" class="headerlink" title="总结架构"></a>总结架构</h4><p>在介绍Transformer中，这里再次对一些关键的术语进行解释：</p>
<ul>
<li><p>分词(tokenize): 分词就是将用户输入的文本(通常是连续的文字)拆解为若干个独立的词汇单元，即一个一个的token;</p>
</li>
<li><p>编码：在NLP中会有一个词表，这些词表与词之间是一一对应的关系，其会为对应的词分配一个独一无二的整数ID代表词表的索引;</p>
</li>
<li><p>嵌入化(embedding): embedding就是将分词通过编码转化成的数字映射到一个高维向量空间中，这个token就被转换为了word embedding，只有这样每个token才能被LLM处理。embedding过程也可以被分为两部分:</p>
<ul>
<li>生成每个token的embedding,其堆叠在一起形成一个embedding矩阵,该矩阵是一个可学习的,通过随机初始化获得;<ul>
<li>token embedding则是将每个token得到的整数ID和一个高维向量相关联,其会去embedding矩阵中查找第token_id的数据作为embedding.</li>
</ul>
</li>
</ul>
</li>
<li><p>位置编码: 对于文本来说, 模型不单单需要理解文本的语义,还需要知道文本中每个单词的顺序,位置编码可以确保单词的顺序不会丢失,契合embedding向量相加,形成最终的嵌入矩阵.</p>
</li>
</ul>
<p>可以结合下图去理解相关操作:<img src="/Transformer%E6%80%BB%E7%BB%93%E4%B8%8E%E7%90%86%E8%A7%A3/image-20251006161500903.png" alt="image-20251006161500903" loading="lazy"></p>
<p><img src="/Transformer%E6%80%BB%E7%BB%93%E4%B8%8E%E7%90%86%E8%A7%A3/image-20251006151258779.png" alt="image-20251006151258779" loading="lazy"></p>
<p>这是Transformer论文中的架构图, 其很明显可以分为四部分:输入,输出,编码器(Encoder, 左部分)和解码器(Decoder, 右部分). 从图中可以看出,编码器和解码器部分都有$ \times N $ 这表示这部分会堆叠N次,<strong>这种将同一结构重复多次的分层机制就是栈.</strong> 这里对分层做一个简短的解释,即第i层的输出会被作为第i+1的输入,以此不断重复N次$ (i\in {1, N-1}) $.</p>
<p>此外,从图中也可以看到解码器的输入其实是有两个:</p>
<ul>
<li>编码器的输出,其将inputs编码成了隐状态(对应于RNN中的hidden state, Transformer中也叫memory), 或者也可以理解为编码器把源语言的完整句子编码成为隐状态,并<strong>一次性</strong>输出给解码器;</li>
<li>Ouputs(shifted right):这个输入位于图的右下角,Outputs实际上是解码器之前输出的拼接,因为解码器不能一次性输出生成的文本,其也是一个一个输出,所以这次的输出需要加到这次输入的后面作为下次的输入,shifted right的目的也就是将序列整体右移一位.</li>
</ul>
<p>作者在这里提到了对<strong>多层的理解</strong>,自己之前只是也多层理解成为对捕获到的信息重复精炼,并没有再往深处思考,文中作者给出了几个观点:</p>
<ul>
<li>BERT中短语表示主要在神经网络的较低层捕捉短语级别的信息，并在中间层中编码了语言要素的复杂层次结构。这个层次结构以表层特征作为基础，中间层提取语法特征，最上层呈现语义特征。(论文: What Does BERT Learn about the Structure of Language?)</li>
<li>较深层的注意力模块（最后25%的层）主要负责记忆, 较浅层的注意力模块对模型的泛化和推理能力至关重要, 在深层注意力模块应用短路（short-circuit）干预可以显著降低记忆所需内存，同时保持模型性能(这里其实也说明了残差连接的重要性).(论文: Analyzing Memorization in Large Language Models through the Lens of Model Attribution)</li>
<li>语言模型存在一种普遍机制：防止过度自信（anti-overconfidence）：在模型的最后若干层，语言模型总是在抑制正确答案的输出。这种抑制具体又分为两种：1. 通过注意力头将输入起始位置的信息复制到了最末位置，我们发现起始位置的信息似乎包含了很多高频的token，模型可以通过这种方法来让高频token稀释残差流中的正确回答，降低回答的自信度; 2. 末层的MLP似乎在将残差流引导向一个“平均”token的方向（平均token是基于训练数据的词频，对token embedding加权平均得到的结果）。论文(Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models)</li>
</ul>
<p>Transformer中有三种注意力结构:</p>
<ul>
<li><p>自注意力(Encoder中): 处理单个序列内部元素之间的关系;</p>
</li>
<li><p>交叉注意力:(Decoder中) 处理两个不同的序列之间的关系;</p>
</li>
<li><p>掩码自注意力(因果自注意力,Decoder中): 通过掩码去控制模型在计算注意力分数时的关注范围, <strong>从而在解码时不会受到未来信息的影响.</strong>(比如一个完整的句子为: 你今天学习自注意力机制了嘛?,当我们处理到”学”时,我们只能利用”你今天”,而”习自注意力机制了嘛”这些信息是不能看到的,因为这是我们之后要预测的内容) . <strong>引入掩码的原因: Transformer是自回归模型, 当当前输出文本加入到输入序列就会变成新的输入, 后续的输出依赖于前面的输出词,这种特性使其具备因果关系. 这种串行结构显然会大幅影响训练时间,所以人们通过引入掩码,这样在计算注意力的时候通过掩码以确保后面的词不会参与前面词的计算.</strong></p>
</li>
</ul>
<p><strong>在Transformer的原论文种,作者就指出如果没有skip-connection和MLP,那么自注意力网络的输出会朝着一个rank-1的矩阵收缩, 这说明skip-connection和MLP可以很好地阻止自注意力网络的这种”秩坍塌”</strong></p>
]]></content>
      <categories>
        <category>写笔记</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
</search>
